{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Problem:\n",
    "\n",
    "I am trying to solve a very basic quantum computing problem (No prior QC knowledge needed). I am trying to implement vanilla policy gradient to see if a model can be trained to make any quantum state reach equal superposition. Let me explain:\n",
    "\n",
    "A random quantum state is in superposition between 0 and 1 with some specific probability. eg, a rough example is 0.2|0> + 0.8|1>. So the probability for 0 is 0.2 and probability for 1 is 0.8. Equal superposition is 0.5|0> + 0.5|1>. So equal probability for 0 and 1. That's it!\n",
    "\n",
    "Now, the gate sequence that needs to be followed (for any random quantum state) is a measurement and Hadamard gate. This is the sequence that the model needs to learn and I have defined these gates in the method \"functions\" and I call these gates using the variable \"command\" as initialised few cells down. \n",
    "\n",
    "So I create an \"Environment\" that randomly initialises a random quantum state and returns that. Then, I create a model and train it according to vanilla PG and I also have a greedy epsilon strategy as \"get_exploration_rate\". So if you run, the model doesn't seem to converge at all. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from qiskit.quantum_info import random_state\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import math\n",
    "import random\n",
    "from functions import *\n",
    "import tensorflow as tf\n",
    "import os\n",
    "# import objgraph\n",
    "import sys\n",
    "import csv\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is in case you make changes to any python file'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''This is in case you make changes to any python file'''\n",
    "# %reload_ext autoreload\n",
    "# %autoreload 2\n",
    "# from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = [0.0,0.0,0.0,0.0,-1.0]\n",
    "# gamma = 0.95\n",
    "# r = np.vstack(r)\n",
    "# d_rw = discounted_reward(r,gamma)\n",
    "# print(d_rw)\n",
    "# d_rw -= np.mean(d_rw)\n",
    "# d_rw /= np.std(d_rw)\n",
    "# print(d_rw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "\n",
    "    def reset(self):\n",
    "        state = random_state(2)\n",
    "        new_state = state_norm(state)\n",
    "        new_state = np.reshape(new_state.flatten(), (1,1,2))\n",
    "        return new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The exploration-exploitation rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class epsilod_strategy():\n",
    "#     def __init__(self, start, end, decay):\n",
    "#         self.start = start\n",
    "#         self.end = end\n",
    "#         self.decay = decay\n",
    "    \n",
    "def get_exploration_rate(current_step, start, end, decay):\n",
    "\n",
    "    return max(end, end + (start - end)/math.exp(1.*current_step*decay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = 1.0\n",
    "end = 0.01\n",
    "decay = 0.009\n",
    "x = []\n",
    "for i in range(10000):\n",
    "    x.append(max(end, get_exploration_rate(i, start, end, decay)))\n",
    "\n",
    "plt.plot(x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The changes I made:\n",
    "\n",
    "Commenting lines 141 and 142 (basically no standardising the discounting sum of rewards)and changing the formula for \"label\" in line 150 and replacing it with line 151."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, state_size, new_state, is_eval=False, model_name=\"\"):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = 6 # measurement, CNOT, bit-flip\n",
    "#         self.memory = deque(maxlen=1000)\n",
    "        self.inventory = []\n",
    "        self.model_name = model_name\n",
    "        self.value = new_state\n",
    "        self.is_eval = is_eval\n",
    "        self.done = False\n",
    "#         self.final_state = [1/math.sqrt(2),1/math.sqrt(2)]\n",
    "\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon_start = 1.0\n",
    "        self.epsilon_end = 0.01\n",
    "        self.epsilon_decay = 0.009\n",
    "        self.learning_rate = 0.01\n",
    "        self.model = self.QC_model()\n",
    "#         self.model = load_model(\"model_June_23.h5\")\n",
    "        self.current_step = 0\n",
    "        self.final_state = np.array([(1/math.sqrt(2))+0j,(1/math.sqrt(2))+0j])\n",
    "\n",
    "#         self.model = load_model(\"models/\" + model_name) if is_eval else self._model()\n",
    "\n",
    "# '''\n",
    "# Method to initialize the network model. It contains 4 fully connected layers, where the last layer gives the\n",
    "# prbabilities of the gates\n",
    "\n",
    "# The network will be optmised using sgd optimizer. \n",
    "\n",
    "# '''\n",
    "    def QC_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=16, input_shape=self.state_size, activation=\"relu\", name='layer1'))\n",
    "        model.add(Dense(units=32, activation=\"relu\", name='layer2'))\n",
    "        model.add(Dense(units=8, activation=\"relu\", name='layer3'))\n",
    "        model.add(Dense(self.action_size, activation=\"softmax\"))\n",
    "        model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.003))\n",
    "        return model\n",
    "# '''\n",
    "# This defines how we select the action using exploration-exploitation rate, we defined above. \n",
    "\n",
    "# If random is more than the output of the exploration rate, then we select exploration. Else, exploitation. \n",
    "# The exploration rate reduces exponentially and we start exploiting rather than exploring by the last epochs. \n",
    "# The rate plot can be assesed through the cell the above\n",
    "\n",
    "# '''\n",
    "\n",
    "    def act(self, state, episode): #add episode to reduce the epsilon value\n",
    "        rate = get_exploration_rate(episode, self.epsilon_start, self.epsilon_end, self.epsilon_decay)\n",
    "        if random.random() <= rate:\n",
    "            options = self.model.predict(state)\n",
    "            options = np.squeeze(options)\n",
    "            action =  random.randrange(self.action_size)\n",
    "        else:\n",
    "            options = self.model.predict(state)\n",
    "            options = np.squeeze(options)\n",
    "            action = np.where(options == np.amax(options))[0][0]\n",
    "        return action, options\n",
    "\n",
    "# '''\n",
    "# Method to collect data and train the model. \n",
    "\n",
    "# '''\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        batch_size = 10\n",
    "        t = 0                   #increment\n",
    "        states, prob_actions, dlogps, drs, reward_data =[], [], [], [], []  #initialize the list variables\n",
    "        tr_x, tr_y = [],[]\n",
    "        avg_reward = []\n",
    "        reward_sum = 0\n",
    "        ep_number = 0\n",
    "        prev_state = None\n",
    "        new_state = self.value\n",
    "\n",
    "        while ep_number<1000:\n",
    "            print(\"episode number: \",ep_number)\n",
    "            print(\"time step: \",t)\n",
    "            prev_state = new_state          # Store the initial state\n",
    "            print(\"ORIGINAL STATE: \",new_state)\n",
    "            states.append(new_state)\n",
    "            action, probs = self.act(new_state, ep_number)        # send the state through the network and get the output\n",
    "            prob_actions.append(probs)\n",
    "            y = np.zeros([self.action_size])\n",
    "            new_state = np.squeeze(new_state)\n",
    "            y[action] = 1                                   # one hot encoding for the actions\n",
    "            print(\"Action: \",command[action])\n",
    "            new_state = eval(command[action])               # run the command according to the action predicted by the network\n",
    "            print(\"NEW STATE: \",new_state)\n",
    "            print(\"RESULT:\", np.allclose(new_state,self.final_state))\n",
    "            if(np.allclose(new_state,self.final_state)):    # if the state is equal to the final state\n",
    "                rw = 1\n",
    "                drs.append(rw)\n",
    "                reward_sum+=rw\n",
    "                self.done = True                                      # set done = True so that we can train the network on the data\n",
    "            if(t<4 and not np.allclose(new_state,self.final_state)):  # when we are NOT done with the episode and want to store the reward\n",
    "                rw = reward(new_state, self.final_state)\n",
    "                drs.append(rw)\n",
    "                reward_sum+=rw\n",
    "            elif(t==4):                      # when we have reached the final time stamp for the episode\n",
    "                self.done = True\n",
    "                if not np.allclose(new_state, self.final_state):\n",
    "                    rw = -1\n",
    "                    drs.append(rw)\n",
    "                    reward_sum+=rw\n",
    "            new_state = np.reshape(new_state, (1,1,2))           # get the shape of state according to keras requirements\n",
    "            print(\"reward till now: \",reward_sum)\n",
    "            dlogps.append(np.array(y).astype('float32') * probs)  # multiply one hot encoding to the probabilities\n",
    "            del(probs, action) \n",
    "            t+=1\n",
    "            if(self.done):                         #### Done State\n",
    "                ep_number+=1\n",
    "                ep_x = np.vstack(states)                     # vertially stack the states we used to train the network on\n",
    "                ep_dlogp = np.vstack(dlogps)                 # vertically stack the action probabilities we got for the states above\n",
    "                ep_reward = np.vstack(drs)                   # vertically stack the rewards corresponding to the state-action pairs\n",
    "                disc_rw = discounted_reward(ep_reward,self.gamma)\n",
    "                print(\"disc_rw: \", (disc_rw))\n",
    "                disc_rw = disc_rw.astype('float32')\n",
    "                tr_y_len = len(ep_dlogp)\n",
    "                ep_dlogp*=disc_rw\n",
    "                if ep_number % batch_size == 0:\n",
    "\n",
    "                    input_tr_y = ep_dlogp\n",
    "                    input_tr_y = np.reshape(input_tr_y, (tr_y_len,1,6))\n",
    "\n",
    "                    self.model.train_on_batch(ep_x, input_tr_y)\n",
    "                    dlogps, drs, states, prob_actions,proj_data, reward_data = [],[],[],[],[],[]\n",
    "                avg_reward.append((reward_sum))\n",
    "\n",
    "                print('Episode {:} reward {:.2f}, Last 30ep Avg. rewards {:.2f}.'.format(\n",
    "                    ep_number,reward_sum,avg_reward[ep_number-1]))\n",
    "                env = Environment()\n",
    "                new_state = env.reset()\n",
    "                t=0\n",
    "                self.done=False\n",
    "\n",
    "        return avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = ['bit_flip_X(new_state)',\n",
    "           'bit_flip_Y(new_state)',\n",
    "           'hadamard_X(new_state)',\n",
    "           'hadamard_Y(new_state)',\n",
    "           'measurement(new_state[0],new_state[1])',\n",
    "           'nothing(new_state)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_exploration_rate(current_step, start, end, decay):\n",
    "        \n",
    "#         return end + (start - end)/math.exp(1.*current_step*decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: The `random_state` function is deprecated as of 0.13.0, and will be removed no earlier than 3 months after that release date. You should use the `random_statevector` function instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode number:  0\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.3254944 +0.09502888j -0.70045601+0.62800023j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.72545649+0.51125878j  0.2651379 -0.37686765j]\n",
      "RESULT: False\n",
      "reward till now:  0.0\n",
      "episode number:  0\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.72545649+0.51125878j  0.2651379 -0.37686765j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  0.0\n",
      "episode number:  0\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  0.0\n",
      "episode number:  0\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  0.0\n",
      "episode number:  0\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.5-0.5j  0.5-0.5j]\n",
      "RESULT: False\n",
      "reward till now:  -1.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 1 reward -1.00, Last 30ep Avg. rewards -1.00.\n",
      "episode number:  1\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.94635149-0.02447203j  0.30131819+0.1141373j ]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.5884643-0.23036847j -0.1957598-0.74987881j]\n",
      "RESULT: False\n",
      "reward till now:  -1.0\n",
      "episode number:  1\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.5884643-0.23036847j -0.1957598-0.74987881j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.55453018-0.6931395j  -0.27768402+0.36734928j]\n",
      "RESULT: False\n",
      "reward till now:  -1.0\n",
      "episode number:  1\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.55453018-0.6931395j  -0.27768402+0.36734928j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.13235688-0.29377139j  0.6864759 -0.65186722j]\n",
      "RESULT: False\n",
      "reward till now:  -1.0\n",
      "episode number:  1\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.13235688-0.29377139j  0.6864759 -0.65186722j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -1.0\n",
      "episode number:  1\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -2.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 2 reward -2.00, Last 30ep Avg. rewards -2.00.\n",
      "episode number:  2\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.3202888 -0.41747255j -0.21024341-0.82397176j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.3202888 -0.41747255j -0.21024341-0.82397176j]\n",
      "RESULT: False\n",
      "reward till now:  -2.0\n",
      "episode number:  2\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.3202888 -0.41747255j -0.21024341-0.82397176j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.3202888 -0.41747255j -0.21024341-0.82397176j]\n",
      "RESULT: False\n",
      "reward till now:  -2.0\n",
      "episode number:  2\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.3202888 -0.41747255j -0.21024341-0.82397176j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.82397176+0.21024341j  0.41747255-0.3202888j ]\n",
      "RESULT: False\n",
      "reward till now:  -2.0\n",
      "episode number:  2\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.82397176+0.21024341j  0.41747255-0.3202888j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.28743835-0.07781384j -0.87783369+0.37514292j]\n",
      "RESULT: False\n",
      "reward till now:  -2.0\n",
      "episode number:  2\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.28743835-0.07781384j -0.87783369+0.37514292j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -3.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 3 reward -3.00, Last 30ep Avg. rewards -3.00.\n",
      "episode number:  3\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.55185897-0.00585913j  0.40723768-0.72771891j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.1022627 -0.51871801j -0.67818375+0.51043194j]\n",
      "RESULT: False\n",
      "reward till now:  -3.0\n",
      "episode number:  3\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.1022627 -0.51871801j -0.67818375+0.51043194j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.1022627 -0.51871801j -0.67818375+0.51043194j]\n",
      "RESULT: False\n",
      "reward till now:  -3.0\n",
      "episode number:  3\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.1022627 -0.51871801j -0.67818375+0.51043194j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.28861924+0.11275931j 0.84633735-0.43324053j]\n",
      "RESULT: False\n",
      "reward till now:  -3.0\n",
      "episode number:  3\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.28861924+0.11275931j 0.84633735-0.43324053j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.8025355 -0.22661445j -0.39436626+0.38608019j]\n",
      "RESULT: False\n",
      "reward till now:  -3.0\n",
      "episode number:  3\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.8025355 -0.22661445j -0.39436626+0.38608019j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.28861924+0.11275931j 0.84633735-0.43324053j]\n",
      "RESULT: False\n",
      "reward till now:  -4.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 4 reward -4.00, Last 30ep Avg. rewards -4.00.\n",
      "episode number:  4\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.6091282 -0.10017258j 0.31792916-0.71961749j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.6091282 -0.10017258j 0.31792916-0.71961749j]\n",
      "RESULT: False\n",
      "reward till now:  -4.0\n",
      "episode number:  4\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.6091282 -0.10017258j 0.31792916-0.71961749j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.71961749-0.31792916j  0.10017258+0.6091282j ]\n",
      "RESULT: False\n",
      "reward till now:  -4.0\n",
      "episode number:  4\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.71961749-0.31792916j  0.10017258+0.6091282j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.4380137 +0.20590882j -0.57967912-0.65552855j]\n",
      "RESULT: False\n",
      "reward till now:  -4.0\n",
      "episode number:  4\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.4380137 +0.20590882j -0.57967912-0.65552855j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.57967912-0.65552855j -0.4380137 +0.20590882j]\n",
      "RESULT: False\n",
      "reward till now:  -4.0\n",
      "episode number:  4\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.57967912-0.65552855j -0.4380137 +0.20590882j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.4380137 +0.20590882j -0.57967912-0.65552855j]\n",
      "RESULT: False\n",
      "reward till now:  -5.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 5 reward -5.00, Last 30ep Avg. rewards -5.00.\n",
      "episode number:  5\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.65071216+0.26877119j -0.56560676-0.429447j  ]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.56560676-0.429447j   -0.65071216+0.26877119j]\n",
      "RESULT: False\n",
      "reward till now:  -5.0\n",
      "episode number:  5\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.56560676-0.429447j   -0.65071216+0.26877119j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.56560676-0.429447j   -0.65071216+0.26877119j]\n",
      "RESULT: False\n",
      "reward till now:  -5.0\n",
      "episode number:  5\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.56560676-0.429447j   -0.65071216+0.26877119j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -5.0\n",
      "episode number:  5\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.+0.j 0.+1.j]\n",
      "RESULT: False\n",
      "reward till now:  -5.0\n",
      "episode number:  5\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 0.+1.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.70710678+0.j         0.        -0.70710678j]\n",
      "RESULT: False\n",
      "reward till now:  -6.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 6 reward -6.00, Last 30ep Avg. rewards -6.00.\n",
      "episode number:  6\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.76249561-0.35909391j -0.24637951+0.47848631j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.47848631+0.24637951j 0.35909391-0.76249561j]\n",
      "RESULT: False\n",
      "reward till now:  -6.0\n",
      "episode number:  6\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.47848631+0.24637951j 0.35909391-0.76249561j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.2008249 -0.07970112j -0.42813436+0.87750673j]\n",
      "RESULT: False\n",
      "reward till now:  -6.0\n",
      "episode number:  6\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.2008249 -0.07970112j -0.42813436+0.87750673j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -6.0\n",
      "episode number:  6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -6.0\n",
      "episode number:  6\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -7.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 7 reward -7.00, Last 30ep Avg. rewards -7.00.\n",
      "episode number:  7\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.83618406+0.48220496j -0.06165529+0.25391578j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [ 0.25391578+0.06165529j -0.48220496-0.83618406j]\n",
      "RESULT: False\n",
      "reward till now:  -7.0\n",
      "episode number:  7\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.25391578+0.06165529j -0.48220496-0.83618406j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [ 0.25391578+0.06165529j -0.48220496-0.83618406j]\n",
      "RESULT: False\n",
      "reward till now:  -7.0\n",
      "episode number:  7\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.25391578+0.06165529j -0.48220496-0.83618406j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [ 0.25391578+0.06165529j -0.48220496-0.83618406j]\n",
      "RESULT: False\n",
      "reward till now:  -7.0\n",
      "episode number:  7\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.25391578+0.06165529j -0.48220496-0.83618406j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [ 0.25391578+0.06165529j -0.48220496-0.83618406j]\n",
      "RESULT: False\n",
      "reward till now:  -7.0\n",
      "episode number:  7\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.25391578+0.06165529j -0.48220496-0.83618406j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -8.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 8 reward -8.00, Last 30ep Avg. rewards -8.00.\n",
      "episode number:  8\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.64396574+0.56826432j 0.28701208-0.42427332j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.155346  +0.19887536j -0.60477174+0.75535908j]\n",
      "RESULT: False\n",
      "reward till now:  -8.0\n",
      "episode number:  8\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.155346  +0.19887536j -0.60477174+0.75535908j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [ 0.155346  +0.19887536j -0.60477174+0.75535908j]\n",
      "RESULT: False\n",
      "reward till now:  -8.0\n",
      "episode number:  8\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.155346  +0.19887536j -0.60477174+0.75535908j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.64396574+0.56826432j 0.28701208-0.42427332j]\n",
      "RESULT: False\n",
      "reward till now:  -8.0\n",
      "episode number:  8\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.64396574+0.56826432j 0.28701208-0.42427332j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.64396574+0.56826432j 0.28701208-0.42427332j]\n",
      "RESULT: False\n",
      "reward till now:  -8.0\n",
      "episode number:  8\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.64396574+0.56826432j 0.28701208-0.42427332j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -9.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 9 reward -9.00, Last 30ep Avg. rewards -9.00.\n",
      "episode number:  9\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.3064353 +0.51822053j  0.02263464+0.7981432j ]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.3064353 +0.51822053j  0.02263464+0.7981432j ]\n",
      "RESULT: False\n",
      "reward till now:  -9.0\n",
      "episode number:  9\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.3064353 +0.51822053j  0.02263464+0.7981432j ]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.3064353 +0.51822053j  0.02263464+0.7981432j ]\n",
      "RESULT: False\n",
      "reward till now:  -9.0\n",
      "episode number:  9\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.3064353 +0.51822053j  0.02263464+0.7981432j ]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [ 0.7981432 -0.02263464j -0.51822053-0.3064353j ]\n",
      "RESULT: False\n",
      "reward till now:  -9.0\n",
      "episode number:  9\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.7981432 -0.02263464j -0.51822053-0.3064353j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -9.0\n",
      "episode number:  9\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -10.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/constant_op.py:96: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return ops.EagerTensor(value, ctx.device_name, dtype)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: The `random_state` function is deprecated as of 0.13.0, and will be removed no earlier than 3 months after that release date. You should use the `random_statevector` function instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10 reward -10.00, Last 30ep Avg. rewards -10.00.\n",
      "episode number:  10\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.54338407-0.03806665j  0.80793879+0.22476565j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.18706844+0.13201613j -0.95552956-0.1858505j ]\n",
      "RESULT: False\n",
      "reward till now:  -10.0\n",
      "episode number:  10\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.18706844+0.13201613j -0.95552956-0.1858505j ]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.00086121+0.76901093j 0.58231193+0.26369351j]\n",
      "RESULT: False\n",
      "reward till now:  -10.0\n",
      "episode number:  10\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.00086121+0.76901093j 0.58231193+0.26369351j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [ 0.26369351-0.58231193j -0.76901093+0.00086121j]\n",
      "RESULT: False\n",
      "reward till now:  -10.0\n",
      "episode number:  10\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.26369351-0.58231193j -0.76901093+0.00086121j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.18706844+0.13201613j 0.95552956+0.1858505j ]\n",
      "RESULT: False\n",
      "reward till now:  -10.0\n",
      "episode number:  10\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.18706844+0.13201613j 0.95552956+0.1858505j ]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.26369351-0.58231193j -0.76901093+0.00086121j]\n",
      "RESULT: False\n",
      "reward till now:  -11.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 11 reward -11.00, Last 30ep Avg. rewards -11.00.\n",
      "episode number:  11\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.0551138+0.28344598j 0.8406056-0.45826093j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.28506807-0.39397134j -0.79482449+0.36301075j]\n",
      "RESULT: False\n",
      "reward till now:  -11.0\n",
      "episode number:  11\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.28506807-0.39397134j -0.79482449+0.36301075j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.0551138+0.28344598j 0.8406056-0.45826093j]\n",
      "RESULT: False\n",
      "reward till now:  -11.0\n",
      "episode number:  11\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.0551138+0.28344598j 0.8406056-0.45826093j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.0551138+0.28344598j 0.8406056-0.45826093j]\n",
      "RESULT: False\n",
      "reward till now:  -11.0\n",
      "episode number:  11\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.0551138+0.28344598j 0.8406056-0.45826093j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -11.0\n",
      "episode number:  11\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -12.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 12 reward -12.00, Last 30ep Avg. rewards -12.00.\n",
      "episode number:  12\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.84219141+0.38895753j -0.16666548+0.33413812j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.35924793+0.3928848j  -0.15718421-0.83179059j]\n",
      "RESULT: False\n",
      "reward till now:  -12.0\n",
      "episode number:  12\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.35924793+0.3928848j  -0.15718421-0.83179059j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.15718421-0.83179059j -0.35924793+0.3928848j ]\n",
      "RESULT: False\n",
      "reward till now:  -12.0\n",
      "episode number:  12\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.15718421-0.83179059j -0.35924793+0.3928848j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -12.0\n",
      "episode number:  12\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -12.0\n",
      "episode number:  12\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -13.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 13 reward -13.00, Last 30ep Avg. rewards -13.00.\n",
      "episode number:  13\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.56837922-0.66879843j 0.45849512+0.13941288j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.56837922-0.66879843j 0.45849512+0.13941288j]\n",
      "RESULT: False\n",
      "reward till now:  -13.0\n",
      "episode number:  13\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.56837922-0.66879843j 0.45849512+0.13941288j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.13941288-0.45849512j 0.66879843+0.56837922j]\n",
      "RESULT: False\n",
      "reward till now:  -13.0\n",
      "episode number:  13\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.13941288-0.45849512j 0.66879843+0.56837922j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.50048459-0.79711691j -0.1487069 -0.30332501j]\n",
      "RESULT: False\n",
      "reward till now:  -13.0\n",
      "episode number:  13\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.50048459-0.79711691j -0.1487069 -0.30332501j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [ 0.50048459-0.79711691j -0.1487069 -0.30332501j]\n",
      "RESULT: False\n",
      "reward till now:  -13.0\n",
      "episode number:  13\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.50048459-0.79711691j -0.1487069 -0.30332501j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.13941288-0.45849512j 0.66879843+0.56837922j]\n",
      "RESULT: False\n",
      "reward till now:  -14.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 14 reward -14.00, Last 30ep Avg. rewards -14.00.\n",
      "episode number:  14\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.20407906+0.29385895j -0.42476862+0.83160705j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [ 0.20407906+0.29385895j -0.42476862+0.83160705j]\n",
      "RESULT: False\n",
      "reward till now:  -14.0\n",
      "episode number:  14\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.20407906+0.29385895j -0.42476862+0.83160705j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.42476862+0.83160705j  0.20407906+0.29385895j]\n",
      "RESULT: False\n",
      "reward till now:  -14.0\n",
      "episode number:  14\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.42476862+0.83160705j  0.20407906+0.29385895j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.09256712+0.44372929j -0.73234067-0.50814642j]\n",
      "RESULT: False\n",
      "reward till now:  -14.0\n",
      "episode number:  14\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.09256712+0.44372929j -0.73234067-0.50814642j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.42476862+0.83160705j  0.20407906+0.29385895j]\n",
      "RESULT: False\n",
      "reward till now:  -14.0\n",
      "episode number:  14\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.42476862+0.83160705j  0.20407906+0.29385895j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -15.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 15 reward -15.00, Last 30ep Avg. rewards -15.00.\n",
      "episode number:  15\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.90048334-0.22829306j 0.09514667-0.35771377j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.35771377-0.09514667j  0.22829306+0.90048334j]\n",
      "RESULT: False\n",
      "reward till now:  -15.0\n",
      "episode number:  15\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.35771377-0.09514667j  0.22829306+0.90048334j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.09151427+0.56945902j -0.4143694 -0.70401673j]\n",
      "RESULT: False\n",
      "reward till now:  -15.0\n",
      "episode number:  15\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.09151427+0.56945902j -0.4143694 -0.70401673j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.70401673+0.4143694j  -0.56945902-0.09151427j]\n",
      "RESULT: False\n",
      "reward till now:  -15.0\n",
      "episode number:  15\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.70401673+0.4143694j  -0.56945902-0.09151427j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.70401673+0.4143694j  -0.56945902-0.09151427j]\n",
      "RESULT: False\n",
      "reward till now:  -15.0\n",
      "episode number:  15\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.70401673+0.4143694j  -0.56945902-0.09151427j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.56252536+0.69567175j  0.10966492-0.43310465j]\n",
      "RESULT: False\n",
      "reward till now:  -16.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 16 reward -16.00, Last 30ep Avg. rewards -16.00.\n",
      "episode number:  16\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.38728765-0.44116297j 0.78210266+0.20904291j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.82688382-0.16413367j -0.27917637-0.45976499j]\n",
      "RESULT: False\n",
      "reward till now:  -16.0\n",
      "episode number:  16\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.82688382-0.16413367j -0.27917637-0.45976499j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.45976499+0.27917637j  0.16413367+0.82688382j]\n",
      "RESULT: False\n",
      "reward till now:  -16.0\n",
      "episode number:  16\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.45976499+0.27917637j  0.16413367+0.82688382j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -16.0\n",
      "episode number:  16\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -16.0\n",
      "episode number:  16\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -17.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 17 reward -17.00, Last 30ep Avg. rewards -17.00.\n",
      "episode number:  17\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.00317004-0.38616407j 0.86642774+0.3164968j ]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.00317004-0.38616407j 0.86642774+0.3164968j ]\n",
      "RESULT: False\n",
      "reward till now:  -17.0\n",
      "episode number:  17\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.00317004-0.38616407j 0.86642774+0.3164968j ]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.00317004-0.38616407j 0.86642774+0.3164968j ]\n",
      "RESULT: False\n",
      "reward till now:  -17.0\n",
      "episode number:  17\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.00317004-0.38616407j 0.86642774+0.3164968j ]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.86642774+0.3164968j  0.00317004-0.38616407j]\n",
      "RESULT: False\n",
      "reward till now:  -17.0\n",
      "episode number:  17\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.86642774+0.3164968j  0.00317004-0.38616407j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.86642774+0.3164968j  0.00317004-0.38616407j]\n",
      "RESULT: False\n",
      "reward till now:  -17.0\n",
      "episode number:  17\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.86642774+0.3164968j  0.00317004-0.38616407j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.3395977 +0.22155548j -0.22603859+0.88571617j]\n",
      "RESULT: False\n",
      "reward till now:  -18.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 18 reward -18.00, Last 30ep Avg. rewards -18.00.\n",
      "episode number:  18\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.68295235+0.55861443j -0.31279948+0.35168523j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -18.0\n",
      "episode number:  18\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.+0.j 0.+1.j]\n",
      "RESULT: False\n",
      "reward till now:  -18.0\n",
      "episode number:  18\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 0.+1.j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -18.0\n",
      "episode number:  18\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.70710678+0.j         0.        +0.70710678j]\n",
      "RESULT: False\n",
      "reward till now:  -18.0\n",
      "episode number:  18\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.70710678+0.j         0.        +0.70710678j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.70710678+0.j         0.        +0.70710678j]\n",
      "RESULT: False\n",
      "reward till now:  -19.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 19 reward -19.00, Last 30ep Avg. rewards -19.00.\n",
      "episode number:  19\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.42562772+0.77069984j  0.46987624-0.06386803j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.34612576+0.21271441j -0.87721976-0.25580273j]\n",
      "RESULT: False\n",
      "reward till now:  -19.0\n",
      "episode number:  19\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.34612576+0.21271441j -0.87721976-0.25580273j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.42562772+0.77069984j  0.46987624-0.06386803j]\n",
      "RESULT: False\n",
      "reward till now:  -19.0\n",
      "episode number:  19\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.42562772+0.77069984j  0.46987624-0.06386803j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.06386803-0.46987624j -0.77069984-0.42562772j]\n",
      "RESULT: False\n",
      "reward till now:  -19.0\n",
      "episode number:  19\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.06386803-0.46987624j -0.77069984-0.42562772j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.06386803-0.46987624j -0.77069984-0.42562772j]\n",
      "RESULT: False\n",
      "reward till now:  -19.0\n",
      "episode number:  19\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.06386803-0.46987624j -0.77069984-0.42562772j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.34612576+0.21271441j  0.87721976+0.25580273j]\n",
      "RESULT: False\n",
      "reward till now:  -20.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 20 reward -20.00, Last 30ep Avg. rewards -20.00.\n",
      "episode number:  20\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.01085066-0.19985976j 0.60178886+0.77316784j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.60178886+0.77316784j 0.01085066-0.19985976j]\n",
      "RESULT: False\n",
      "reward till now:  -20.0\n",
      "episode number:  20\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.60178886+0.77316784j 0.01085066-0.19985976j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.01085066-0.19985976j 0.60178886+0.77316784j]\n",
      "RESULT: False\n",
      "reward till now:  -20.0\n",
      "episode number:  20\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.01085066-0.19985976j 0.60178886+0.77316784j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.5543848 -0.56685118j -0.28420679-0.53903965j]\n",
      "RESULT: False\n",
      "reward till now:  -20.0\n",
      "episode number:  20\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.5543848 -0.56685118j -0.28420679-0.53903965j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.01085066-0.19985976j 0.60178886+0.77316784j]\n",
      "RESULT: False\n",
      "reward till now:  -20.0\n",
      "episode number:  20\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.01085066-0.19985976j 0.60178886+0.77316784j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.77316784-0.60178886j 0.19985976+0.01085066j]\n",
      "RESULT: False\n",
      "reward till now:  -21.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 21 reward -21.00, Last 30ep Avg. rewards -21.00.\n",
      "episode number:  21\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.10281884-0.84073046j  0.41392765+0.3335633j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -21.0\n",
      "episode number:  21\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -21.0\n",
      "episode number:  21\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -21.0\n",
      "episode number:  21\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -21.0\n",
      "episode number:  21\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -22.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 22 reward -22.00, Last 30ep Avg. rewards -22.00.\n",
      "episode number:  22\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.07183387+0.73333701j  0.38550806-0.55537398j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.44350293+0.24595221j -0.79114293+0.3419145j ]\n",
      "RESULT: False\n",
      "reward till now:  -22.0\n",
      "episode number:  22\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.44350293+0.24595221j -0.79114293+0.3419145j ]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [ 0.3419145 +0.79114293j -0.24595221-0.44350293j]\n",
      "RESULT: False\n",
      "reward till now:  -22.0\n",
      "episode number:  22\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.3419145 +0.79114293j -0.24595221-0.44350293j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [ 0.3419145 +0.79114293j -0.24595221-0.44350293j]\n",
      "RESULT: False\n",
      "reward till now:  -22.0\n",
      "episode number:  22\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.3419145 +0.79114293j -0.24595221-0.44350293j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.06785558+0.24581861j 0.41568453+0.87302646j]\n",
      "RESULT: False\n",
      "reward till now:  -22.0\n",
      "episode number:  22\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.06785558+0.24581861j 0.41568453+0.87302646j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.66530407-0.12011335j -0.46775336-0.56934179j]\n",
      "RESULT: False\n",
      "reward till now:  -23.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 23 reward -23.00, Last 30ep Avg. rewards -23.00.\n",
      "episode number:  23\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.16853891+0.25337085j -0.84305379+0.44346156j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.71530406+0.49273493j  0.47695405-0.13441443j]\n",
      "RESULT: False\n",
      "reward till now:  -23.0\n",
      "episode number:  23\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.71530406+0.49273493j  0.47695405-0.13441443j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.71530406+0.49273493j  0.47695405-0.13441443j]\n",
      "RESULT: False\n",
      "reward till now:  -23.0\n",
      "episode number:  23\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.71530406+0.49273493j  0.47695405-0.13441443j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [ 0.47695405-0.13441443j -0.71530406+0.49273493j]\n",
      "RESULT: False\n",
      "reward till now:  -23.0\n",
      "episode number:  23\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.47695405-0.13441443j -0.71530406+0.49273493j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.71530406+0.49273493j  0.47695405-0.13441443j]\n",
      "RESULT: False\n",
      "reward till now:  -23.0\n",
      "episode number:  23\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.71530406+0.49273493j  0.47695405-0.13441443j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.16853891+0.25337085j -0.84305379+0.44346156j]\n",
      "RESULT: False\n",
      "reward till now:  -24.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 24 reward -24.00, Last 30ep Avg. rewards -24.00.\n",
      "episode number:  24\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.04056952-0.93393603j -0.25211721+0.250109j  ]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.25211721+0.250109j   -0.04056952-0.93393603j]\n",
      "RESULT: False\n",
      "reward till now:  -24.0\n",
      "episode number:  24\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.25211721+0.250109j   -0.04056952-0.93393603j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.83866629+0.20554075j -0.14816678+0.48211872j]\n",
      "RESULT: False\n",
      "reward till now:  -24.0\n",
      "episode number:  24\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.83866629+0.20554075j -0.14816678+0.48211872j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.69779636+0.48624867j -0.48825688-0.19557015j]\n",
      "RESULT: False\n",
      "reward till now:  -24.0\n",
      "episode number:  24\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.69779636+0.48624867j -0.48825688-0.19557015j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.48825688-0.19557015j -0.69779636+0.48624867j]\n",
      "RESULT: False\n",
      "reward till now:  -24.0\n",
      "episode number:  24\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.48825688-0.19557015j -0.69779636+0.48624867j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.69779636+0.48624867j -0.48825688-0.19557015j]\n",
      "RESULT: False\n",
      "reward till now:  -25.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 25 reward -25.00, Last 30ep Avg. rewards -25.00.\n",
      "episode number:  25\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.38952475+0.59220174j  0.6152967 -0.3449312j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -25.0\n",
      "episode number:  25\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -25.0\n",
      "episode number:  25\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.5+0.5j 0.5+0.5j]\n",
      "RESULT: False\n",
      "reward till now:  -25.0\n",
      "episode number:  25\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.5+0.5j 0.5+0.5j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -25.0\n",
      "episode number:  25\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -26.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 26 reward -26.00, Last 30ep Avg. rewards -26.00.\n",
      "episode number:  26\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.90856776-0.2563149j  0.21023866-0.25417907j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.79111561-0.36097375j 0.49379324-0.00151026j]\n",
      "RESULT: False\n",
      "reward till now:  -26.0\n",
      "episode number:  26\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.79111561-0.36097375j 0.49379324-0.00151026j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.5583353 -0.60441154j -0.09391756+0.56047112j]\n",
      "RESULT: False\n",
      "reward till now:  -26.0\n",
      "episode number:  26\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.5583353 -0.60441154j -0.09391756+0.56047112j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.79111561-0.36097375j 0.49379324-0.00151026j]\n",
      "RESULT: False\n",
      "reward till now:  -26.0\n",
      "episode number:  26\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.79111561-0.36097375j 0.49379324-0.00151026j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.90856776-0.2563149j  0.21023866-0.25417907j]\n",
      "RESULT: False\n",
      "reward till now:  -26.0\n",
      "episode number:  26\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.90856776-0.2563149j  0.21023866-0.25417907j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.25417907-0.21023866j  0.2563149 +0.90856776j]\n",
      "RESULT: False\n",
      "reward till now:  -27.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 27 reward -27.00, Last 30ep Avg. rewards -27.00.\n",
      "episode number:  27\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.38071337+0.04484801j -0.50123673+0.77576268j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.50123673+0.77576268j  0.38071337+0.04484801j]\n",
      "RESULT: False\n",
      "reward till now:  -27.0\n",
      "episode number:  27\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.50123673+0.77576268j  0.38071337+0.04484801j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.32271556+0.27934205j -0.81775205-0.38614022j]\n",
      "RESULT: False\n",
      "reward till now:  -27.0\n",
      "episode number:  27\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.32271556+0.27934205j -0.81775205-0.38614022j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.38614022+0.81775205j -0.27934205-0.32271556j]\n",
      "RESULT: False\n",
      "reward till now:  -27.0\n",
      "episode number:  27\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.38614022+0.81775205j -0.27934205-0.32271556j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.32271556+0.27934205j -0.81775205-0.38614022j]\n",
      "RESULT: False\n",
      "reward till now:  -27.0\n",
      "episode number:  27\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.32271556+0.27934205j -0.81775205-0.38614022j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.50123673+0.77576268j  0.38071337+0.04484801j]\n",
      "RESULT: False\n",
      "reward till now:  -28.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 28 reward -28.00, Last 30ep Avg. rewards -28.00.\n",
      "episode number:  28\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.7594731 -0.48163315j -0.28682134+0.33009642j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.33009642+0.28682134j 0.48163315-0.7594731j ]\n",
      "RESULT: False\n",
      "reward till now:  -28.0\n",
      "episode number:  28\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.33009642+0.28682134j 0.48163315-0.7594731j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -28.0\n",
      "episode number:  28\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -28.0\n",
      "episode number:  28\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -27.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 29 reward -27.00, Last 30ep Avg. rewards -27.00.\n",
      "episode number:  29\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.46077235-0.34873432j  0.07084053+0.81305279j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [ 0.07084053+0.81305279j -0.46077235-0.34873432j]\n",
      "RESULT: False\n",
      "reward till now:  -27.0\n",
      "episode number:  29\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.07084053+0.81305279j -0.46077235-0.34873432j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.27572343+0.32832274j  0.37590707+0.82150754j]\n",
      "RESULT: False\n",
      "reward till now:  -27.0\n",
      "episode number:  29\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.27572343+0.32832274j  0.37590707+0.82150754j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [ 0.37590707+0.82150754j -0.27572343+0.32832274j]\n",
      "RESULT: False\n",
      "reward till now:  -27.0\n",
      "episode number:  29\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.37590707+0.82150754j -0.27572343+0.32832274j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.07084053+0.81305279j 0.46077235+0.34873432j]\n",
      "RESULT: False\n",
      "reward till now:  -27.0\n",
      "episode number:  29\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.07084053+0.81305279j 0.46077235+0.34873432j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -28.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 30 reward -28.00, Last 30ep Avg. rewards -28.00.\n",
      "episode number:  30\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.36470633-0.57454914j -0.73110442-0.04867139j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [ 0.36470633-0.57454914j -0.73110442-0.04867139j]\n",
      "RESULT: False\n",
      "reward till now:  -28.0\n",
      "episode number:  30\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.36470633-0.57454914j -0.73110442-0.04867139j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.04867139+0.73110442j  0.57454914+0.36470633j]\n",
      "RESULT: False\n",
      "reward till now:  -28.0\n",
      "episode number:  30\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.04867139+0.73110442j  0.57454914+0.36470633j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.22347045+0.11070131j -0.92323649-0.29230219j]\n",
      "RESULT: False\n",
      "reward till now:  -28.0\n",
      "episode number:  30\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.22347045+0.11070131j -0.92323649-0.29230219j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.49480931-0.12841122j  0.81084425+0.2849665j ]\n",
      "RESULT: False\n",
      "reward till now:  -28.0\n",
      "episode number:  30\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.49480931-0.12841122j  0.81084425+0.2849665j ]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [ 0.81084425+0.2849665j  -0.49480931-0.12841122j]\n",
      "RESULT: False\n",
      "reward till now:  -29.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 31 reward -29.00, Last 30ep Avg. rewards -29.00.\n",
      "episode number:  31\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.67737185+0.25767216j 0.23124508+0.64907484j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -29.0\n",
      "episode number:  31\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -28.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 32 reward -28.00, Last 30ep Avg. rewards -28.00.\n",
      "episode number:  32\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.62410561-0.70218838j  0.23106505+0.25304664j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.26237832-0.65990983j  0.3331345 -0.62024031j]\n",
      "RESULT: False\n",
      "reward till now:  -28.0\n",
      "episode number:  32\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.26237832-0.65990983j  0.3331345 -0.62024031j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.62024031-0.3331345j   0.65990983-0.26237832j]\n",
      "RESULT: False\n",
      "reward till now:  -28.0\n",
      "episode number:  32\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.62024031-0.3331345j   0.65990983-0.26237832j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.02805059-0.42109115j -0.90520284-0.05003218j]\n",
      "RESULT: False\n",
      "reward till now:  -28.0\n",
      "episode number:  32\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.02805059-0.42109115j -0.90520284-0.05003218j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -28.0\n",
      "episode number:  32\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.-1.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -29.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 33 reward -29.00, Last 30ep Avg. rewards -29.00.\n",
      "episode number:  33\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.34298046-0.69210325j -0.44589861+0.45225207j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.34298046-0.69210325j -0.44589861+0.45225207j]\n",
      "RESULT: False\n",
      "reward till now:  -29.0\n",
      "episode number:  33\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.34298046-0.69210325j -0.44589861+0.45225207j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.07726669-0.17409297j 0.80468883-0.56231431j]\n",
      "RESULT: False\n",
      "reward till now:  -29.0\n",
      "episode number:  33\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.07726669-0.17409297j 0.80468883-0.56231431j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.62363673-0.52071858j -0.51436512+0.27451394j]\n",
      "RESULT: False\n",
      "reward till now:  -29.0\n",
      "episode number:  33\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.62363673-0.52071858j -0.51436512+0.27451394j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.27451394+0.51436512j 0.52071858+0.62363673j]\n",
      "RESULT: False\n",
      "reward till now:  -29.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode number:  33\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.27451394+0.51436512j 0.52071858+0.62363673j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.52071858+0.62363673j 0.27451394+0.51436512j]\n",
      "RESULT: False\n",
      "reward till now:  -30.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 34 reward -30.00, Last 30ep Avg. rewards -30.00.\n",
      "episode number:  34\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.38219835-0.49237509j  0.7053266 +0.33764712j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.33764712-0.7053266j  0.49237509-0.38219835j]\n",
      "RESULT: False\n",
      "reward till now:  -30.0\n",
      "episode number:  34\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.33764712-0.7053266j  0.49237509-0.38219835j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.03150247-0.84690299j  0.15057946+0.50900762j]\n",
      "RESULT: False\n",
      "reward till now:  -30.0\n",
      "episode number:  34\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.03150247-0.84690299j  0.15057946+0.50900762j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.08420015-0.23892811j -0.12875137-0.95877358j]\n",
      "RESULT: False\n",
      "reward till now:  -30.0\n",
      "episode number:  34\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.08420015-0.23892811j -0.12875137-0.95877358j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.61841681-0.07790672j  0.25998865+0.73749379j]\n",
      "RESULT: False\n",
      "reward till now:  -30.0\n",
      "episode number:  34\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.61841681-0.07790672j  0.25998865+0.73749379j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.73749379-0.25998865j 0.07790672-0.61841681j]\n",
      "RESULT: False\n",
      "reward till now:  -31.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 35 reward -31.00, Last 30ep Avg. rewards -31.00.\n",
      "episode number:  35\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.38933346+0.63457631j 0.34725378+0.57021678j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.67850448+0.20316771j -0.69425871-0.12790383j]\n",
      "RESULT: False\n",
      "reward till now:  -31.0\n",
      "episode number:  35\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.67850448+0.20316771j -0.69425871-0.12790383j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.12790383+0.69425871j -0.20316771+0.67850448j]\n",
      "RESULT: False\n",
      "reward till now:  -31.0\n",
      "episode number:  35\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.12790383+0.69425871j -0.20316771+0.67850448j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.12790383+0.69425871j -0.20316771+0.67850448j]\n",
      "RESULT: False\n",
      "reward till now:  -31.0\n",
      "episode number:  35\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.12790383+0.69425871j -0.20316771+0.67850448j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.20316771+0.67850448j -0.12790383+0.69425871j]\n",
      "RESULT: False\n",
      "reward till now:  -31.0\n",
      "episode number:  35\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.20316771+0.67850448j -0.12790383+0.69425871j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [ 0.69425871+0.12790383j -0.67850448-0.20316771j]\n",
      "RESULT: False\n",
      "reward till now:  -32.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 36 reward -32.00, Last 30ep Avg. rewards -32.00.\n",
      "episode number:  36\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.4426185 +0.88234562j -0.10482956+0.12068895j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [ 0.12068895+0.10482956j -0.88234562-0.4426185j ]\n",
      "RESULT: False\n",
      "reward till now:  -32.0\n",
      "episode number:  36\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.12068895+0.10482956j -0.88234562-0.4426185j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.5385726 -0.23885286j  0.70925255+0.38710424j]\n",
      "RESULT: False\n",
      "reward till now:  -32.0\n",
      "episode number:  36\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.5385726 -0.23885286j  0.70925255+0.38710424j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.38710424-0.70925255j 0.23885286-0.5385726j ]\n",
      "RESULT: False\n",
      "reward till now:  -32.0\n",
      "episode number:  36\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.38710424-0.70925255j 0.23885286-0.5385726j ]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.5385726 -0.23885286j  0.70925255+0.38710424j]\n",
      "RESULT: False\n",
      "reward till now:  -32.0\n",
      "episode number:  36\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.5385726 -0.23885286j  0.70925255+0.38710424j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.38710424-0.70925255j 0.23885286-0.5385726j ]\n",
      "RESULT: False\n",
      "reward till now:  -33.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 37 reward -33.00, Last 30ep Avg. rewards -33.00.\n",
      "episode number:  37\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.11444262+0.16925876j  0.97619303+0.0728116j ]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.02943758-0.5705887j  -0.80995672-0.13240873j]\n",
      "RESULT: False\n",
      "reward till now:  -33.0\n",
      "episode number:  37\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.02943758-0.5705887j  -0.80995672-0.13240873j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.13240873+0.80995672j  0.5705887 -0.02943758j]\n",
      "RESULT: False\n",
      "reward till now:  -33.0\n",
      "episode number:  37\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.13240873+0.80995672j  0.5705887 -0.02943758j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.11444262+0.16925876j -0.97619303-0.0728116j ]\n",
      "RESULT: False\n",
      "reward till now:  -33.0\n",
      "episode number:  37\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.11444262+0.16925876j -0.97619303-0.0728116j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.77119587+0.06819844j  0.60934956+0.17116959j]\n",
      "RESULT: False\n",
      "reward till now:  -33.0\n",
      "episode number:  37\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.77119587+0.06819844j  0.60934956+0.17116959j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -34.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 38 reward -34.00, Last 30ep Avg. rewards -34.00.\n",
      "episode number:  38\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.34963683+0.30868582j 0.81165616-0.35168371j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.34963683+0.30868582j 0.81165616-0.35168371j]\n",
      "RESULT: False\n",
      "reward till now:  -34.0\n",
      "episode number:  38\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.34963683+0.30868582j 0.81165616-0.35168371j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.34963683+0.30868582j 0.81165616-0.35168371j]\n",
      "RESULT: False\n",
      "reward till now:  -34.0\n",
      "episode number:  38\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.34963683+0.30868582j 0.81165616-0.35168371j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.34963683+0.30868582j 0.81165616-0.35168371j]\n",
      "RESULT: False\n",
      "reward till now:  -34.0\n",
      "episode number:  38\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.34963683+0.30868582j 0.81165616-0.35168371j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.00144736-0.35565373j -0.79220141+0.49590851j]\n",
      "RESULT: False\n",
      "reward till now:  -34.0\n",
      "episode number:  38\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.00144736-0.35565373j -0.79220141+0.49590851j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.49590851+0.79220141j 0.35565373-0.00144736j]\n",
      "RESULT: False\n",
      "reward till now:  -35.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 39 reward -35.00, Last 30ep Avg. rewards -35.00.\n",
      "episode number:  39\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.37754876+0.38059618j  0.18350846-0.82397095j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.37754876+0.38059618j  0.18350846-0.82397095j]\n",
      "RESULT: False\n",
      "reward till now:  -35.0\n",
      "episode number:  39\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.37754876+0.38059618j  0.18350846-0.82397095j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.82397095-0.18350846j -0.38059618-0.37754876j]\n",
      "RESULT: False\n",
      "reward till now:  -35.0\n",
      "episode number:  39\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.82397095-0.18350846j -0.38059618-0.37754876j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -35.0\n",
      "episode number:  39\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -35.0\n",
      "episode number:  39\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.        +0.70710678j 0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -36.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 40 reward -36.00, Last 30ep Avg. rewards -36.00.\n",
      "episode number:  40\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.68140934+0.57666346j -0.37418103+0.25125508j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [ 0.25125508+0.37418103j -0.57666346+0.68140934j]\n",
      "RESULT: False\n",
      "reward till now:  -36.0\n",
      "episode number:  40\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.25125508+0.37418103j -0.57666346+0.68140934j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [ 0.68140934+0.57666346j -0.37418103+0.25125508j]\n",
      "RESULT: False\n",
      "reward till now:  -36.0\n",
      "episode number:  40\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.68140934+0.57666346j -0.37418103+0.25125508j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [ 0.25125508+0.37418103j -0.57666346+0.68140934j]\n",
      "RESULT: False\n",
      "reward till now:  -36.0\n",
      "episode number:  40\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.25125508+0.37418103j -0.57666346+0.68140934j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -36.0\n",
      "episode number:  40\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -37.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 41 reward -37.00, Last 30ep Avg. rewards -37.00.\n",
      "episode number:  41\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.45814892+0.86862743j -0.17939873+0.0583272j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -37.0\n",
      "episode number:  41\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.+0.j 0.+1.j]\n",
      "RESULT: False\n",
      "reward till now:  -37.0\n",
      "episode number:  41\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 0.+1.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 0.+1.j]\n",
      "RESULT: False\n",
      "reward till now:  -37.0\n",
      "episode number:  41\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 0.+1.j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -37.0\n",
      "episode number:  41\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -38.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 42 reward -38.00, Last 30ep Avg. rewards -38.00.\n",
      "episode number:  42\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.49536132-0.0054495j  -0.40709266-0.76737412j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [ 0.49536132-0.0054495j  -0.40709266-0.76737412j]\n",
      "RESULT: False\n",
      "reward till now:  -38.0\n",
      "episode number:  42\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.49536132-0.0054495j  -0.40709266-0.76737412j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.76737412+0.40709266j  0.0054495 +0.49536132j]\n",
      "RESULT: False\n",
      "reward till now:  -38.0\n",
      "episode number:  42\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.76737412+0.40709266j  0.0054495 +0.49536132j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [ 0.0054495 +0.49536132j -0.76737412+0.40709266j]\n",
      "RESULT: False\n",
      "reward till now:  -38.0\n",
      "episode number:  42\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.0054495 +0.49536132j -0.76737412+0.40709266j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [ 0.40709266+0.76737412j -0.49536132+0.0054495j ]\n",
      "RESULT: False\n",
      "reward till now:  -38.0\n",
      "episode number:  42\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.40709266+0.76737412j -0.49536132+0.0054495j ]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [ 0.0054495 +0.49536132j -0.76737412+0.40709266j]\n",
      "RESULT: False\n",
      "reward till now:  -39.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 43 reward -39.00, Last 30ep Avg. rewards -39.00.\n",
      "episode number:  43\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.10852746+0.3471068j  -0.86875718+0.33615416j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [ 0.33615416+0.86875718j -0.3471068 +0.10852746j]\n",
      "RESULT: False\n",
      "reward till now:  -39.0\n",
      "episode number:  43\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.33615416+0.86875718j -0.3471068 +0.10852746j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [ 0.10852746+0.3471068j  -0.86875718+0.33615416j]\n",
      "RESULT: False\n",
      "reward till now:  -39.0\n",
      "episode number:  43\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.10852746+0.3471068j  -0.86875718+0.33615416j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.31443739+0.85974567j 0.36886252-0.16095639j]\n",
      "RESULT: False\n",
      "reward till now:  -39.0\n",
      "episode number:  43\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.31443739+0.85974567j 0.36886252-0.16095639j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.10852746+0.3471068j  -0.86875718+0.33615416j]\n",
      "RESULT: False\n",
      "reward till now:  -39.0\n",
      "episode number:  43\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.10852746+0.3471068j  -0.86875718+0.33615416j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.53756359+0.48313846j  0.69104459+0.00774468j]\n",
      "RESULT: False\n",
      "reward till now:  -40.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 44 reward -40.00, Last 30ep Avg. rewards -40.00.\n",
      "episode number:  44\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.22380796+0.00519797j -0.71813219+0.65891512j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [ 0.65891512+0.71813219j -0.00519797-0.22380796j]\n",
      "RESULT: False\n",
      "reward till now:  -40.0\n",
      "episode number:  44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.65891512+0.71813219j -0.00519797-0.22380796j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.30766723+0.51147166j -0.50412062+0.62417947j]\n",
      "RESULT: False\n",
      "reward till now:  -40.0\n",
      "episode number:  44\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.30766723+0.51147166j -0.50412062+0.62417947j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.13891353+0.80302662j  0.57402069-0.07969646j]\n",
      "RESULT: False\n",
      "reward till now:  -40.0\n",
      "episode number:  44\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.13891353+0.80302662j  0.57402069-0.07969646j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.30766723+0.51147166j -0.50412062+0.62417947j]\n",
      "RESULT: False\n",
      "reward till now:  -40.0\n",
      "episode number:  44\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.30766723+0.51147166j -0.50412062+0.62417947j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [ 0.62417947+0.50412062j -0.51147166+0.30766723j]\n",
      "RESULT: False\n",
      "reward till now:  -41.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 45 reward -41.00, Last 30ep Avg. rewards -41.00.\n",
      "episode number:  45\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.604557  -0.39421987j  0.68504568+0.09906532j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.604557  -0.39421987j  0.68504568+0.09906532j]\n",
      "RESULT: False\n",
      "reward till now:  -41.0\n",
      "episode number:  45\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.604557  -0.39421987j  0.68504568+0.09906532j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.3574366-0.76315599j -0.2056449-0.49753612j]\n",
      "RESULT: False\n",
      "reward till now:  -41.0\n",
      "episode number:  45\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.3574366-0.76315599j -0.2056449-0.49753612j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.3574366-0.76315599j -0.2056449-0.49753612j]\n",
      "RESULT: False\n",
      "reward till now:  -41.0\n",
      "episode number:  45\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.3574366-0.76315599j -0.2056449-0.49753612j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.604557  -0.39421987j  0.68504568+0.09906532j]\n",
      "RESULT: False\n",
      "reward till now:  -41.0\n",
      "episode number:  45\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.604557  -0.39421987j  0.68504568+0.09906532j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [ 0.68504568+0.09906532j -0.604557  -0.39421987j]\n",
      "RESULT: False\n",
      "reward till now:  -42.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 46 reward -42.00, Last 30ep Avg. rewards -42.00.\n",
      "episode number:  46\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.30931035-0.2311925j   0.86687151+0.31529496j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [ 0.86687151+0.31529496j -0.30931035-0.2311925j ]\n",
      "RESULT: False\n",
      "reward till now:  -42.0\n",
      "episode number:  46\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.86687151+0.31529496j -0.30931035-0.2311925j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -42.0\n",
      "episode number:  46\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.70710678+0.j         0.        +0.70710678j]\n",
      "RESULT: False\n",
      "reward till now:  -42.0\n",
      "episode number:  46\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.70710678+0.j         0.        +0.70710678j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -42.0\n",
      "episode number:  46\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.+0.j 0.+1.j]\n",
      "RESULT: False\n",
      "reward till now:  -43.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 47 reward -43.00, Last 30ep Avg. rewards -43.00.\n",
      "episode number:  47\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.05371855+0.18088983j  0.5055954 -0.8418827j ]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.05371855+0.18088983j  0.5055954 -0.8418827j ]\n",
      "RESULT: False\n",
      "reward till now:  -43.0\n",
      "episode number:  47\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.05371855+0.18088983j  0.5055954 -0.8418827j ]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.63328572-0.22960151j -0.48541836+0.55731621j]\n",
      "RESULT: False\n",
      "reward till now:  -43.0\n",
      "episode number:  47\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.63328572-0.22960151j -0.48541836+0.55731621j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.63328572-0.22960151j -0.48541836+0.55731621j]\n",
      "RESULT: False\n",
      "reward till now:  -43.0\n",
      "episode number:  47\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.63328572-0.22960151j -0.48541836+0.55731621j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.05371855+0.18088983j  0.5055954 -0.8418827j ]\n",
      "RESULT: False\n",
      "reward till now:  -43.0\n",
      "episode number:  47\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.05371855+0.18088983j  0.5055954 -0.8418827j ]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.63328572-0.22960151j -0.48541836+0.55731621j]\n",
      "RESULT: False\n",
      "reward till now:  -44.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 48 reward -44.00, Last 30ep Avg. rewards -44.00.\n",
      "episode number:  48\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.45960333+0.03123216j -0.74605158+0.48082883j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.01500869+0.5496226j  0.50545366-0.66498596j]\n",
      "RESULT: False\n",
      "reward till now:  -44.0\n",
      "episode number:  48\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.01500869+0.5496226j  0.50545366-0.66498596j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.45960333+0.03123216j -0.74605158+0.48082883j]\n",
      "RESULT: False\n",
      "reward till now:  -44.0\n",
      "episode number:  48\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.45960333+0.03123216j -0.74605158+0.48082883j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.85252677+0.3620818j   0.2025495 -0.31791285j]\n",
      "RESULT: False\n",
      "reward till now:  -44.0\n",
      "episode number:  48\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.85252677+0.3620818j   0.2025495 -0.31791285j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.82762579+0.11280637j -0.39925462-0.37802912j]\n",
      "RESULT: False\n",
      "reward till now:  -44.0\n",
      "episode number:  48\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.82762579+0.11280637j -0.39925462-0.37802912j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.39925462-0.37802912j -0.82762579+0.11280637j]\n",
      "RESULT: False\n",
      "reward till now:  -45.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 49 reward -45.00, Last 30ep Avg. rewards -45.00.\n",
      "episode number:  49\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.58601274-0.45247392j -0.13890825-0.65769364j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.31615061-0.78500701j 0.51259655+0.14511225j]\n",
      "RESULT: False\n",
      "reward till now:  -45.0\n",
      "episode number:  49\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.31615061-0.78500701j 0.51259655+0.14511225j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.3261621 -0.91754427j 0.19262328+0.12094238j]\n",
      "RESULT: False\n",
      "reward till now:  -45.0\n",
      "episode number:  49\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.3261621 -0.91754427j 0.19262328+0.12094238j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.3261621 -0.91754427j 0.19262328+0.12094238j]\n",
      "RESULT: False\n",
      "reward till now:  -45.0\n",
      "episode number:  49\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.3261621 -0.91754427j 0.19262328+0.12094238j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.31615061-0.78500701j 0.51259655+0.14511225j]\n",
      "RESULT: False\n",
      "reward till now:  -45.0\n",
      "episode number:  49\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.31615061-0.78500701j 0.51259655+0.14511225j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.3261621 -0.91754427j 0.19262328+0.12094238j]\n",
      "RESULT: False\n",
      "reward till now:  -46.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 50 reward -46.00, Last 30ep Avg. rewards -46.00.\n",
      "episode number:  50\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.17047503+0.2373636j  0.3488348 +0.89045554j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.7501912 -0.07882204j -0.41450486-0.5091031j ]\n",
      "RESULT: False\n",
      "reward till now:  -46.0\n",
      "episode number:  50\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.7501912 -0.07882204j -0.41450486-0.5091031j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -46.0\n",
      "episode number:  50\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -46.0\n",
      "episode number:  50\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.        +0.70710678j 0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -46.0\n",
      "episode number:  50\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.        +0.70710678j 0.70710678+0.j        ]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.+0.j -1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -47.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 51 reward -47.00, Last 30ep Avg. rewards -47.00.\n",
      "episode number:  51\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.55030913-0.7513327j  -0.35697034+0.07232719j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.64154346-0.4801294j  -0.13671117-0.58241549j]\n",
      "RESULT: False\n",
      "reward till now:  -47.0\n",
      "episode number:  51\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.64154346-0.4801294j  -0.13671117-0.58241549j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.58241549+0.13671117j  0.4801294 -0.64154346j]\n",
      "RESULT: False\n",
      "reward till now:  -47.0\n",
      "episode number:  51\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.58241549+0.13671117j  0.4801294 -0.64154346j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.64154346-0.4801294j  -0.13671117-0.58241549j]\n",
      "RESULT: False\n",
      "reward till now:  -47.0\n",
      "episode number:  51\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.64154346-0.4801294j  -0.13671117-0.58241549j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.55030913-0.7513327j  -0.35697034+0.07232719j]\n",
      "RESULT: False\n",
      "reward till now:  -47.0\n",
      "episode number:  51\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.55030913-0.7513327j  -0.35697034+0.07232719j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.55030913-0.7513327j  -0.35697034+0.07232719j]\n",
      "RESULT: False\n",
      "reward till now:  -48.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 52 reward -48.00, Last 30ep Avg. rewards -48.00.\n",
      "episode number:  52\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.67537301-0.33030216j 0.18509586+0.63285963j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.60844337+0.21394044j 0.34667829-0.68105823j]\n",
      "RESULT: False\n",
      "reward till now:  -48.0\n",
      "episode number:  52\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.60844337+0.21394044j 0.34667829-0.68105823j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -48.0\n",
      "episode number:  52\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -48.0\n",
      "episode number:  52\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -48.0\n",
      "episode number:  52\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -47.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 53 reward -47.00, Last 30ep Avg. rewards -47.00.\n",
      "episode number:  53\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.38521041+0.16092389j -0.85990388+0.29373756j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.38521041+0.16092389j -0.85990388+0.29373756j]\n",
      "RESULT: False\n",
      "reward till now:  -47.0\n",
      "episode number:  53\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.38521041+0.16092389j -0.85990388+0.29373756j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -47.0\n",
      "episode number:  53\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -47.0\n",
      "episode number:  53\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -47.0\n",
      "episode number:  53\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.        +0.70710678j 0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -48.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 54 reward -48.00, Last 30ep Avg. rewards -48.00.\n",
      "episode number:  54\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.29601161-0.03083357j -0.95253284-0.06409061j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.16399291+0.65173981j 0.69534506+0.25463072j]\n",
      "RESULT: False\n",
      "reward till now:  -48.0\n",
      "episode number:  54\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.16399291+0.65173981j 0.69534506+0.25463072j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.69534506+0.25463072j 0.16399291+0.65173981j]\n",
      "RESULT: False\n",
      "reward till now:  -48.0\n",
      "episode number:  54\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.69534506+0.25463072j 0.16399291+0.65173981j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -48.0\n",
      "episode number:  54\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -48.0\n",
      "episode number:  54\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.        +0.70710678j 0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -49.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 55 reward -49.00, Last 30ep Avg. rewards -49.00.\n",
      "episode number:  55\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.13378139-0.86303021j -0.02176961+0.48662869j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.02176961+0.48662869j  0.13378139-0.86303021j]\n",
      "RESULT: False\n",
      "reward till now:  -49.0\n",
      "episode number:  55\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.02176961+0.48662869j  0.13378139-0.86303021j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.07920429-0.26615606j -0.10999116+0.95435296j]\n",
      "RESULT: False\n",
      "reward till now:  -49.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode number:  55\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.07920429-0.26615606j -0.10999116+0.95435296j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -49.0\n",
      "episode number:  55\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -49.0\n",
      "episode number:  55\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.        +0.70710678j 0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -50.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 56 reward -50.00, Last 30ep Avg. rewards -50.00.\n",
      "episode number:  56\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.79489188+0.26186099j -0.47949952-0.2639241j ]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.2639241 +0.47949952j -0.26186099-0.79489188j]\n",
      "RESULT: False\n",
      "reward till now:  -50.0\n",
      "episode number:  56\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.2639241 +0.47949952j -0.26186099-0.79489188j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.26186099-0.79489188j -0.2639241 +0.47949952j]\n",
      "RESULT: False\n",
      "reward till now:  -50.0\n",
      "episode number:  56\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.26186099-0.79489188j -0.2639241 +0.47949952j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.26186099-0.79489188j -0.2639241 +0.47949952j]\n",
      "RESULT: False\n",
      "reward till now:  -50.0\n",
      "episode number:  56\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.26186099-0.79489188j -0.2639241 +0.47949952j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.47949952+0.2639241j  0.79489188-0.26186099j]\n",
      "RESULT: False\n",
      "reward till now:  -50.0\n",
      "episode number:  56\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.47949952+0.2639241j  0.79489188-0.26186099j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.9011308 +0.00145884j -0.22301608+0.3717862j ]\n",
      "RESULT: False\n",
      "reward till now:  -51.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 57 reward -51.00, Last 30ep Avg. rewards -51.00.\n",
      "episode number:  57\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.04951961-0.37477387j 0.5321211 +0.75758794j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.41128209+0.27069043j -0.34125079-0.80070072j]\n",
      "RESULT: False\n",
      "reward till now:  -51.0\n",
      "episode number:  57\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.41128209+0.27069043j -0.34125079-0.80070072j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.27536055+0.43270778j  0.04989371+0.85700126j]\n",
      "RESULT: False\n",
      "reward till now:  -51.0\n",
      "episode number:  57\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.27536055+0.43270778j  0.04989371+0.85700126j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [ 0.85700126-0.04989371j -0.43270778-0.27536055j]\n",
      "RESULT: False\n",
      "reward till now:  -51.0\n",
      "episode number:  57\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.85700126-0.04989371j -0.43270778-0.27536055j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.30002079-0.22998949j 0.91196201+0.15942914j]\n",
      "RESULT: False\n",
      "reward till now:  -51.0\n",
      "episode number:  57\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.30002079-0.22998949j 0.91196201+0.15942914j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.85700126-0.04989371j -0.43270778-0.27536055j]\n",
      "RESULT: False\n",
      "reward till now:  -52.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 58 reward -52.00, Last 30ep Avg. rewards -52.00.\n",
      "episode number:  58\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.18460546+0.6335541j  0.20309767+0.7233819j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.01307596+0.95949864j -0.27414751-0.06351785j]\n",
      "RESULT: False\n",
      "reward till now:  -52.0\n",
      "episode number:  58\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.01307596+0.95949864j -0.27414751-0.06351785j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.06351785+0.27414751j -0.95949864+0.01307596j]\n",
      "RESULT: False\n",
      "reward till now:  -52.0\n",
      "episode number:  58\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.06351785+0.27414751j -0.95949864+0.01307596j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [ 0.01307596+0.95949864j -0.27414751-0.06351785j]\n",
      "RESULT: False\n",
      "reward till now:  -52.0\n",
      "episode number:  58\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.01307596+0.95949864j -0.27414751-0.06351785j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.06351785+0.27414751j -0.95949864+0.01307596j]\n",
      "RESULT: False\n",
      "reward till now:  -52.0\n",
      "episode number:  58\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.06351785+0.27414751j -0.95949864+0.01307596j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.7233819+0.20309767j  0.6335541+0.18460546j]\n",
      "RESULT: False\n",
      "reward till now:  -53.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 59 reward -53.00, Last 30ep Avg. rewards -53.00.\n",
      "episode number:  59\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.32270798-0.67826211j 0.63095591-0.19420274j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.19420274-0.63095591j  0.67826211+0.32270798j]\n",
      "RESULT: False\n",
      "reward till now:  -53.0\n",
      "episode number:  59\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.19420274-0.63095591j  0.67826211+0.32270798j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.34228166-0.2179642j  -0.61692581-0.67434221j]\n",
      "RESULT: False\n",
      "reward till now:  -53.0\n",
      "episode number:  59\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.34228166-0.2179642j  -0.61692581-0.67434221j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.67434221+0.61692581j  0.2179642 +0.34228166j]\n",
      "RESULT: False\n",
      "reward till now:  -53.0\n",
      "episode number:  59\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.67434221+0.61692581j  0.2179642 +0.34228166j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.32270798+0.67826211j -0.63095591+0.19420274j]\n",
      "RESULT: False\n",
      "reward till now:  -53.0\n",
      "episode number:  59\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.32270798+0.67826211j -0.63095591+0.19420274j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.67434221+0.61692581j  0.2179642 +0.34228166j]\n",
      "RESULT: False\n",
      "reward till now:  -54.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 60 reward -54.00, Last 30ep Avg. rewards -54.00.\n",
      "episode number:  60\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.2790528 +0.29752242j -0.60198148-0.68646066j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.60198148-0.68646066j -0.2790528 +0.29752242j]\n",
      "RESULT: False\n",
      "reward till now:  -54.0\n",
      "episode number:  60\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.60198148-0.68646066j -0.2790528 +0.29752242j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.29752242+0.2790528j  0.68646066-0.60198148j]\n",
      "RESULT: False\n",
      "reward till now:  -54.0\n",
      "episode number:  60\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.29752242+0.2790528j  0.68646066-0.60198148j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.29752242+0.2790528j  0.68646066-0.60198148j]\n",
      "RESULT: False\n",
      "reward till now:  -54.0\n",
      "episode number:  60\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.29752242+0.2790528j  0.68646066-0.60198148j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.21528506-0.28808086j -0.68272112+0.63604531j]\n",
      "RESULT: False\n",
      "reward till now:  -54.0\n",
      "episode number:  60\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.21528506-0.28808086j -0.68272112+0.63604531j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.63604531+0.68272112j 0.28808086-0.21528506j]\n",
      "RESULT: False\n",
      "reward till now:  -55.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 61 reward -55.00, Last 30ep Avg. rewards -55.00.\n",
      "episode number:  61\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.37579907+0.7338712j  -0.54473674+0.15319924j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [ 0.15319924+0.54473674j -0.7338712 -0.37579907j]\n",
      "RESULT: False\n",
      "reward till now:  -55.0\n",
      "episode number:  61\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.15319924+0.54473674j -0.7338712 -0.37579907j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.41059708+0.11945697j  0.62725352+0.65091711j]\n",
      "RESULT: False\n",
      "reward till now:  -55.0\n",
      "episode number:  61\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.41059708+0.11945697j  0.62725352+0.65091711j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.41059708+0.11945697j  0.62725352+0.65091711j]\n",
      "RESULT: False\n",
      "reward till now:  -55.0\n",
      "episode number:  61\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.41059708+0.11945697j  0.62725352+0.65091711j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.41059708+0.11945697j  0.62725352+0.65091711j]\n",
      "RESULT: False\n",
      "reward till now:  -55.0\n",
      "episode number:  61\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.41059708+0.11945697j  0.62725352+0.65091711j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.41059708+0.11945697j  0.62725352+0.65091711j]\n",
      "RESULT: False\n",
      "reward till now:  -56.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 62 reward -56.00, Last 30ep Avg. rewards -56.00.\n",
      "episode number:  62\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.23248737+0.91758928j -0.0332918 +0.32073538j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.0332918 +0.32073538j  0.23248737+0.91758928j]\n",
      "RESULT: False\n",
      "reward till now:  -56.0\n",
      "episode number:  62\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.0332918 +0.32073538j  0.23248737+0.91758928j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [ 0.91758928-0.23248737j -0.32073538-0.0332918j ]\n",
      "RESULT: False\n",
      "reward till now:  -56.0\n",
      "episode number:  62\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.91758928-0.23248737j -0.32073538-0.0332918j ]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [ 0.91758928-0.23248737j -0.32073538-0.0332918j ]\n",
      "RESULT: False\n",
      "reward till now:  -56.0\n",
      "episode number:  62\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.91758928-0.23248737j -0.32073538-0.0332918j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.42203945-0.18793426j 0.87562777-0.14085254j]\n",
      "RESULT: False\n",
      "reward till now:  -56.0\n",
      "episode number:  62\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.42203945-0.18793426j 0.87562777-0.14085254j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.42203945-0.18793426j 0.87562777-0.14085254j]\n",
      "RESULT: False\n",
      "reward till now:  -57.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 63 reward -57.00, Last 30ep Avg. rewards -57.00.\n",
      "episode number:  63\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.05559109-0.79466974j -0.2600561 +0.54569264j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.54569264+0.2600561j  0.79466974-0.05559109j]\n",
      "RESULT: False\n",
      "reward till now:  -57.0\n",
      "episode number:  63\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.54569264+0.2600561j  0.79466974-0.05559109j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.34655413-0.37802894j -0.74580379+0.4251718j ]\n",
      "RESULT: False\n",
      "reward till now:  -57.0\n",
      "episode number:  63\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.34655413-0.37802894j -0.74580379+0.4251718j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.28231214+0.03333504j  0.7724137 -0.56794869j]\n",
      "RESULT: False\n",
      "reward till now:  -57.0\n",
      "episode number:  63\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.28231214+0.03333504j  0.7724137 -0.56794869j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.34655413-0.37802894j -0.74580379+0.4251718j ]\n",
      "RESULT: False\n",
      "reward till now:  -57.0\n",
      "episode number:  63\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.34655413-0.37802894j -0.74580379+0.4251718j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.28231214+0.03333504j  0.7724137 -0.56794869j]\n",
      "RESULT: False\n",
      "reward till now:  -58.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 64 reward -58.00, Last 30ep Avg. rewards -58.00.\n",
      "episode number:  64\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.59511406+0.3845691j   0.57080125-0.41488769j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.59511406+0.3845691j   0.57080125-0.41488769j]\n",
      "RESULT: False\n",
      "reward till now:  -58.0\n",
      "episode number:  64\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.59511406+0.3845691j   0.57080125-0.41488769j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.59511406+0.3845691j   0.57080125-0.41488769j]\n",
      "RESULT: False\n",
      "reward till now:  -58.0\n",
      "episode number:  64\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.59511406+0.3845691j   0.57080125-0.41488769j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.59511406+0.3845691j   0.57080125-0.41488769j]\n",
      "RESULT: False\n",
      "reward till now:  -58.0\n",
      "episode number:  64\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.59511406+0.3845691j   0.57080125-0.41488769j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.01719176-0.02143848j -0.82442662+0.56530132j]\n",
      "RESULT: False\n",
      "reward till now:  -58.0\n",
      "episode number:  64\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.01719176-0.02143848j -0.82442662+0.56530132j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.59511406+0.3845691j   0.57080125-0.41488769j]\n",
      "RESULT: False\n",
      "reward till now:  -59.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 65 reward -59.00, Last 30ep Avg. rewards -59.00.\n",
      "episode number:  65\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.69455394+0.11505393j -0.24852816+0.66527526j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.96154445+0.25709136j 0.09438053+0.02070315j]\n",
      "RESULT: False\n",
      "reward till now:  -59.0\n",
      "episode number:  65\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.96154445+0.25709136j 0.09438053+0.02070315j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.69455394+0.11505393j -0.24852816+0.66527526j]\n",
      "RESULT: False\n",
      "reward till now:  -59.0\n",
      "episode number:  65\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.69455394+0.11505393j -0.24852816+0.66527526j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -59.0\n",
      "episode number:  65\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -59.0\n",
      "episode number:  65\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.5-0.5j  0.5-0.5j]\n",
      "RESULT: False\n",
      "reward till now:  -60.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 66 reward -60.00, Last 30ep Avg. rewards -60.00.\n",
      "episode number:  66\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.18216109-0.59781277j -0.49252979+0.60568279j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.60568279+0.49252979j 0.59781277-0.18216109j]\n",
      "RESULT: False\n",
      "reward till now:  -60.0\n",
      "episode number:  66\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.60568279+0.49252979j 0.59781277-0.18216109j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.29947507-0.07444631j -0.77098861+0.55708975j]\n",
      "RESULT: False\n",
      "reward till now:  -60.0\n",
      "episode number:  66\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.29947507-0.07444631j -0.77098861+0.55708975j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.33341043+0.34128045j  0.75693213-0.44656343j]\n",
      "RESULT: False\n",
      "reward till now:  -60.0\n",
      "episode number:  66\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.33341043+0.34128045j  0.75693213-0.44656343j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [ 0.75693213-0.44656343j -0.33341043+0.34128045j]\n",
      "RESULT: False\n",
      "reward till now:  -60.0\n",
      "episode number:  66\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.75693213-0.44656343j -0.33341043+0.34128045j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -61.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 67 reward -61.00, Last 30ep Avg. rewards -61.00.\n",
      "episode number:  67\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.81307773-0.31758887j  0.47101867-0.12721374j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.66488647-0.55762974j -0.10849125-0.48497908j]\n",
      "RESULT: False\n",
      "reward till now:  -61.0\n",
      "episode number:  67\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.66488647-0.55762974j -0.10849125-0.48497908j]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.66488647-0.55762974j -0.10849125-0.48497908j]\n",
      "RESULT: False\n",
      "reward till now:  -61.0\n",
      "episode number:  67\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.66488647-0.55762974j -0.10849125-0.48497908j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -61.0\n",
      "episode number:  67\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.70710678+0.j         0.        +0.70710678j]\n",
      "RESULT: False\n",
      "reward till now:  -61.0\n",
      "episode number:  67\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.70710678+0.j         0.        +0.70710678j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.5+0.5j 0.5-0.5j]\n",
      "RESULT: False\n",
      "reward till now:  -62.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 68 reward -62.00, Last 30ep Avg. rewards -62.00.\n",
      "episode number:  68\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.10798342+0.72123407j 0.54338063-0.41581063j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.2176667 +0.12576137j -0.89421763+0.37037833j]\n",
      "RESULT: False\n",
      "reward till now:  -62.0\n",
      "episode number:  68\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.2176667 +0.12576137j -0.89421763+0.37037833j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [ 0.37037833+0.89421763j -0.12576137-0.2176667j ]\n",
      "RESULT: False\n",
      "reward till now:  -62.0\n",
      "episode number:  68\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.37037833+0.89421763j -0.12576137-0.2176667j ]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [ 0.37037833+0.89421763j -0.12576137-0.2176667j ]\n",
      "RESULT: False\n",
      "reward till now:  -62.0\n",
      "episode number:  68\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.37037833+0.89421763j -0.12576137-0.2176667j ]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.12576137-0.2176667j   0.37037833+0.89421763j]\n",
      "RESULT: False\n",
      "reward till now:  -62.0\n",
      "episode number:  68\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.12576137-0.2176667j   0.37037833+0.89421763j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [ 0.37037833+0.89421763j -0.12576137-0.2176667j ]\n",
      "RESULT: False\n",
      "reward till now:  -63.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 69 reward -63.00, Last 30ep Avg. rewards -63.00.\n",
      "episode number:  69\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.51807136+0.31708663j -0.73797052+0.29403682j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -63.0\n",
      "episode number:  69\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -63.0\n",
      "episode number:  69\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -63.0\n",
      "episode number:  69\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.-1.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -63.0\n",
      "episode number:  69\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.-1.j 0.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.+0.j 0.-1.j]\n",
      "RESULT: False\n",
      "reward till now:  -64.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 70 reward -64.00, Last 30ep Avg. rewards -64.00.\n",
      "episode number:  70\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.69314884+0.30083992j 0.15069999+0.63743983j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.69314884+0.30083992j 0.15069999+0.63743983j]\n",
      "RESULT: False\n",
      "reward till now:  -64.0\n",
      "episode number:  70\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.69314884+0.30083992j 0.15069999+0.63743983j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.15069999+0.63743983j 0.69314884+0.30083992j]\n",
      "RESULT: False\n",
      "reward till now:  -64.0\n",
      "episode number:  70\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.15069999+0.63743983j 0.69314884+0.30083992j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.31928693-0.03939222j -0.94086827-0.10616496j]\n",
      "RESULT: False\n",
      "reward till now:  -64.0\n",
      "episode number:  70\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.31928693-0.03939222j -0.94086827-0.10616496j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.15069999+0.63743983j 0.69314884+0.30083992j]\n",
      "RESULT: False\n",
      "reward till now:  -64.0\n",
      "episode number:  70\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.15069999+0.63743983j 0.69314884+0.30083992j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.69314884+0.30083992j 0.15069999+0.63743983j]\n",
      "RESULT: False\n",
      "reward till now:  -65.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 71 reward -65.00, Last 30ep Avg. rewards -65.00.\n",
      "episode number:  71\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.94856751-0.15529889j -0.20946131+0.17952131j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -65.0\n",
      "episode number:  71\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -65.0\n",
      "episode number:  71\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -65.0\n",
      "episode number:  71\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -65.0\n",
      "episode number:  71\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.5-0.5j  0.5-0.5j]\n",
      "RESULT: False\n",
      "reward till now:  -66.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 72 reward -66.00, Last 30ep Avg. rewards -66.00.\n",
      "episode number:  72\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.0556535 -0.03307458j  0.96923667-0.23746375j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.23746375-0.96923667j  0.03307458-0.0556535j ]\n",
      "RESULT: False\n",
      "reward till now:  -66.0\n",
      "episode number:  72\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.23746375-0.96923667j  0.03307458-0.0556535j ]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.0556535 -0.03307458j  0.96923667-0.23746375j]\n",
      "RESULT: False\n",
      "reward till now:  -66.0\n",
      "episode number:  72\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.0556535 -0.03307458j  0.96923667-0.23746375j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -66.0\n",
      "episode number:  72\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -66.0\n",
      "episode number:  72\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -67.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 73 reward -67.00, Last 30ep Avg. rewards -67.00.\n",
      "episode number:  73\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.14180095+0.15027553j -0.80838727+0.55119849j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.67188454+0.49601704j  0.47134771-0.28349534j]\n",
      "RESULT: False\n",
      "reward till now:  -67.0\n",
      "episode number:  73\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.67188454+0.49601704j  0.47134771-0.28349534j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.14180095+0.15027553j -0.80838727+0.55119849j]\n",
      "RESULT: False\n",
      "reward till now:  -67.0\n",
      "episode number:  73\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.14180095+0.15027553j -0.80838727+0.55119849j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.67188454+0.49601704j  0.47134771-0.28349534j]\n",
      "RESULT: False\n",
      "reward till now:  -67.0\n",
      "episode number:  73\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.67188454+0.49601704j  0.47134771-0.28349534j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [ 0.47134771-0.28349534j -0.67188454+0.49601704j]\n",
      "RESULT: False\n",
      "reward till now:  -67.0\n",
      "episode number:  73\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.47134771-0.28349534j -0.67188454+0.49601704j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -68.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 74 reward -68.00, Last 30ep Avg. rewards -68.00.\n",
      "episode number:  74\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.40940428+0.01704683j 0.38840509-0.82537206j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.40940428+0.01704683j 0.38840509-0.82537206j]\n",
      "RESULT: False\n",
      "reward till now:  -68.0\n",
      "episode number:  74\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.40940428+0.01704683j 0.38840509-0.82537206j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.40940428+0.01704683j 0.38840509-0.82537206j]\n",
      "RESULT: False\n",
      "reward till now:  -68.0\n",
      "episode number:  74\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.40940428+0.01704683j 0.38840509-0.82537206j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.82537206-0.38840509j -0.01704683+0.40940428j]\n",
      "RESULT: False\n",
      "reward till now:  -68.0\n",
      "episode number:  74\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.82537206-0.38840509j -0.01704683+0.40940428j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.40940428+0.01704683j 0.38840509-0.82537206j]\n",
      "RESULT: False\n",
      "reward till now:  -68.0\n",
      "episode number:  74\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.40940428+0.01704683j 0.38840509-0.82537206j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.40940428+0.01704683j 0.38840509-0.82537206j]\n",
      "RESULT: False\n",
      "reward till now:  -69.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 75 reward -69.00, Last 30ep Avg. rewards -69.00.\n",
      "episode number:  75\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.49140246+0.75056777j -0.31540645+0.30933868j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -69.0\n",
      "episode number:  75\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -69.0\n",
      "episode number:  75\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -69.0\n",
      "episode number:  75\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -69.0\n",
      "episode number:  75\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -68.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 76 reward -68.00, Last 30ep Avg. rewards -68.00.\n",
      "episode number:  76\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.69060539+0.68516357j 0.177853  -0.148268j  ]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.177853  -0.148268j   0.69060539+0.68516357j]\n",
      "RESULT: False\n",
      "reward till now:  -68.0\n",
      "episode number:  76\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.177853  -0.148268j   0.69060539+0.68516357j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.177853  -0.148268j   0.69060539+0.68516357j]\n",
      "RESULT: False\n",
      "reward till now:  -68.0\n",
      "episode number:  76\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.177853  -0.148268j   0.69060539+0.68516357j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.177853  -0.148268j   0.69060539+0.68516357j]\n",
      "RESULT: False\n",
      "reward till now:  -68.0\n",
      "episode number:  76\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.177853  -0.148268j   0.69060539+0.68516357j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.61024486-0.59317306j -0.38349045-0.35872274j]\n",
      "RESULT: False\n",
      "reward till now:  -68.0\n",
      "episode number:  76\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.61024486-0.59317306j -0.38349045-0.35872274j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.16033959-0.67309198j 0.70267698-0.16578141j]\n",
      "RESULT: False\n",
      "reward till now:  -69.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 77 reward -69.00, Last 30ep Avg. rewards -69.00.\n",
      "episode number:  77\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.03378441+0.50351066j -0.15124778+0.84997631j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [ 0.03378441+0.50351066j -0.15124778+0.84997631j]\n",
      "RESULT: False\n",
      "reward till now:  -69.0\n",
      "episode number:  77\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.03378441+0.50351066j -0.15124778+0.84997631j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.6249132 +0.46298413j -0.24908748-0.57713483j]\n",
      "RESULT: False\n",
      "reward till now:  -69.0\n",
      "episode number:  77\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.6249132 +0.46298413j -0.24908748-0.57713483j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -69.0\n",
      "episode number:  77\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -69.0\n",
      "episode number:  77\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.70710678+0.j          0.        -0.70710678j]\n",
      "RESULT: False\n",
      "reward till now:  -70.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 78 reward -70.00, Last 30ep Avg. rewards -70.00.\n",
      "episode number:  78\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.00440832-0.95693809j 0.21546407-0.19448724j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.21546407-0.19448724j 0.00440832-0.95693809j]\n",
      "RESULT: False\n",
      "reward till now:  -70.0\n",
      "episode number:  78\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.21546407-0.19448724j 0.00440832-0.95693809j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -70.0\n",
      "episode number:  78\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -70.0\n",
      "episode number:  78\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.5-0.5j  0.5-0.5j]\n",
      "RESULT: False\n",
      "reward till now:  -70.0\n",
      "episode number:  78\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.5-0.5j  0.5-0.5j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.5-0.5j  0.5-0.5j]\n",
      "RESULT: False\n",
      "reward till now:  -71.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 79 reward -71.00, Last 30ep Avg. rewards -71.00.\n",
      "episode number:  79\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.70543515+0.24037345j -0.66195785+0.07996036j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.66195785+0.07996036j  0.70543515+0.24037345j]\n",
      "RESULT: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward till now:  -71.0\n",
      "episode number:  79\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.66195785+0.07996036j  0.70543515+0.24037345j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [ 0.70543515+0.24037345j -0.66195785+0.07996036j]\n",
      "RESULT: False\n",
      "reward till now:  -71.0\n",
      "episode number:  79\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70543515+0.24037345j -0.66195785+0.07996036j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -71.0\n",
      "episode number:  79\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.+0.j 0.+1.j]\n",
      "RESULT: False\n",
      "reward till now:  -71.0\n",
      "episode number:  79\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 0.+1.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.+1.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -72.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 80 reward -72.00, Last 30ep Avg. rewards -72.00.\n",
      "episode number:  80\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.20274968+0.74204159j 0.62840248+0.11565967j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.58771333+0.60648638j -0.30098198+0.4429189j ]\n",
      "RESULT: False\n",
      "reward till now:  -72.0\n",
      "episode number:  80\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.58771333+0.60648638j -0.30098198+0.4429189j ]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [ 0.4429189 +0.30098198j -0.60648638+0.58771333j]\n",
      "RESULT: False\n",
      "reward till now:  -72.0\n",
      "episode number:  80\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.4429189 +0.30098198j -0.60648638+0.58771333j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -72.0\n",
      "episode number:  80\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -72.0\n",
      "episode number:  80\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -73.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 81 reward -73.00, Last 30ep Avg. rewards -73.00.\n",
      "episode number:  81\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.78844754-0.39734464j 0.01431577+0.46932161j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.01431577+0.46932161j 0.78844754-0.39734464j]\n",
      "RESULT: False\n",
      "reward till now:  -73.0\n",
      "episode number:  81\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.01431577+0.46932161j 0.78844754-0.39734464j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.39734464-0.78844754j -0.46932161+0.01431577j]\n",
      "RESULT: False\n",
      "reward till now:  -73.0\n",
      "episode number:  81\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.39734464-0.78844754j -0.46932161+0.01431577j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.61282558-0.54739382j  0.0508954 -0.56763938j]\n",
      "RESULT: False\n",
      "reward till now:  -73.0\n",
      "episode number:  81\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.61282558-0.54739382j  0.0508954 -0.56763938j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.61282558-0.54739382j  0.0508954 -0.56763938j]\n",
      "RESULT: False\n",
      "reward till now:  -73.0\n",
      "episode number:  81\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.61282558-0.54739382j  0.0508954 -0.56763938j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.56763938-0.0508954j   0.54739382-0.61282558j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 82 reward -74.00, Last 30ep Avg. rewards -74.00.\n",
      "episode number:  82\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.28611489-0.66992377j 0.68466807+0.02387557j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.68644722-0.45682507j -0.28181966-0.49059022j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  82\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.68644722-0.45682507j -0.28181966-0.49059022j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  82\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  82\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  82\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 83 reward -75.00, Last 30ep Avg. rewards -75.00.\n",
      "episode number:  83\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.13933346-0.21895606j 0.68512841-0.68061993j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.13933346-0.21895606j 0.68512841-0.68061993j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  83\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.13933346-0.21895606j 0.68512841-0.68061993j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.13933346-0.21895606j 0.68512841-0.68061993j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  83\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.13933346-0.21895606j 0.68512841-0.68061993j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.13933346-0.21895606j 0.68512841-0.68061993j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  83\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.13933346-0.21895606j 0.68512841-0.68061993j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.38274733-0.63928426j -0.32963363+0.5797946j ]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  83\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.38274733-0.63928426j -0.32963363+0.5797946j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.50372941-0.04206554j -0.03755706-0.86201893j]\n",
      "RESULT: False\n",
      "reward till now:  -76.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 84 reward -76.00, Last 30ep Avg. rewards -76.00.\n",
      "episode number:  84\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.77370057+0.48584985j -0.29707904-0.27763535j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -76.0\n",
      "episode number:  84\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -76.0\n",
      "episode number:  84\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.+0.j 0.+1.j]\n",
      "RESULT: False\n",
      "reward till now:  -76.0\n",
      "episode number:  84\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 0.+1.j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -76.0\n",
      "episode number:  84\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -77.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 85 reward -77.00, Last 30ep Avg. rewards -77.00.\n",
      "episode number:  85\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.28766522-0.43309508j -0.74927674+0.41019721j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -77.0\n",
      "episode number:  85\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -77.0\n",
      "episode number:  85\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -77.0\n",
      "episode number:  85\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.70710678+0.j         0.        +0.70710678j]\n",
      "RESULT: False\n",
      "reward till now:  -77.0\n",
      "episode number:  85\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.70710678+0.j         0.        +0.70710678j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.        +0.70710678j 0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -78.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 86 reward -78.00, Last 30ep Avg. rewards -78.00.\n",
      "episode number:  86\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.46618307-0.82483987j  0.00987358+0.31971088j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.10357148-0.59023154j  0.57626819-0.55571094j]\n",
      "RESULT: False\n",
      "reward till now:  -78.0\n",
      "episode number:  86\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.10357148-0.59023154j  0.57626819-0.55571094j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.55571094-0.57626819j  0.59023154-0.10357148j]\n",
      "RESULT: False\n",
      "reward till now:  -78.0\n",
      "episode number:  86\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.55571094-0.57626819j  0.59023154-0.10357148j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.55571094-0.57626819j  0.59023154-0.10357148j]\n",
      "RESULT: False\n",
      "reward till now:  -78.0\n",
      "episode number:  86\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.55571094-0.57626819j  0.59023154-0.10357148j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.55571094-0.57626819j  0.59023154-0.10357148j]\n",
      "RESULT: False\n",
      "reward till now:  -78.0\n",
      "episode number:  86\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.55571094-0.57626819j  0.59023154-0.10357148j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [ 0.59023154-0.10357148j -0.55571094-0.57626819j]\n",
      "RESULT: False\n",
      "reward till now:  -79.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 87 reward -79.00, Last 30ep Avg. rewards -79.00.\n",
      "episode number:  87\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.65543525-0.41461264j -0.23037807-0.58773033j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.23037807-0.58773033j  0.65543525-0.41461264j]\n",
      "RESULT: False\n",
      "reward till now:  -79.0\n",
      "episode number:  87\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.23037807-0.58773033j  0.65543525-0.41461264j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.23037807-0.58773033j  0.65543525-0.41461264j]\n",
      "RESULT: False\n",
      "reward till now:  -79.0\n",
      "episode number:  87\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.23037807-0.58773033j  0.65543525-0.41461264j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.41461264-0.65543525j  0.58773033-0.23037807j]\n",
      "RESULT: False\n",
      "reward till now:  -79.0\n",
      "episode number:  87\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.41461264-0.65543525j  0.58773033-0.23037807j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.41461264-0.65543525j  0.58773033-0.23037807j]\n",
      "RESULT: False\n",
      "reward till now:  -79.0\n",
      "episode number:  87\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.41461264-0.65543525j  0.58773033-0.23037807j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -80.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 88 reward -80.00, Last 30ep Avg. rewards -80.00.\n",
      "episode number:  88\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.57399153+0.0285637j  -0.12835949+0.80823368j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [ 0.80823368+0.12835949j -0.0285637 -0.57399153j]\n",
      "RESULT: False\n",
      "reward till now:  -80.0\n",
      "episode number:  88\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.80823368+0.12835949j -0.0285637 -0.57399153j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.57399153+0.0285637j  -0.12835949+0.80823368j]\n",
      "RESULT: False\n",
      "reward till now:  -80.0\n",
      "episode number:  88\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.57399153+0.0285637j  -0.12835949+0.80823368j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [ 0.80823368+0.12835949j -0.0285637 -0.57399153j]\n",
      "RESULT: False\n",
      "reward till now:  -80.0\n",
      "episode number:  88\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.80823368+0.12835949j -0.0285637 -0.57399153j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [ 0.80823368+0.12835949j -0.0285637 -0.57399153j]\n",
      "RESULT: False\n",
      "reward till now:  -80.0\n",
      "episode number:  88\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.80823368+0.12835949j -0.0285637 -0.57399153j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [ 0.80823368+0.12835949j -0.0285637 -0.57399153j]\n",
      "RESULT: False\n",
      "reward till now:  -81.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 89 reward -81.00, Last 30ep Avg. rewards -81.00.\n",
      "episode number:  89\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.31532174-0.71003271j -0.03409399-0.62869974j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.62869974+0.03409399j  0.71003271-0.31532174j]\n",
      "RESULT: False\n",
      "reward till now:  -81.0\n",
      "episode number:  89\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.62869974+0.03409399j  0.71003271-0.31532174j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.62869974+0.03409399j  0.71003271-0.31532174j]\n",
      "RESULT: False\n",
      "reward till now:  -81.0\n",
      "episode number:  89\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.62869974+0.03409399j  0.71003271-0.31532174j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.62869974+0.03409399j  0.71003271-0.31532174j]\n",
      "RESULT: False\n",
      "reward till now:  -81.0\n",
      "episode number:  89\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.62869974+0.03409399j  0.71003271-0.31532174j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.62869974+0.03409399j  0.71003271-0.31532174j]\n",
      "RESULT: False\n",
      "reward till now:  -81.0\n",
      "episode number:  89\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.62869974+0.03409399j  0.71003271-0.31532174j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.62869974+0.03409399j  0.71003271-0.31532174j]\n",
      "RESULT: False\n",
      "reward till now:  -82.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 90 reward -82.00, Last 30ep Avg. rewards -82.00.\n",
      "episode number:  90\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.26965422+0.60938115j  0.73173356-0.1432034j ]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [ 0.73173356-0.1432034j  -0.26965422+0.60938115j]\n",
      "RESULT: False\n",
      "reward till now:  -82.0\n",
      "episode number:  90\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.73173356-0.1432034j  -0.26965422+0.60938115j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -82.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode number:  90\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -82.0\n",
      "episode number:  90\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -82.0\n",
      "episode number:  90\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -83.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 91 reward -83.00, Last 30ep Avg. rewards -83.00.\n",
      "episode number:  91\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.11330475+0.77886053j  0.60480035+0.12146953j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [ 0.12146953-0.60480035j -0.77886053-0.11330475j]\n",
      "RESULT: False\n",
      "reward till now:  -83.0\n",
      "episode number:  91\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.12146953-0.60480035j -0.77886053-0.11330475j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.00577337+0.12307914j 0.97839599+0.16601049j]\n",
      "RESULT: False\n",
      "reward till now:  -83.0\n",
      "episode number:  91\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.00577337+0.12307914j 0.97839599+0.16601049j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.00577337+0.12307914j 0.97839599+0.16601049j]\n",
      "RESULT: False\n",
      "reward till now:  -83.0\n",
      "episode number:  91\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.00577337+0.12307914j 0.97839599+0.16601049j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.00577337+0.12307914j 0.97839599+0.16601049j]\n",
      "RESULT: False\n",
      "reward till now:  -83.0\n",
      "episode number:  91\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.00577337+0.12307914j 0.97839599+0.16601049j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.00577337+0.12307914j 0.97839599+0.16601049j]\n",
      "RESULT: False\n",
      "reward till now:  -84.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 92 reward -84.00, Last 30ep Avg. rewards -84.00.\n",
      "episode number:  92\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.30190482+0.07686926j -0.90427617-0.29194041j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.85289876-0.15207827j  0.42594086+0.26078781j]\n",
      "RESULT: False\n",
      "reward till now:  -84.0\n",
      "episode number:  92\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.85289876-0.15207827j  0.42594086+0.26078781j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.85289876-0.15207827j  0.42594086+0.26078781j]\n",
      "RESULT: False\n",
      "reward till now:  -84.0\n",
      "episode number:  92\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.85289876-0.15207827j  0.42594086+0.26078781j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.26078781-0.42594086j 0.15207827-0.85289876j]\n",
      "RESULT: False\n",
      "reward till now:  -84.0\n",
      "episode number:  92\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.26078781-0.42594086j 0.15207827-0.85289876j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.15207827-0.85289876j 0.26078781-0.42594086j]\n",
      "RESULT: False\n",
      "reward till now:  -84.0\n",
      "episode number:  92\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.15207827-0.85289876j 0.26078781-0.42594086j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.26078781-0.42594086j 0.15207827-0.85289876j]\n",
      "RESULT: False\n",
      "reward till now:  -85.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 93 reward -85.00, Last 30ep Avg. rewards -85.00.\n",
      "episode number:  93\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.2340573 +0.33310276j -0.43415438-0.80359797j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.47249701-0.33269035j  0.14149   +0.8037688j ]\n",
      "RESULT: False\n",
      "reward till now:  -85.0\n",
      "episode number:  93\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.47249701-0.33269035j  0.14149   +0.8037688j ]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.8037688 -0.14149j    0.33269035-0.47249701j]\n",
      "RESULT: False\n",
      "reward till now:  -85.0\n",
      "episode number:  93\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.8037688 -0.14149j    0.33269035-0.47249701j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.33269035-0.47249701j 0.8037688 -0.14149j   ]\n",
      "RESULT: False\n",
      "reward till now:  -85.0\n",
      "episode number:  93\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.33269035-0.47249701j 0.8037688 -0.14149j   ]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.33269035-0.47249701j 0.8037688 -0.14149j   ]\n",
      "RESULT: False\n",
      "reward till now:  -85.0\n",
      "episode number:  93\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.33269035-0.47249701j 0.8037688 -0.14149j   ]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.33269035-0.47249701j 0.8037688 -0.14149j   ]\n",
      "RESULT: False\n",
      "reward till now:  -86.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 94 reward -86.00, Last 30ep Avg. rewards -86.00.\n",
      "episode number:  94\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.01929536-0.15496375j 0.26731727-0.95087086j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -86.0\n",
      "episode number:  94\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.-1.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -86.0\n",
      "episode number:  94\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.-1.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.-0.70710678j 0.-0.70710678j]\n",
      "RESULT: False\n",
      "reward till now:  -86.0\n",
      "episode number:  94\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.-0.70710678j 0.-0.70710678j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -86.0\n",
      "episode number:  94\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -87.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 95 reward -87.00, Last 30ep Avg. rewards -87.00.\n",
      "episode number:  95\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.35617925+0.18164478j 0.87109847+0.28518236j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.8678164 +0.33009663j -0.36410287-0.07321213j]\n",
      "RESULT: False\n",
      "reward till now:  -87.0\n",
      "episode number:  95\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.8678164 +0.33009663j -0.36410287-0.07321213j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.35617925+0.18164478j 0.87109847+0.28518236j]\n",
      "RESULT: False\n",
      "reward till now:  -87.0\n",
      "episode number:  95\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.35617925+0.18164478j 0.87109847+0.28518236j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.8678164 +0.33009663j -0.36410287-0.07321213j]\n",
      "RESULT: False\n",
      "reward till now:  -87.0\n",
      "episode number:  95\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.8678164 +0.33009663j -0.36410287-0.07321213j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -87.0\n",
      "episode number:  95\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.+0.j 0.+1.j]\n",
      "RESULT: False\n",
      "reward till now:  -88.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 96 reward -88.00, Last 30ep Avg. rewards -88.00.\n",
      "episode number:  96\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.2430852 -0.07352092j -0.90085354-0.3520897j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.80888684-0.30095216j  0.46511246+0.19697788j]\n",
      "RESULT: False\n",
      "reward till now:  -88.0\n",
      "episode number:  96\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.80888684-0.30095216j  0.46511246+0.19697788j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.43268498-0.54168948j -0.11607886-0.71125376j]\n",
      "RESULT: False\n",
      "reward till now:  -88.0\n",
      "episode number:  96\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.43268498-0.54168948j -0.11607886-0.71125376j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.38803463-0.88596466j -0.22387433+0.11990005j]\n",
      "RESULT: False\n",
      "reward till now:  -88.0\n",
      "episode number:  96\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.38803463-0.88596466j -0.22387433+0.11990005j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.11990005+0.22387433j 0.88596466-0.38803463j]\n",
      "RESULT: False\n",
      "reward till now:  -88.0\n",
      "episode number:  96\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.11990005+0.22387433j 0.88596466-0.38803463j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.11990005+0.22387433j 0.88596466-0.38803463j]\n",
      "RESULT: False\n",
      "reward till now:  -89.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 97 reward -89.00, Last 30ep Avg. rewards -89.00.\n",
      "episode number:  97\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.49830552+0.27970341j -0.25089509-0.78135092j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.52976483-0.35471836j -0.17494559+0.75027872j]\n",
      "RESULT: False\n",
      "reward till now:  -89.0\n",
      "episode number:  97\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.52976483-0.35471836j -0.17494559+0.75027872j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -89.0\n",
      "episode number:  97\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -88.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 98 reward -88.00, Last 30ep Avg. rewards -88.00.\n",
      "episode number:  98\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.60550024+0.75385649j 0.04889927+0.25035716j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.04889927+0.25035716j 0.60550024+0.75385649j]\n",
      "RESULT: False\n",
      "reward till now:  -88.0\n",
      "episode number:  98\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.04889927+0.25035716j 0.60550024+0.75385649j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -88.0\n",
      "episode number:  98\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -88.0\n",
      "episode number:  98\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -88.0\n",
      "episode number:  98\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -89.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 99 reward -89.00, Last 30ep Avg. rewards -89.00.\n",
      "episode number:  99\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.38501412-0.64414304j -0.59774322-0.28204061j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.15042219-0.65491074j  0.69491438-0.25604509j]\n",
      "RESULT: False\n",
      "reward till now:  -89.0\n",
      "episode number:  99\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.15042219-0.65491074j  0.69491438-0.25604509j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.25604509-0.69491438j  0.65491074-0.15042219j]\n",
      "RESULT: False\n",
      "reward till now:  -89.0\n",
      "episode number:  99\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.25604509-0.69491438j  0.65491074-0.15042219j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.15042219-0.65491074j  0.69491438-0.25604509j]\n",
      "RESULT: False\n",
      "reward till now:  -89.0\n",
      "episode number:  99\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.15042219-0.65491074j  0.69491438-0.25604509j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.25604509-0.69491438j  0.65491074-0.15042219j]\n",
      "RESULT: False\n",
      "reward till now:  -89.0\n",
      "episode number:  99\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.25604509-0.69491438j  0.65491074-0.15042219j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.15042219-0.65491074j  0.69491438-0.25604509j]\n",
      "RESULT: False\n",
      "reward till now:  -90.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 100 reward -90.00, Last 30ep Avg. rewards -90.00.\n",
      "episode number:  100\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.04490647+0.99486778j -0.07207454-0.05501611j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -90.0\n",
      "episode number:  100\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -90.0\n",
      "episode number:  100\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -90.0\n",
      "episode number:  100\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -90.0\n",
      "episode number:  100\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -91.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 101 reward -91.00, Last 30ep Avg. rewards -91.00.\n",
      "episode number:  101\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.13492345-0.47514472j 0.81352159+0.30694589j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.81352159+0.30694589j 0.13492345-0.47514472j]\n",
      "RESULT: False\n",
      "reward till now:  -91.0\n",
      "episode number:  101\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.81352159+0.30694589j 0.13492345-0.47514472j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.13492345-0.47514472j 0.81352159+0.30694589j]\n",
      "RESULT: False\n",
      "reward till now:  -91.0\n",
      "episode number:  101\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.13492345-0.47514472j 0.81352159+0.30694589j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.81352159+0.30694589j 0.13492345-0.47514472j]\n",
      "RESULT: False\n",
      "reward till now:  -91.0\n",
      "episode number:  101\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.81352159+0.30694589j 0.13492345-0.47514472j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.13492345-0.47514472j 0.81352159+0.30694589j]\n",
      "RESULT: False\n",
      "reward till now:  -91.0\n",
      "episode number:  101\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.13492345-0.47514472j 0.81352159+0.30694589j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.81352159+0.30694589j 0.13492345-0.47514472j]\n",
      "RESULT: False\n",
      "reward till now:  -92.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 102 reward -92.00, Last 30ep Avg. rewards -92.00.\n",
      "episode number:  102\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.35929703-0.18298565j 0.13894901-0.90449714j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.13894901-0.90449714j 0.35929703-0.18298565j]\n",
      "RESULT: False\n",
      "reward till now:  -92.0\n",
      "episode number:  102\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.13894901-0.90449714j 0.35929703-0.18298565j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.35929703-0.18298565j 0.13894901-0.90449714j]\n",
      "RESULT: False\n",
      "reward till now:  -92.0\n",
      "episode number:  102\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.35929703-0.18298565j 0.13894901-0.90449714j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.13894901-0.90449714j 0.35929703-0.18298565j]\n",
      "RESULT: False\n",
      "reward till now:  -92.0\n",
      "episode number:  102\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.13894901-0.90449714j 0.35929703-0.18298565j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.35929703-0.18298565j 0.13894901-0.90449714j]\n",
      "RESULT: False\n",
      "reward till now:  -92.0\n",
      "episode number:  102\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.35929703-0.18298565j 0.13894901-0.90449714j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.13894901-0.90449714j 0.35929703-0.18298565j]\n",
      "RESULT: False\n",
      "reward till now:  -93.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 103 reward -93.00, Last 30ep Avg. rewards -93.00.\n",
      "episode number:  103\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.50648272-0.41761228j 0.35936612+0.66327313j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.35936612+0.66327313j 0.50648272-0.41761228j]\n",
      "RESULT: False\n",
      "reward till now:  -93.0\n",
      "episode number:  103\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.35936612+0.66327313j 0.50648272-0.41761228j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.35936612+0.66327313j 0.50648272-0.41761228j]\n",
      "RESULT: False\n",
      "reward till now:  -93.0\n",
      "episode number:  103\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.35936612+0.66327313j 0.50648272-0.41761228j]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.50648272-0.41761228j 0.35936612+0.66327313j]\n",
      "RESULT: False\n",
      "reward till now:  -93.0\n",
      "episode number:  103\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.50648272-0.41761228j 0.35936612+0.66327313j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.35936612+0.66327313j 0.50648272-0.41761228j]\n",
      "RESULT: False\n",
      "reward till now:  -93.0\n",
      "episode number:  103\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.35936612+0.66327313j 0.50648272-0.41761228j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.61224758+0.17370845j -0.10402715+0.7643014j ]\n",
      "RESULT: False\n",
      "reward till now:  -94.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 104 reward -94.00, Last 30ep Avg. rewards -94.00.\n",
      "episode number:  104\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.65963025-0.38917926j 0.62442001+0.15338544j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.65963025-0.38917926j 0.62442001+0.15338544j]\n",
      "RESULT: False\n",
      "reward till now:  -94.0\n",
      "episode number:  104\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.65963025-0.38917926j 0.62442001+0.15338544j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.65963025-0.38917926j 0.62442001+0.15338544j]\n",
      "RESULT: False\n",
      "reward till now:  -94.0\n",
      "episode number:  104\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.65963025-0.38917926j 0.62442001+0.15338544j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.65963025-0.38917926j 0.62442001+0.15338544j]\n",
      "RESULT: False\n",
      "reward till now:  -94.0\n",
      "episode number:  104\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.65963025-0.38917926j 0.62442001+0.15338544j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.65963025-0.38917926j 0.62442001+0.15338544j]\n",
      "RESULT: False\n",
      "reward till now:  -94.0\n",
      "episode number:  104\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.65963025-0.38917926j 0.62442001+0.15338544j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.65963025-0.38917926j 0.62442001+0.15338544j]\n",
      "RESULT: False\n",
      "reward till now:  -95.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 105 reward -95.00, Last 30ep Avg. rewards -95.00.\n",
      "episode number:  105\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.03300477-0.15904526j -0.80775495-0.56669853j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -95.0\n",
      "episode number:  105\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -95.0\n",
      "episode number:  105\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -95.0\n",
      "episode number:  105\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -95.0\n",
      "episode number:  105\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -96.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 106 reward -96.00, Last 30ep Avg. rewards -96.00.\n",
      "episode number:  106\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.23622089+0.40267554j  0.2832796 -0.83773789j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.83773789-0.2832796j  -0.40267554-0.23622089j]\n",
      "RESULT: False\n",
      "reward till now:  -96.0\n",
      "episode number:  106\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.83773789-0.2832796j  -0.40267554-0.23622089j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.75940353+0.08442567j  0.48504353-0.42533675j]\n",
      "RESULT: False\n",
      "reward till now:  -96.0\n",
      "episode number:  106\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.75940353+0.08442567j  0.48504353-0.42533675j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.83773789-0.2832796j  -0.40267554-0.23622089j]\n",
      "RESULT: False\n",
      "reward till now:  -96.0\n",
      "episode number:  106\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.83773789-0.2832796j  -0.40267554-0.23622089j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.87710475-0.36734232j -0.30763554-0.03327554j]\n",
      "RESULT: False\n",
      "reward till now:  -96.0\n",
      "episode number:  106\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.87710475-0.36734232j -0.30763554-0.03327554j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.03327554+0.30763554j  0.36734232-0.87710475j]\n",
      "RESULT: False\n",
      "reward till now:  -97.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 107 reward -97.00, Last 30ep Avg. rewards -97.00.\n",
      "episode number:  107\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.08827179+0.66790442j 0.53004773-0.51493803j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -97.0\n",
      "episode number:  107\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -97.0\n",
      "episode number:  107\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -97.0\n",
      "episode number:  107\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -97.0\n",
      "episode number:  107\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -98.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 108 reward -98.00, Last 30ep Avg. rewards -98.00.\n",
      "episode number:  108\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.17610017+0.75351562j 0.57887025-0.25712288j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.25712288-0.57887025j -0.75351562+0.17610017j]\n",
      "RESULT: False\n",
      "reward till now:  -98.0\n",
      "episode number:  108\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.25712288-0.57887025j -0.75351562+0.17610017j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.71462933-0.28480146j  0.35100268-0.5338447j ]\n",
      "RESULT: False\n",
      "reward till now:  -98.0\n",
      "episode number:  108\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.71462933-0.28480146j  0.35100268-0.5338447j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -98.0\n",
      "episode number:  108\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -98.0\n",
      "episode number:  108\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -99.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 109 reward -99.00, Last 30ep Avg. rewards -99.00.\n",
      "episode number:  109\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.90436771+0.04678253j -0.19462156+0.37689905j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -99.0\n",
      "episode number:  109\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -99.0\n",
      "episode number:  109\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -99.0\n",
      "episode number:  109\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -99.0\n",
      "episode number:  109\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -100.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 110 reward -100.00, Last 30ep Avg. rewards -100.00.\n",
      "episode number:  110\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.58490336-0.09127794j -0.11369098-0.79789144j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.11369098-0.79789144j  0.58490336-0.09127794j]\n",
      "RESULT: False\n",
      "reward till now:  -100.0\n",
      "episode number:  110\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.11369098-0.79789144j  0.58490336-0.09127794j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [ 0.58490336-0.09127794j -0.11369098-0.79789144j]\n",
      "RESULT: False\n",
      "reward till now:  -100.0\n",
      "episode number:  110\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.58490336-0.09127794j -0.11369098-0.79789144j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.79789144+0.11369098j  0.09127794+0.58490336j]\n",
      "RESULT: False\n",
      "reward till now:  -100.0\n",
      "episode number:  110\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.79789144+0.11369098j  0.09127794+0.58490336j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.79789144+0.11369098j  0.09127794+0.58490336j]\n",
      "RESULT: False\n",
      "reward till now:  -100.0\n",
      "episode number:  110\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.79789144+0.11369098j  0.09127794+0.58490336j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.79789144+0.11369098j  0.09127794+0.58490336j]\n",
      "RESULT: False\n",
      "reward till now:  -101.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 111 reward -101.00, Last 30ep Avg. rewards -101.00.\n",
      "episode number:  111\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.54709771-0.5777579j  -0.39866335+0.45601254j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.39866335+0.45601254j  0.54709771-0.5777579j ]\n",
      "RESULT: False\n",
      "reward till now:  -101.0\n",
      "episode number:  111\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.39866335+0.45601254j  0.54709771-0.5777579j ]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.69043409-0.06440694j -0.70930606+0.12663897j]\n",
      "RESULT: False\n",
      "reward till now:  -101.0\n",
      "episode number:  111\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.69043409-0.06440694j -0.70930606+0.12663897j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.98976575+0.04400469j  0.0133445 -0.13508986j]\n",
      "RESULT: False\n",
      "reward till now:  -101.0\n",
      "episode number:  111\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.98976575+0.04400469j  0.0133445 -0.13508986j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.13508986-0.0133445j  -0.04400469-0.98976575j]\n",
      "RESULT: False\n",
      "reward till now:  -101.0\n",
      "episode number:  111\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.13508986-0.0133445j  -0.04400469-0.98976575j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.04400469-0.98976575j -0.13508986-0.0133445j ]\n",
      "RESULT: False\n",
      "reward till now:  -102.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 112 reward -102.00, Last 30ep Avg. rewards -102.00.\n",
      "episode number:  112\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.40580771-0.17329334j -0.46964736-0.76467043j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.46964736-0.76467043j  0.40580771-0.17329334j]\n",
      "RESULT: False\n",
      "reward till now:  -102.0\n",
      "episode number:  112\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.46964736-0.76467043j  0.40580771-0.17329334j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -102.0\n",
      "episode number:  112\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -102.0\n",
      "episode number:  112\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -102.0\n",
      "episode number:  112\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -103.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 113 reward -103.00, Last 30ep Avg. rewards -103.00.\n",
      "episode number:  113\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.39523911+0.71376796j -0.53461116-0.22025496j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.22025496+0.53461116j -0.71376796-0.39523911j]\n",
      "RESULT: False\n",
      "reward till now:  -103.0\n",
      "episode number:  113\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.22025496+0.53461116j -0.71376796-0.39523911j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.66045394+0.09855093j  0.34896639+0.65750343j]\n",
      "RESULT: False\n",
      "reward till now:  -103.0\n",
      "episode number:  113\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.66045394+0.09855093j  0.34896639+0.65750343j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.66045394+0.09855093j  0.34896639+0.65750343j]\n",
      "RESULT: False\n",
      "reward till now:  -103.0\n",
      "episode number:  113\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.66045394+0.09855093j  0.34896639+0.65750343j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.66045394+0.09855093j  0.34896639+0.65750343j]\n",
      "RESULT: False\n",
      "reward till now:  -103.0\n",
      "episode number:  113\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.66045394+0.09855093j  0.34896639+0.65750343j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.66045394+0.09855093j  0.34896639+0.65750343j]\n",
      "RESULT: False\n",
      "reward till now:  -104.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 114 reward -104.00, Last 30ep Avg. rewards -104.00.\n",
      "episode number:  114\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.95573985+0.11187284j 0.06112826-0.26515871j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.06112826-0.26515871j 0.95573985+0.11187284j]\n",
      "RESULT: False\n",
      "reward till now:  -104.0\n",
      "episode number:  114\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.06112826-0.26515871j 0.95573985+0.11187284j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.95573985+0.11187284j 0.06112826-0.26515871j]\n",
      "RESULT: False\n",
      "reward till now:  -104.0\n",
      "episode number:  114\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.95573985+0.11187284j 0.06112826-0.26515871j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.26515871-0.06112826j -0.11187284+0.95573985j]\n",
      "RESULT: False\n",
      "reward till now:  -104.0\n",
      "episode number:  114\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.26515871-0.06112826j -0.11187284+0.95573985j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -104.0\n",
      "episode number:  114\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -105.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 115 reward -105.00, Last 30ep Avg. rewards -105.00.\n",
      "episode number:  115\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.24420333+0.94564106j 0.18890421-0.10219065j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.18890421-0.10219065j 0.24420333+0.94564106j]\n",
      "RESULT: False\n",
      "reward till now:  -105.0\n",
      "episode number:  115\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.18890421-0.10219065j 0.24420333+0.94564106j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.24420333+0.94564106j 0.18890421-0.10219065j]\n",
      "RESULT: False\n",
      "reward till now:  -105.0\n",
      "episode number:  115\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.24420333+0.94564106j 0.18890421-0.10219065j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.18890421-0.10219065j 0.24420333+0.94564106j]\n",
      "RESULT: False\n",
      "reward till now:  -105.0\n",
      "episode number:  115\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.18890421-0.10219065j 0.24420333+0.94564106j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.24420333+0.94564106j 0.18890421-0.10219065j]\n",
      "RESULT: False\n",
      "reward till now:  -105.0\n",
      "episode number:  115\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.24420333+0.94564106j 0.18890421-0.10219065j]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.10041812+0.53509376j -0.80224465+0.24493753j]\n",
      "RESULT: False\n",
      "reward till now:  -106.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 116 reward -106.00, Last 30ep Avg. rewards -106.00.\n",
      "episode number:  116\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.27744657+0.74629627j 0.56059675-0.22758858j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.59258611+0.36678172j -0.20021742+0.68864058j]\n",
      "RESULT: False\n",
      "reward till now:  -106.0\n",
      "episode number:  116\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.59258611+0.36678172j -0.20021742+0.68864058j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.20021742+0.68864058j  0.59258611+0.36678172j]\n",
      "RESULT: False\n",
      "reward till now:  -106.0\n",
      "episode number:  116\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.20021742+0.68864058j  0.59258611+0.36678172j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [ 0.36678172-0.59258611j -0.68864058-0.20021742j]\n",
      "RESULT: False\n",
      "reward till now:  -106.0\n",
      "episode number:  116\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.36678172-0.59258611j -0.68864058-0.20021742j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.68864058-0.20021742j  0.36678172-0.59258611j]\n",
      "RESULT: False\n",
      "reward till now:  -106.0\n",
      "episode number:  116\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.68864058-0.20021742j  0.36678172-0.59258611j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.68864058-0.20021742j  0.36678172-0.59258611j]\n",
      "RESULT: False\n",
      "reward till now:  -107.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 117 reward -107.00, Last 30ep Avg. rewards -107.00.\n",
      "episode number:  117\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.12626064+0.31078024j -0.94164154-0.0280196j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.57656136+0.19994196j  0.75512088+0.23956766j]\n",
      "RESULT: False\n",
      "reward till now:  -107.0\n",
      "episode number:  117\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.57656136+0.19994196j  0.75512088+0.23956766j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [ 0.23956766-0.75512088j -0.19994196-0.57656136j]\n",
      "RESULT: False\n",
      "reward till now:  -107.0\n",
      "episode number:  117\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.23956766-0.75512088j -0.19994196-0.57656136j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.19994196-0.57656136j  0.23956766-0.75512088j]\n",
      "RESULT: False\n",
      "reward till now:  -107.0\n",
      "episode number:  117\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.19994196-0.57656136j  0.23956766-0.75512088j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [ 0.23956766-0.75512088j -0.19994196-0.57656136j]\n",
      "RESULT: False\n",
      "reward till now:  -107.0\n",
      "episode number:  117\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.23956766-0.75512088j -0.19994196-0.57656136j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.19994196-0.57656136j  0.23956766-0.75512088j]\n",
      "RESULT: False\n",
      "reward till now:  -108.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 118 reward -108.00, Last 30ep Avg. rewards -108.00.\n",
      "episode number:  118\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.01409361-0.78935473j -0.39880635+0.46655543j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -108.0\n",
      "episode number:  118\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.+0.j 0.+1.j]\n",
      "RESULT: False\n",
      "reward till now:  -108.0\n",
      "episode number:  118\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 0.+1.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 0.+1.j]\n",
      "RESULT: False\n",
      "reward till now:  -108.0\n",
      "episode number:  118\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 0.+1.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.+1.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -108.0\n",
      "episode number:  118\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+1.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -109.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 119 reward -109.00, Last 30ep Avg. rewards -109.00.\n",
      "episode number:  119\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.2454511 -0.2747945j  0.36644331-0.85437757j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.36644331-0.85437757j 0.2454511 -0.2747945j ]\n",
      "RESULT: False\n",
      "reward till now:  -109.0\n",
      "episode number:  119\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.36644331-0.85437757j 0.2454511 -0.2747945j ]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.2454511 -0.2747945j  0.36644331-0.85437757j]\n",
      "RESULT: False\n",
      "reward till now:  -109.0\n",
      "episode number:  119\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.2454511 -0.2747945j  0.36644331-0.85437757j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.36644331-0.85437757j 0.2454511 -0.2747945j ]\n",
      "RESULT: False\n",
      "reward till now:  -109.0\n",
      "episode number:  119\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.36644331-0.85437757j 0.2454511 -0.2747945j ]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.06480549-0.77769631j 0.43057603+0.45342361j]\n",
      "RESULT: False\n",
      "reward till now:  -109.0\n",
      "episode number:  119\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.06480549-0.77769631j 0.43057603+0.45342361j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.45342361-0.43057603j 0.77769631+0.06480549j]\n",
      "RESULT: False\n",
      "reward till now:  -110.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 120 reward -110.00, Last 30ep Avg. rewards -110.00.\n",
      "episode number:  120\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.270382 +0.39622556j 0.8117089-0.33320794j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -110.0\n",
      "episode number:  120\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -110.0\n",
      "episode number:  120\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -110.0\n",
      "episode number:  120\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -110.0\n",
      "episode number:  120\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -111.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 121 reward -111.00, Last 30ep Avg. rewards -111.00.\n",
      "episode number:  121\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.99482808-0.03664785j -0.04994155+0.0804976j ]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.04994155+0.0804976j  -0.99482808-0.03664785j]\n",
      "RESULT: False\n",
      "reward till now:  -111.0\n",
      "episode number:  121\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.04994155+0.0804976j  -0.99482808-0.03664785j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.03664785+0.99482808j -0.0804976 -0.04994155j]\n",
      "RESULT: False\n",
      "reward till now:  -111.0\n",
      "episode number:  121\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.03664785+0.99482808j -0.0804976 -0.04994155j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -111.0\n",
      "episode number:  121\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -111.0\n",
      "episode number:  121\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.+0.j 0.+1.j]\n",
      "RESULT: False\n",
      "reward till now:  -112.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 122 reward -112.00, Last 30ep Avg. rewards -112.00.\n",
      "episode number:  122\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.57692483-0.13440656j  0.0839049 -0.80128184j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.80128184-0.0839049j   0.13440656-0.57692483j]\n",
      "RESULT: False\n",
      "reward till now:  -112.0\n",
      "episode number:  122\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.80128184-0.0839049j   0.13440656-0.57692483j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.57692483-0.13440656j  0.0839049 -0.80128184j]\n",
      "RESULT: False\n",
      "reward till now:  -112.0\n",
      "episode number:  122\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.57692483-0.13440656j  0.0839049 -0.80128184j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.80128184-0.0839049j   0.13440656-0.57692483j]\n",
      "RESULT: False\n",
      "reward till now:  -112.0\n",
      "episode number:  122\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.80128184-0.0839049j   0.13440656-0.57692483j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.80128184-0.0839049j   0.13440656-0.57692483j]\n",
      "RESULT: False\n",
      "reward till now:  -112.0\n",
      "episode number:  122\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.80128184-0.0839049j   0.13440656-0.57692483j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.80128184-0.0839049j   0.13440656-0.57692483j]\n",
      "RESULT: False\n",
      "reward till now:  -113.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 123 reward -113.00, Last 30ep Avg. rewards -113.00.\n",
      "episode number:  123\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.15548648-0.36852656j -0.07318166-0.91359541j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.16169279-0.90659714j -0.05819829+0.38542188j]\n",
      "RESULT: False\n",
      "reward till now:  -113.0\n",
      "episode number:  123\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.16169279-0.90659714j -0.05819829+0.38542188j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -113.0\n",
      "episode number:  123\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -113.0\n",
      "episode number:  123\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -113.0\n",
      "episode number:  123\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -114.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 124 reward -114.00, Last 30ep Avg. rewards -114.00.\n",
      "episode number:  124\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.31498625+0.14155144j -0.89901911+0.26927959j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.89901911+0.26927959j -0.31498625+0.14155144j]\n",
      "RESULT: False\n",
      "reward till now:  -114.0\n",
      "episode number:  124\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.89901911+0.26927959j -0.31498625+0.14155144j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.31498625+0.14155144j -0.89901911+0.26927959j]\n",
      "RESULT: False\n",
      "reward till now:  -114.0\n",
      "episode number:  124\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.31498625+0.14155144j -0.89901911+0.26927959j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.85843142+0.2905014j   0.41297359-0.09031744j]\n",
      "RESULT: False\n",
      "reward till now:  -114.0\n",
      "episode number:  124\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.85843142+0.2905014j   0.41297359-0.09031744j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.85843142+0.2905014j   0.41297359-0.09031744j]\n",
      "RESULT: False\n",
      "reward till now:  -114.0\n",
      "episode number:  124\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.85843142+0.2905014j   0.41297359-0.09031744j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.31498625+0.14155144j -0.89901911+0.26927959j]\n",
      "RESULT: False\n",
      "reward till now:  -115.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 125 reward -115.00, Last 30ep Avg. rewards -115.00.\n",
      "episode number:  125\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.56177796+0.18639828j -0.72880149-0.34425222j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.11810347-0.11161959j  0.91257748+0.37522657j]\n",
      "RESULT: False\n",
      "reward till now:  -115.0\n",
      "episode number:  125\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.11810347-0.11161959j  0.91257748+0.37522657j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [ 0.91257748+0.37522657j -0.11810347-0.11161959j]\n",
      "RESULT: False\n",
      "reward till now:  -115.0\n",
      "episode number:  125\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.91257748+0.37522657j -0.11810347-0.11161959j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.11810347-0.11161959j  0.91257748+0.37522657j]\n",
      "RESULT: False\n",
      "reward till now:  -115.0\n",
      "episode number:  125\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.11810347-0.11161959j  0.91257748+0.37522657j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [ 0.91257748+0.37522657j -0.11810347-0.11161959j]\n",
      "RESULT: False\n",
      "reward till now:  -115.0\n",
      "episode number:  125\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.91257748+0.37522657j -0.11810347-0.11161959j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.56636276+0.34883701j -0.18181348+0.7242167j ]\n",
      "RESULT: False\n",
      "reward till now:  -116.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 126 reward -116.00, Last 30ep Avg. rewards -116.00.\n",
      "episode number:  126\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.09427584-0.52762479j -0.06452235+0.84176066j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -116.0\n",
      "episode number:  126\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -116.0\n",
      "episode number:  126\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -116.0\n",
      "episode number:  126\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -116.0\n",
      "episode number:  126\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -117.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 127 reward -117.00, Last 30ep Avg. rewards -117.00.\n",
      "episode number:  127\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.31288987-0.45534936j -0.76762405+0.32482335j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.76762405+0.32482335j  0.31288987-0.45534936j]\n",
      "RESULT: False\n",
      "reward till now:  -117.0\n",
      "episode number:  127\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.76762405+0.32482335j  0.31288987-0.45534936j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.76762405+0.32482335j  0.31288987-0.45534936j]\n",
      "RESULT: False\n",
      "reward till now:  -117.0\n",
      "episode number:  127\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.76762405+0.32482335j  0.31288987-0.45534936j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.76762405+0.32482335j  0.31288987-0.45534936j]\n",
      "RESULT: False\n",
      "reward till now:  -117.0\n",
      "episode number:  127\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.76762405+0.32482335j  0.31288987-0.45534936j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.86477279+0.00843824j -0.45093134-0.22081155j]\n",
      "RESULT: False\n",
      "reward till now:  -117.0\n",
      "episode number:  127\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.86477279+0.00843824j -0.45093134-0.22081155j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.93034332-0.15017061j -0.29263009+0.16210408j]\n",
      "RESULT: False\n",
      "reward till now:  -118.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 128 reward -118.00, Last 30ep Avg. rewards -118.00.\n",
      "episode number:  128\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.92093086+0.25641041j -0.15335914-0.25024195j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.15335914-0.25024195j  0.92093086+0.25641041j]\n",
      "RESULT: False\n",
      "reward till now:  -118.0\n",
      "episode number:  128\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.15335914-0.25024195j  0.92093086+0.25641041j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.25641041-0.92093086j 0.25024195-0.15335914j]\n",
      "RESULT: False\n",
      "reward till now:  -118.0\n",
      "episode number:  128\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.25641041-0.92093086j 0.25024195-0.15335914j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.15335914-0.25024195j  0.92093086+0.25641041j]\n",
      "RESULT: False\n",
      "reward till now:  -118.0\n",
      "episode number:  128\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.15335914-0.25024195j  0.92093086+0.25641041j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.54275517+0.00436176j -0.75963774-0.35825731j]\n",
      "RESULT: False\n",
      "reward till now:  -118.0\n",
      "episode number:  128\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.54275517+0.00436176j -0.75963774-0.35825731j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.15335914-0.25024195j  0.92093086+0.25641041j]\n",
      "RESULT: False\n",
      "reward till now:  -119.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 129 reward -119.00, Last 30ep Avg. rewards -119.00.\n",
      "episode number:  129\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.07756684-0.07594271j  0.86828251-0.48404707j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [ 0.86828251-0.48404707j -0.07756684-0.07594271j]\n",
      "RESULT: False\n",
      "reward till now:  -119.0\n",
      "episode number:  129\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.86828251-0.48404707j -0.07756684-0.07594271j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.07756684-0.07594271j  0.86828251-0.48404707j]\n",
      "RESULT: False\n",
      "reward till now:  -119.0\n",
      "episode number:  129\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.07756684-0.07594271j  0.86828251-0.48404707j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [ 0.86828251-0.48404707j -0.07756684-0.07594271j]\n",
      "RESULT: False\n",
      "reward till now:  -119.0\n",
      "episode number:  129\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.86828251-0.48404707j -0.07756684-0.07594271j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.07756684-0.07594271j  0.86828251-0.48404707j]\n",
      "RESULT: False\n",
      "reward till now:  -119.0\n",
      "episode number:  129\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.07756684-0.07594271j  0.86828251-0.48404707j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [ 0.86828251-0.48404707j -0.07756684-0.07594271j]\n",
      "RESULT: False\n",
      "reward till now:  -120.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 130 reward -120.00, Last 30ep Avg. rewards -120.00.\n",
      "episode number:  130\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.7287394 -0.32580808j 0.57180352-0.18928474j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.91962272-0.36422562j 0.11097043-0.09653658j]\n",
      "RESULT: False\n",
      "reward till now:  -120.0\n",
      "episode number:  130\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.91962272-0.36422562j 0.11097043-0.09653658j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.09653658-0.11097043j  0.36422562+0.91962272j]\n",
      "RESULT: False\n",
      "reward till now:  -120.0\n",
      "episode number:  130\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.09653658-0.11097043j  0.36422562+0.91962272j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.91962272-0.36422562j 0.11097043-0.09653658j]\n",
      "RESULT: False\n",
      "reward till now:  -120.0\n",
      "episode number:  130\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.91962272-0.36422562j 0.11097043-0.09653658j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.11097043-0.09653658j 0.91962272-0.36422562j]\n",
      "RESULT: False\n",
      "reward till now:  -120.0\n",
      "episode number:  130\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.11097043-0.09653658j 0.91962272-0.36422562j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.91962272-0.36422562j 0.11097043-0.09653658j]\n",
      "RESULT: False\n",
      "reward till now:  -121.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 131 reward -121.00, Last 30ep Avg. rewards -121.00.\n",
      "episode number:  131\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.18103333+0.51988224j 0.63435329-0.54272027j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.63435329-0.54272027j 0.18103333+0.51988224j]\n",
      "RESULT: False\n",
      "reward till now:  -121.0\n",
      "episode number:  131\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.63435329-0.54272027j 0.18103333+0.51988224j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.18103333+0.51988224j 0.63435329-0.54272027j]\n",
      "RESULT: False\n",
      "reward till now:  -121.0\n",
      "episode number:  131\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.18103333+0.51988224j 0.63435329-0.54272027j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.63435329-0.54272027j 0.18103333+0.51988224j]\n",
      "RESULT: False\n",
      "reward till now:  -121.0\n",
      "episode number:  131\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.63435329-0.54272027j 0.18103333+0.51988224j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.18103333+0.51988224j 0.63435329-0.54272027j]\n",
      "RESULT: False\n",
      "reward till now:  -121.0\n",
      "episode number:  131\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.18103333+0.51988224j 0.63435329-0.54272027j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.63435329-0.54272027j 0.18103333+0.51988224j]\n",
      "RESULT: False\n",
      "reward till now:  -122.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 132 reward -122.00, Last 30ep Avg. rewards -122.00.\n",
      "episode number:  132\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.09017728-0.50903566j 0.29989171+0.80175789j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -122.0\n",
      "episode number:  132\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -122.0\n",
      "episode number:  132\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -122.0\n",
      "episode number:  132\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.+0.j 0.+1.j]\n",
      "RESULT: False\n",
      "reward till now:  -122.0\n",
      "episode number:  132\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 0.+1.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -123.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 133 reward -123.00, Last 30ep Avg. rewards -123.00.\n",
      "episode number:  133\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.062399  +0.48584999j 0.31396809-0.81331433j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -123.0\n",
      "episode number:  133\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -123.0\n",
      "episode number:  133\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -123.0\n",
      "episode number:  133\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -123.0\n",
      "episode number:  133\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -124.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 134 reward -124.00, Last 30ep Avg. rewards -124.00.\n",
      "episode number:  134\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.20735486-0.08847409j -0.73841499-0.63554669j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.37551622-0.51196j     0.66876027+0.38683874j]\n",
      "RESULT: False\n",
      "reward till now:  -124.0\n",
      "episode number:  134\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.37551622-0.51196j     0.66876027+0.38683874j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.38683874-0.66876027j 0.51196   -0.37551622j]\n",
      "RESULT: False\n",
      "reward till now:  -124.0\n",
      "episode number:  134\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.38683874-0.66876027j 0.51196   -0.37551622j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.51196   -0.37551622j 0.38683874-0.66876027j]\n",
      "RESULT: False\n",
      "reward till now:  -124.0\n",
      "episode number:  134\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.51196   -0.37551622j 0.38683874-0.66876027j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -124.0\n",
      "episode number:  134\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -125.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 135 reward -125.00, Last 30ep Avg. rewards -125.00.\n",
      "episode number:  135\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.2683743 +0.35467509j -0.51061327+0.7358362j ]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.51061327+0.7358362j   0.2683743 +0.35467509j]\n",
      "RESULT: False\n",
      "reward till now:  -125.0\n",
      "episode number:  135\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.51061327+0.7358362j   0.2683743 +0.35467509j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [ 0.35467509-0.2683743j  -0.7358362 -0.51061327j]\n",
      "RESULT: False\n",
      "reward till now:  -125.0\n",
      "episode number:  135\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.35467509-0.2683743j  -0.7358362 -0.51061327j]]]\n",
      "Action:  hadamard_X(new_state)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW STATE:  [-0.26952161-0.55082739j  0.77110793+0.17128882j]\n",
      "RESULT: False\n",
      "reward till now:  -125.0\n",
      "episode number:  135\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.26952161-0.55082739j  0.77110793+0.17128882j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.17128882-0.77110793j 0.55082739-0.26952161j]\n",
      "RESULT: False\n",
      "reward till now:  -125.0\n",
      "episode number:  135\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.17128882-0.77110793j 0.55082739-0.26952161j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.55082739-0.26952161j 0.17128882-0.77110793j]\n",
      "RESULT: False\n",
      "reward till now:  -126.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 136 reward -126.00, Last 30ep Avg. rewards -126.00.\n",
      "episode number:  136\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.83718498+0.4258918j  -0.08643456+0.33206407j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.65309765+0.53595573j -0.53086071+0.06634622j]\n",
      "RESULT: False\n",
      "reward till now:  -126.0\n",
      "episode number:  136\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.65309765+0.53595573j -0.53086071+0.06634622j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.83718498+0.4258918j  -0.08643456+0.33206407j]\n",
      "RESULT: False\n",
      "reward till now:  -126.0\n",
      "episode number:  136\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.83718498+0.4258918j  -0.08643456+0.33206407j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.08643456+0.33206407j -0.83718498+0.4258918j ]\n",
      "RESULT: False\n",
      "reward till now:  -126.0\n",
      "episode number:  136\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.08643456+0.33206407j -0.83718498+0.4258918j ]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.83718498+0.4258918j  -0.08643456+0.33206407j]\n",
      "RESULT: False\n",
      "reward till now:  -126.0\n",
      "episode number:  136\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.83718498+0.4258918j  -0.08643456+0.33206407j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.65309765+0.53595573j -0.53086071+0.06634622j]\n",
      "RESULT: False\n",
      "reward till now:  -127.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 137 reward -127.00, Last 30ep Avg. rewards -127.00.\n",
      "episode number:  137\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.18204188+0.59736584j 0.47257319-0.62184354j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.47257319-0.62184354j 0.18204188+0.59736584j]\n",
      "RESULT: False\n",
      "reward till now:  -127.0\n",
      "episode number:  137\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.47257319-0.62184354j 0.18204188+0.59736584j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -127.0\n",
      "episode number:  137\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -127.0\n",
      "episode number:  137\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -127.0\n",
      "episode number:  137\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -128.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 138 reward -128.00, Last 30ep Avg. rewards -128.00.\n",
      "episode number:  138\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.53604252-0.64489136j  0.37301112-0.39703432j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.65978496-0.71976575j  0.19224836-0.09829364j]\n",
      "RESULT: False\n",
      "reward till now:  -128.0\n",
      "episode number:  138\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.65978496-0.71976575j  0.19224836-0.09829364j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.09829364-0.19224836j  0.71976575-0.65978496j]\n",
      "RESULT: False\n",
      "reward till now:  -128.0\n",
      "episode number:  138\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.09829364-0.19224836j  0.71976575-0.65978496j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [ 0.71976575-0.65978496j -0.09829364-0.19224836j]\n",
      "RESULT: False\n",
      "reward till now:  -128.0\n",
      "episode number:  138\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.71976575-0.65978496j -0.09829364-0.19224836j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -128.0\n",
      "episode number:  138\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -129.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 139 reward -129.00, Last 30ep Avg. rewards -129.00.\n",
      "episode number:  139\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.40638214+0.34051925j 0.72089241+0.44633432j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.72089241+0.44633432j 0.40638214+0.34051925j]\n",
      "RESULT: False\n",
      "reward till now:  -129.0\n",
      "episode number:  139\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.72089241+0.44633432j 0.40638214+0.34051925j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.40638214+0.34051925j 0.72089241+0.44633432j]\n",
      "RESULT: False\n",
      "reward till now:  -129.0\n",
      "episode number:  139\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.40638214+0.34051925j 0.72089241+0.44633432j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.72089241+0.44633432j 0.40638214+0.34051925j]\n",
      "RESULT: False\n",
      "reward till now:  -129.0\n",
      "episode number:  139\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.72089241+0.44633432j 0.40638214+0.34051925j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.40638214+0.34051925j 0.72089241+0.44633432j]\n",
      "RESULT: False\n",
      "reward till now:  -129.0\n",
      "episode number:  139\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.40638214+0.34051925j 0.72089241+0.44633432j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.72089241+0.44633432j 0.40638214+0.34051925j]\n",
      "RESULT: False\n",
      "reward till now:  -130.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 140 reward -130.00, Last 30ep Avg. rewards -130.00.\n",
      "episode number:  140\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.41006218+0.43152777j -0.70387578-0.38754572j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.20775759+0.03110001j  0.78767309+0.57917241j]\n",
      "RESULT: False\n",
      "reward till now:  -130.0\n",
      "episode number:  140\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.20775759+0.03110001j  0.78767309+0.57917241j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -130.0\n",
      "episode number:  140\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -130.0\n",
      "episode number:  140\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -130.0\n",
      "episode number:  140\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -131.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 141 reward -131.00, Last 30ep Avg. rewards -131.00.\n",
      "episode number:  141\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.30957361+0.46332185j  0.14355954+0.81785555j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -131.0\n",
      "episode number:  141\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -131.0\n",
      "episode number:  141\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -131.0\n",
      "episode number:  141\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.-1.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -131.0\n",
      "episode number:  141\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.-1.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -132.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 142 reward -132.00, Last 30ep Avg. rewards -132.00.\n",
      "episode number:  142\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.46021962+0.26880382j 0.82994921+0.16470188j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.91228713+0.30653482j -0.2614383 +0.07361119j]\n",
      "RESULT: False\n",
      "reward till now:  -132.0\n",
      "episode number:  142\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.91228713+0.30653482j -0.2614383 +0.07361119j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [ 0.91228713+0.30653482j -0.2614383 +0.07361119j]\n",
      "RESULT: False\n",
      "reward till now:  -132.0\n",
      "episode number:  142\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.91228713+0.30653482j -0.2614383 +0.07361119j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -132.0\n",
      "episode number:  142\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -131.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 143 reward -131.00, Last 30ep Avg. rewards -131.00.\n",
      "episode number:  143\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.40308415+0.65577924j -0.05362343-0.63608277j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -131.0\n",
      "episode number:  143\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -131.0\n",
      "episode number:  143\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -130.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 144 reward -130.00, Last 30ep Avg. rewards -130.00.\n",
      "episode number:  144\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.3900223 +0.60026963j -0.34866791-0.6049708j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.52233285-0.00332423j -0.02924197+0.85223368j]\n",
      "RESULT: False\n",
      "reward till now:  -130.0\n",
      "episode number:  144\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.52233285-0.00332423j -0.02924197+0.85223368j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.3900223 +0.60026963j -0.34866791-0.6049708j ]\n",
      "RESULT: False\n",
      "reward till now:  -130.0\n",
      "episode number:  144\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.3900223 +0.60026963j -0.34866791-0.6049708j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.52233285-0.00332423j -0.02924197+0.85223368j]\n",
      "RESULT: False\n",
      "reward till now:  -130.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode number:  144\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.52233285-0.00332423j -0.02924197+0.85223368j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.52233285-0.00332423j -0.02924197+0.85223368j]\n",
      "RESULT: False\n",
      "reward till now:  -130.0\n",
      "episode number:  144\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.52233285-0.00332423j -0.02924197+0.85223368j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.02924197+0.85223368j -0.52233285-0.00332423j]\n",
      "RESULT: False\n",
      "reward till now:  -131.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 145 reward -131.00, Last 30ep Avg. rewards -131.00.\n",
      "episode number:  145\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.55856217-0.4759104j -0.0507287 +0.6774542j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.43083371+0.14251299j -0.35909248-0.81555193j]\n",
      "RESULT: False\n",
      "reward till now:  -131.0\n",
      "episode number:  145\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.43083371+0.14251299j -0.35909248-0.81555193j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.55856217-0.4759104j -0.0507287 +0.6774542j]\n",
      "RESULT: False\n",
      "reward till now:  -131.0\n",
      "episode number:  145\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.55856217-0.4759104j -0.0507287 +0.6774542j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -131.0\n",
      "episode number:  145\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -131.0\n",
      "episode number:  145\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -132.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 146 reward -132.00, Last 30ep Avg. rewards -132.00.\n",
      "episode number:  146\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.56100586-0.4566828j   0.64448633+0.24769056j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.24769056-0.64448633j 0.4566828 -0.56100586j]\n",
      "RESULT: False\n",
      "reward till now:  -132.0\n",
      "episode number:  146\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.24769056-0.64448633j 0.4566828 -0.56100586j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -132.0\n",
      "episode number:  146\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.+0.j 0.+1.j]\n",
      "RESULT: False\n",
      "reward till now:  -132.0\n",
      "episode number:  146\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 0.+1.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -132.0\n",
      "episode number:  146\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -133.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 147 reward -133.00, Last 30ep Avg. rewards -133.00.\n",
      "episode number:  147\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.72972149-0.64505793j -0.1826498 +0.13433487j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -133.0\n",
      "episode number:  147\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -132.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 148 reward -132.00, Last 30ep Avg. rewards -132.00.\n",
      "episode number:  148\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.3307771 +0.33472164j -0.52142336+0.71180448j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.52142336+0.71180448j  0.3307771 +0.33472164j]\n",
      "RESULT: False\n",
      "reward till now:  -132.0\n",
      "episode number:  148\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.52142336+0.71180448j  0.3307771 +0.33472164j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.13201805+0.26942704j -0.73721651-0.60538593j]\n",
      "RESULT: False\n",
      "reward till now:  -132.0\n",
      "episode number:  148\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.13201805+0.26942704j -0.73721651-0.60538593j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.61464165-0.23755881j  0.42793993+0.61858619j]\n",
      "RESULT: False\n",
      "reward till now:  -132.0\n",
      "episode number:  148\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.61464165-0.23755881j  0.42793993+0.61858619j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -132.0\n",
      "episode number:  148\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -131.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 149 reward -131.00, Last 30ep Avg. rewards -131.00.\n",
      "episode number:  149\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.10631258-0.43642411j -0.84637778-0.28614032j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.12715742+0.28988102j  0.90707792+0.2775061j ]\n",
      "RESULT: False\n",
      "reward till now:  -131.0\n",
      "episode number:  149\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.12715742+0.28988102j  0.90707792+0.2775061j ]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [ 0.90707792+0.2775061j  -0.12715742+0.28988102j]\n",
      "RESULT: False\n",
      "reward till now:  -131.0\n",
      "episode number:  149\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.90707792+0.2775061j  -0.12715742+0.28988102j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -131.0\n",
      "episode number:  149\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -130.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 150 reward -130.00, Last 30ep Avg. rewards -130.00.\n",
      "episode number:  150\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.58748209-0.52516115j -0.44610925-0.42433135j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.09996569-0.67139259j 0.73085945-0.07129743j]\n",
      "RESULT: False\n",
      "reward till now:  -130.0\n",
      "episode number:  150\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.09996569-0.67139259j 0.73085945-0.07129743j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -130.0\n",
      "episode number:  150\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.-1.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -130.0\n",
      "episode number:  150\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.-1.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -130.0\n",
      "episode number:  150\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -129.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.81450625]\n",
      " [0.857375  ]\n",
      " [0.9025    ]\n",
      " [0.95      ]\n",
      " [1.        ]]\n",
      "Episode 151 reward -129.00, Last 30ep Avg. rewards -129.00.\n",
      "episode number:  151\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.1408734 -0.16040005j -0.87260059+0.43931165j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.71663433+0.19722028j  0.51740926-0.42406021j]\n",
      "RESULT: False\n",
      "reward till now:  -129.0\n",
      "episode number:  151\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.71663433+0.19722028j  0.51740926-0.42406021j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.42406021-0.51740926j -0.19722028-0.71663433j]\n",
      "RESULT: False\n",
      "reward till now:  -129.0\n",
      "episode number:  151\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.42406021-0.51740926j -0.19722028-0.71663433j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -129.0\n",
      "episode number:  151\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -129.0\n",
      "episode number:  151\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.5-0.5j  0.5-0.5j]\n",
      "RESULT: False\n",
      "reward till now:  -130.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 152 reward -130.00, Last 30ep Avg. rewards -130.00.\n",
      "episode number:  152\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.19774113-0.18629232j 0.76474006+0.58426557j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.68057698+0.28140959j -0.40092879-0.54486671j]\n",
      "RESULT: False\n",
      "reward till now:  -130.0\n",
      "episode number:  152\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.68057698+0.28140959j -0.40092879-0.54486671j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.19774113-0.18629232j 0.76474006+0.58426557j]\n",
      "RESULT: False\n",
      "reward till now:  -130.0\n",
      "episode number:  152\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.19774113-0.18629232j 0.76474006+0.58426557j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.19774113-0.18629232j 0.76474006+0.58426557j]\n",
      "RESULT: False\n",
      "reward till now:  -130.0\n",
      "episode number:  152\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.19774113-0.18629232j 0.76474006+0.58426557j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.68057698+0.28140959j -0.40092879-0.54486671j]\n",
      "RESULT: False\n",
      "reward till now:  -130.0\n",
      "episode number:  152\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.68057698+0.28140959j -0.40092879-0.54486671j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.09596165+0.48248609j 0.08451284+0.86651954j]\n",
      "RESULT: False\n",
      "reward till now:  -131.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 153 reward -131.00, Last 30ep Avg. rewards -131.00.\n",
      "episode number:  153\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.70309395-0.25120873j -0.39551363+0.53490376j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.21749213+0.20060267j 0.77683287-0.55586547j]\n",
      "RESULT: False\n",
      "reward till now:  -131.0\n",
      "episode number:  153\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.21749213+0.20060267j 0.77683287-0.55586547j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70309395-0.25120873j -0.39551363+0.53490376j]\n",
      "RESULT: False\n",
      "reward till now:  -131.0\n",
      "episode number:  153\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70309395-0.25120873j -0.39551363+0.53490376j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.21749213+0.20060267j 0.77683287-0.55586547j]\n",
      "RESULT: False\n",
      "reward till now:  -131.0\n",
      "episode number:  153\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.21749213+0.20060267j 0.77683287-0.55586547j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70309395-0.25120873j -0.39551363+0.53490376j]\n",
      "RESULT: False\n",
      "reward till now:  -131.0\n",
      "episode number:  153\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70309395-0.25120873j -0.39551363+0.53490376j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.21749213+0.20060267j 0.77683287-0.55586547j]\n",
      "RESULT: False\n",
      "reward till now:  -132.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 154 reward -132.00, Last 30ep Avg. rewards -132.00.\n",
      "episode number:  154\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.62844983-0.41202342j  0.26724412-0.6032148j ]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.6032148 -0.26724412j  0.41202342-0.62844983j]\n",
      "RESULT: False\n",
      "reward till now:  -132.0\n",
      "episode number:  154\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.6032148 -0.26724412j  0.41202342-0.62844983j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.62844983-0.41202342j  0.26724412-0.6032148j ]\n",
      "RESULT: False\n",
      "reward till now:  -132.0\n",
      "episode number:  154\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.62844983-0.41202342j  0.26724412-0.6032148j ]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.6032148 -0.26724412j  0.41202342-0.62844983j]\n",
      "RESULT: False\n",
      "reward till now:  -132.0\n",
      "episode number:  154\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.6032148 -0.26724412j  0.41202342-0.62844983j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.62844983-0.41202342j  0.26724412-0.6032148j ]\n",
      "RESULT: False\n",
      "reward till now:  -132.0\n",
      "episode number:  154\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.62844983-0.41202342j  0.26724412-0.6032148j ]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.6032148 -0.26724412j  0.41202342-0.62844983j]\n",
      "RESULT: False\n",
      "reward till now:  -133.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 155 reward -133.00, Last 30ep Avg. rewards -133.00.\n",
      "episode number:  155\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.25368417+0.1795641j  -0.67681397-0.66732595j]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.2924889 +0.60555075j  0.35160875+0.6512525j ]\n",
      "RESULT: False\n",
      "reward till now:  -133.0\n",
      "episode number:  155\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.2924889 +0.60555075j  0.35160875+0.6512525j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -133.0\n",
      "episode number:  155\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -133.0\n",
      "episode number:  155\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -133.0\n",
      "episode number:  155\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.        +0.70710678j 0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -134.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 156 reward -134.00, Last 30ep Avg. rewards -134.00.\n",
      "episode number:  156\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.20147586-0.87259087j -0.05874792-0.4410684j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.184006 -0.92889738j -0.1009239-0.30513246j]\n",
      "RESULT: False\n",
      "reward till now:  -134.0\n",
      "episode number:  156\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.184006 -0.92889738j -0.1009239-0.30513246j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.1009239-0.30513246j -0.184006 -0.92889738j]\n",
      "RESULT: False\n",
      "reward till now:  -134.0\n",
      "episode number:  156\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.1009239-0.30513246j -0.184006 -0.92889738j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.184006 -0.92889738j -0.1009239-0.30513246j]\n",
      "RESULT: False\n",
      "reward till now:  -134.0\n",
      "episode number:  156\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.184006 -0.92889738j -0.1009239-0.30513246j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -134.0\n",
      "episode number:  156\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -133.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 157 reward -133.00, Last 30ep Avg. rewards -133.00.\n",
      "episode number:  157\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.41130863-0.15755355j -0.45718002+0.77265032j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -133.0\n",
      "episode number:  157\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -133.0\n",
      "episode number:  157\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.5-0.5j  0.5-0.5j]\n",
      "RESULT: False\n",
      "reward till now:  -133.0\n",
      "episode number:  157\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.5-0.5j  0.5-0.5j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.5-0.5j  0.5-0.5j]\n",
      "RESULT: False\n",
      "reward till now:  -133.0\n",
      "episode number:  157\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.5-0.5j  0.5-0.5j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.5-0.5j  0.5-0.5j]\n",
      "RESULT: False\n",
      "reward till now:  -134.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 158 reward -134.00, Last 30ep Avg. rewards -134.00.\n",
      "episode number:  158\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.68459084+0.72116649j 0.0678553 +0.08154721j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -134.0\n",
      "episode number:  158\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -133.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 159 reward -133.00, Last 30ep Avg. rewards -133.00.\n",
      "episode number:  159\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.08354481-0.63325211j  0.05526457-0.76743589j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.76743589-0.05526457j  0.63325211-0.08354481j]\n",
      "RESULT: False\n",
      "reward till now:  -133.0\n",
      "episode number:  159\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.76743589-0.05526457j  0.63325211-0.08354481j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.08354481-0.63325211j  0.05526457-0.76743589j]\n",
      "RESULT: False\n",
      "reward till now:  -133.0\n",
      "episode number:  159\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.08354481-0.63325211j  0.05526457-0.76743589j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -133.0\n",
      "episode number:  159\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -133.0\n",
      "episode number:  159\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.5-0.5j  0.5-0.5j]\n",
      "RESULT: False\n",
      "reward till now:  -134.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 160 reward -134.00, Last 30ep Avg. rewards -134.00.\n",
      "episode number:  160\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.6327838 +0.51053354j -0.36994608-0.44953317j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.7090371 +0.04313377j -0.18585433+0.67886968j]\n",
      "RESULT: False\n",
      "reward till now:  -134.0\n",
      "episode number:  160\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.7090371 +0.04313377j -0.18585433+0.67886968j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.6327838 +0.51053354j -0.36994608-0.44953317j]\n",
      "RESULT: False\n",
      "reward till now:  -134.0\n",
      "episode number:  160\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.6327838 +0.51053354j -0.36994608-0.44953317j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.7090371 +0.04313377j -0.18585433+0.67886968j]\n",
      "RESULT: False\n",
      "reward till now:  -134.0\n",
      "episode number:  160\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.7090371 +0.04313377j -0.18585433+0.67886968j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.6327838 +0.51053354j -0.36994608-0.44953317j]\n",
      "RESULT: False\n",
      "reward till now:  -134.0\n",
      "episode number:  160\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.6327838 +0.51053354j -0.36994608-0.44953317j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.7090371 +0.04313377j -0.18585433+0.67886968j]\n",
      "RESULT: False\n",
      "reward till now:  -135.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 161 reward -135.00, Last 30ep Avg. rewards -135.00.\n",
      "episode number:  161\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.52832819-0.72058437j -0.24674781-0.37515731j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.5480615-0.77480637j -0.1991074-0.24425382j]\n",
      "RESULT: False\n",
      "reward till now:  -135.0\n",
      "episode number:  161\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.5480615-0.77480637j -0.1991074-0.24425382j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.52832819-0.72058437j -0.24674781-0.37515731j]\n",
      "RESULT: False\n",
      "reward till now:  -135.0\n",
      "episode number:  161\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.52832819-0.72058437j -0.24674781-0.37515731j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.5480615-0.77480637j -0.1991074-0.24425382j]\n",
      "RESULT: False\n",
      "reward till now:  -135.0\n",
      "episode number:  161\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.5480615-0.77480637j -0.1991074-0.24425382j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.52832819-0.72058437j -0.24674781-0.37515731j]\n",
      "RESULT: False\n",
      "reward till now:  -135.0\n",
      "episode number:  161\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.52832819-0.72058437j -0.24674781-0.37515731j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.5480615-0.77480637j -0.1991074-0.24425382j]\n",
      "RESULT: False\n",
      "reward till now:  -136.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 162 reward -136.00, Last 30ep Avg. rewards -136.00.\n",
      "episode number:  162\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.32059323+0.29216795j -0.18162956+0.88253531j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -136.0\n",
      "episode number:  162\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -135.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 163 reward -135.00, Last 30ep Avg. rewards -135.00.\n",
      "episode number:  163\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.89311028-0.14453763j -0.31781495+0.28364865j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [ 0.89311028-0.14453763j -0.31781495+0.28364865j]\n",
      "RESULT: False\n",
      "reward till now:  -135.0\n",
      "episode number:  163\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.89311028-0.14453763j -0.31781495+0.28364865j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [ 0.89311028-0.14453763j -0.31781495+0.28364865j]\n",
      "RESULT: False\n",
      "reward till now:  -135.0\n",
      "episode number:  163\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.89311028-0.14453763j -0.31781495+0.28364865j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [ 0.89311028-0.14453763j -0.31781495+0.28364865j]\n",
      "RESULT: False\n",
      "reward till now:  -135.0\n",
      "episode number:  163\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.89311028-0.14453763j -0.31781495+0.28364865j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [ 0.89311028-0.14453763j -0.31781495+0.28364865j]\n",
      "RESULT: False\n",
      "reward till now:  -135.0\n",
      "episode number:  163\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.89311028-0.14453763j -0.31781495+0.28364865j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [ 0.89311028-0.14453763j -0.31781495+0.28364865j]\n",
      "RESULT: False\n",
      "reward till now:  -136.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 164 reward -136.00, Last 30ep Avg. rewards -136.00.\n",
      "episode number:  164\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.16926323+0.93466782j  0.30169533+0.08201187j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -136.0\n",
      "episode number:  164\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -135.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 165 reward -135.00, Last 30ep Avg. rewards -135.00.\n",
      "episode number:  165\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.64064404-0.65839485j  0.22986128+0.32133352j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.32133352-0.22986128j 0.65839485-0.64064404j]\n",
      "RESULT: False\n",
      "reward till now:  -135.0\n",
      "episode number:  165\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.32133352-0.22986128j 0.65839485-0.64064404j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -135.0\n",
      "episode number:  165\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -134.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 166 reward -134.00, Last 30ep Avg. rewards -134.00.\n",
      "episode number:  166\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.37121754+0.08843757j  0.76367638-0.5207444j ]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.63071233-0.47746594j -0.60253555+0.10573146j]\n",
      "RESULT: False\n",
      "reward till now:  -134.0\n",
      "episode number:  166\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.63071233-0.47746594j -0.60253555+0.10573146j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.87203794-0.26285597j -0.01992399-0.41238283j]\n",
      "RESULT: False\n",
      "reward till now:  -134.0\n",
      "episode number:  166\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.87203794-0.26285597j -0.01992399-0.41238283j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.63071233-0.47746594j -0.60253555+0.10573146j]\n",
      "RESULT: False\n",
      "reward till now:  -134.0\n",
      "episode number:  166\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.63071233-0.47746594j -0.60253555+0.10573146j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.87203794-0.26285597j -0.01992399-0.41238283j]\n",
      "RESULT: False\n",
      "reward till now:  -134.0\n",
      "episode number:  166\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.87203794-0.26285597j -0.01992399-0.41238283j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.63071233-0.47746594j -0.60253555+0.10573146j]\n",
      "RESULT: False\n",
      "reward till now:  -135.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 167 reward -135.00, Last 30ep Avg. rewards -135.00.\n",
      "episode number:  167\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.85485947+0.36911534j 0.27038353-0.24466692j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.24466692-0.27038353j -0.36911534+0.85485947j]\n",
      "RESULT: False\n",
      "reward till now:  -135.0\n",
      "episode number:  167\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.24466692-0.27038353j -0.36911534+0.85485947j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -135.0\n",
      "episode number:  167\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -135.0\n",
      "episode number:  167\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.5-0.5j  0.5-0.5j]\n",
      "RESULT: False\n",
      "reward till now:  -135.0\n",
      "episode number:  167\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.5-0.5j  0.5-0.5j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.5-0.5j  0.5-0.5j]\n",
      "RESULT: False\n",
      "reward till now:  -136.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 168 reward -136.00, Last 30ep Avg. rewards -136.00.\n",
      "episode number:  168\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.4171009 -0.0871216j  0.52824026-0.7344378j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -136.0\n",
      "episode number:  168\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -136.0\n",
      "episode number:  168\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.5-0.5j  0.5-0.5j]\n",
      "RESULT: False\n",
      "reward till now:  -136.0\n",
      "episode number:  168\n",
      "time step:  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL STATE:  [[[-0.5-0.5j  0.5-0.5j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -136.0\n",
      "episode number:  168\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -137.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 169 reward -137.00, Last 30ep Avg. rewards -137.00.\n",
      "episode number:  169\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.74155437-0.35244005j -0.24564757-0.51530612j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.69805719-0.6135892j -0.35065906+0.1151637j]\n",
      "RESULT: False\n",
      "reward till now:  -137.0\n",
      "episode number:  169\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.69805719-0.6135892j -0.35065906+0.1151637j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.74155437-0.35244005j -0.24564757-0.51530612j]\n",
      "RESULT: False\n",
      "reward till now:  -137.0\n",
      "episode number:  169\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.74155437-0.35244005j -0.24564757-0.51530612j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.69805719-0.6135892j -0.35065906+0.1151637j]\n",
      "RESULT: False\n",
      "reward till now:  -137.0\n",
      "episode number:  169\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.69805719-0.6135892j -0.35065906+0.1151637j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.74155437-0.35244005j -0.24564757-0.51530612j]\n",
      "RESULT: False\n",
      "reward till now:  -137.0\n",
      "episode number:  169\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.74155437-0.35244005j -0.24564757-0.51530612j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.69805719-0.6135892j -0.35065906+0.1151637j]\n",
      "RESULT: False\n",
      "reward till now:  -138.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 170 reward -138.00, Last 30ep Avg. rewards -138.00.\n",
      "episode number:  170\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.04148738-0.22515603j -0.82077584-0.52336468j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.39941072+0.4211668j   0.73958552+0.34073871j]\n",
      "RESULT: False\n",
      "reward till now:  -138.0\n",
      "episode number:  170\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.39941072+0.4211668j   0.73958552+0.34073871j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.04148738-0.22515603j -0.82077584-0.52336468j]\n",
      "RESULT: False\n",
      "reward till now:  -138.0\n",
      "episode number:  170\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.04148738-0.22515603j -0.82077584-0.52336468j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.39941072+0.4211668j   0.73958552+0.34073871j]\n",
      "RESULT: False\n",
      "reward till now:  -138.0\n",
      "episode number:  170\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.39941072+0.4211668j   0.73958552+0.34073871j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [ 0.34073871-0.73958552j -0.4211668 -0.39941072j]\n",
      "RESULT: False\n",
      "reward till now:  -138.0\n",
      "episode number:  170\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.34073871-0.73958552j -0.4211668 -0.39941072j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.04148738-0.22515603j  0.82077584+0.52336468j]\n",
      "RESULT: False\n",
      "reward till now:  -139.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 171 reward -139.00, Last 30ep Avg. rewards -139.00.\n",
      "episode number:  171\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.1264915 +0.12023711j -0.60512071-0.77677015j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -139.0\n",
      "episode number:  171\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -139.0\n",
      "episode number:  171\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.5-0.5j  0.5-0.5j]\n",
      "RESULT: False\n",
      "reward till now:  -139.0\n",
      "episode number:  171\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.5-0.5j  0.5-0.5j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.5-0.5j  0.5-0.5j]\n",
      "RESULT: False\n",
      "reward till now:  -139.0\n",
      "episode number:  171\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.5-0.5j  0.5-0.5j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -140.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 172 reward -140.00, Last 30ep Avg. rewards -140.00.\n",
      "episode number:  172\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.46303586-0.02342532j  0.61032576-0.64230173j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.78159171-0.44812969j -0.41500128+0.12676011j]\n",
      "RESULT: False\n",
      "reward till now:  -140.0\n",
      "episode number:  172\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.78159171-0.44812969j -0.41500128+0.12676011j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.41500128+0.12676011j -0.78159171-0.44812969j]\n",
      "RESULT: False\n",
      "reward till now:  -140.0\n",
      "episode number:  172\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.41500128+0.12676011j -0.78159171-0.44812969j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.41500128+0.12676011j -0.78159171-0.44812969j]\n",
      "RESULT: False\n",
      "reward till now:  -140.0\n",
      "episode number:  172\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.41500128+0.12676011j -0.78159171-0.44812969j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -140.0\n",
      "episode number:  172\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -141.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 173 reward -141.00, Last 30ep Avg. rewards -141.00.\n",
      "episode number:  173\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.47046128+0.66223604j -0.04282962-0.58161434j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -141.0\n",
      "episode number:  173\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.+0.j 0.+1.j]\n",
      "RESULT: False\n",
      "reward till now:  -141.0\n",
      "episode number:  173\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 0.+1.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.70710678+0.j         0.        -0.70710678j]\n",
      "RESULT: False\n",
      "reward till now:  -141.0\n",
      "episode number:  173\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.70710678+0.j         0.        -0.70710678j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -141.0\n",
      "episode number:  173\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -142.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 174 reward -142.00, Last 30ep Avg. rewards -142.00.\n",
      "episode number:  174\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.29706842-0.16476357j 0.77160241-0.53780391j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.29706842-0.16476357j 0.77160241-0.53780391j]\n",
      "RESULT: False\n",
      "reward till now:  -142.0\n",
      "episode number:  174\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.29706842-0.16476357j 0.77160241-0.53780391j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -142.0\n",
      "episode number:  174\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.-1.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -142.0\n",
      "episode number:  174\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.-1.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -142.0\n",
      "episode number:  174\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -143.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 175 reward -143.00, Last 30ep Avg. rewards -143.00.\n",
      "episode number:  175\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.46295133-0.39081748j -0.31017822+0.73261671j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -143.0\n",
      "episode number:  175\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -143.0\n",
      "episode number:  175\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.5-0.5j  0.5-0.5j]\n",
      "RESULT: False\n",
      "reward till now:  -143.0\n",
      "episode number:  175\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.5-0.5j  0.5-0.5j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -143.0\n",
      "episode number:  175\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -144.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 176 reward -144.00, Last 30ep Avg. rewards -144.00.\n",
      "episode number:  176\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.03737407+0.21643354j  0.93756991+0.26967084j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -144.0\n",
      "episode number:  176\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -144.0\n",
      "episode number:  176\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.5-0.5j  0.5-0.5j]\n",
      "RESULT: False\n",
      "reward till now:  -144.0\n",
      "episode number:  176\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.5-0.5j  0.5-0.5j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -144.0\n",
      "episode number:  176\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -145.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 177 reward -145.00, Last 30ep Avg. rewards -145.00.\n",
      "episode number:  177\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.64935605-0.74423154j  0.14622468+0.05544791j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -145.0\n",
      "episode number:  177\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -145.0\n",
      "episode number:  177\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -145.0\n",
      "episode number:  177\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -145.0\n",
      "episode number:  177\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -146.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 178 reward -146.00, Last 30ep Avg. rewards -146.00.\n",
      "episode number:  178\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.56514948+0.14530471j  0.80824597+0.07893705j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.34380411-0.46877026j -0.67426215-0.45543795j]\n",
      "RESULT: False\n",
      "reward till now:  -146.0\n",
      "episode number:  178\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.34380411-0.46877026j -0.67426215-0.45543795j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -146.0\n",
      "episode number:  178\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -146.0\n",
      "episode number:  178\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.5-0.5j  0.5-0.5j]\n",
      "RESULT: False\n",
      "reward till now:  -146.0\n",
      "episode number:  178\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.5-0.5j  0.5-0.5j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -147.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 179 reward -147.00, Last 30ep Avg. rewards -147.00.\n",
      "episode number:  179\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.93985381+0.11317698j 0.30299077+0.10982888j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -147.0\n",
      "episode number:  179\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -147.0\n",
      "episode number:  179\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -147.0\n",
      "episode number:  179\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -147.0\n",
      "episode number:  179\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -148.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 180 reward -148.00, Last 30ep Avg. rewards -148.00.\n",
      "episode number:  180\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.4088684 -0.80677116j  0.31851736-0.28371395j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.06388783-0.77108942j -0.51433941-0.3698573j ]\n",
      "RESULT: False\n",
      "reward till now:  -148.0\n",
      "episode number:  180\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.06388783-0.77108942j -0.51433941-0.3698573j ]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -148.0\n",
      "episode number:  180\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -148.0\n",
      "episode number:  180\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -148.0\n",
      "episode number:  180\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -149.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 181 reward -149.00, Last 30ep Avg. rewards -149.00.\n",
      "episode number:  181\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.24827305-0.72814421j -0.6330137 +0.08637224j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -149.0\n",
      "episode number:  181\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -149.0\n",
      "episode number:  181\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -149.0\n",
      "episode number:  181\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -149.0\n",
      "episode number:  181\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -150.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 182 reward -150.00, Last 30ep Avg. rewards -150.00.\n",
      "episode number:  182\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.54473879-0.62026407j -0.24518011-0.50834914j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.50834914+0.24518011j  0.62026407+0.54473879j]\n",
      "RESULT: False\n",
      "reward till now:  -150.0\n",
      "episode number:  182\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.50834914+0.24518011j  0.62026407+0.54473879j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -150.0\n",
      "episode number:  182\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -150.0\n",
      "episode number:  182\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -150.0\n",
      "episode number:  182\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -151.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 183 reward -151.00, Last 30ep Avg. rewards -151.00.\n",
      "episode number:  183\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.16937588-0.37093779j 0.89827792-0.16374904j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -151.0\n",
      "episode number:  183\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -151.0\n",
      "episode number:  183\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -151.0\n",
      "episode number:  183\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.5+0.5j 0.5+0.5j]\n",
      "RESULT: False\n",
      "reward till now:  -151.0\n",
      "episode number:  183\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.5+0.5j 0.5+0.5j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -152.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 184 reward -152.00, Last 30ep Avg. rewards -152.00.\n",
      "episode number:  184\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.22226374+0.3201859j  0.91735481+0.08087008j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [ 0.08087008-0.91735481j -0.3201859 +0.22226374j]\n",
      "RESULT: False\n",
      "reward till now:  -152.0\n",
      "episode number:  184\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.08087008-0.91735481j -0.3201859 +0.22226374j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -152.0\n",
      "episode number:  184\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -152.0\n",
      "episode number:  184\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -152.0\n",
      "episode number:  184\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -153.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 185 reward -153.00, Last 30ep Avg. rewards -153.00.\n",
      "episode number:  185\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.5739425 -0.63812264j -0.49397074-0.13922077j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.05654857-0.54966479j 0.75512869-0.3527769j ]\n",
      "RESULT: False\n",
      "reward till now:  -153.0\n",
      "episode number:  185\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.05654857-0.54966479j 0.75512869-0.3527769j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -153.0\n",
      "episode number:  185\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -153.0\n",
      "episode number:  185\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -153.0\n",
      "episode number:  185\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -154.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 186 reward -154.00, Last 30ep Avg. rewards -154.00.\n",
      "episode number:  186\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.42248879-0.22999217j 0.38583198+0.78723599j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -154.0\n",
      "episode number:  186\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -154.0\n",
      "episode number:  186\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -154.0\n",
      "episode number:  186\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -154.0\n",
      "episode number:  186\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -155.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 187 reward -155.00, Last 30ep Avg. rewards -155.00.\n",
      "episode number:  187\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.77727787+0.2803367j  -0.19786525-0.52735168j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -155.0\n",
      "episode number:  187\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.70710678+0.j         0.        +0.70710678j]\n",
      "RESULT: False\n",
      "reward till now:  -155.0\n",
      "episode number:  187\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.70710678+0.j         0.        +0.70710678j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.70710678+0.j         0.        +0.70710678j]\n",
      "RESULT: False\n",
      "reward till now:  -155.0\n",
      "episode number:  187\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.70710678+0.j         0.        +0.70710678j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -155.0\n",
      "episode number:  187\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -156.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 188 reward -156.00, Last 30ep Avg. rewards -156.00.\n",
      "episode number:  188\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.40412818-0.81637757j 0.16081635+0.37992919j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -156.0\n",
      "episode number:  188\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -156.0\n",
      "episode number:  188\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -156.0\n",
      "episode number:  188\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -156.0\n",
      "episode number:  188\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -157.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 189 reward -157.00, Last 30ep Avg. rewards -157.00.\n",
      "episode number:  189\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.64846566+0.47206034j 0.55358452+0.22404353j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -157.0\n",
      "episode number:  189\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -157.0\n",
      "episode number:  189\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -157.0\n",
      "episode number:  189\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -157.0\n",
      "episode number:  189\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -158.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 190 reward -158.00, Last 30ep Avg. rewards -158.00.\n",
      "episode number:  190\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.26546921-0.01725074j -0.96300087+0.04310256j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.04310256+0.96300087j 0.01725074+0.26546921j]\n",
      "RESULT: False\n",
      "reward till now:  -158.0\n",
      "episode number:  190\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.04310256+0.96300087j 0.01725074+0.26546921j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -158.0\n",
      "episode number:  190\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -158.0\n",
      "episode number:  190\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -157.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 191 reward -157.00, Last 30ep Avg. rewards -157.00.\n",
      "episode number:  191\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.65393353+0.06747572j 0.4974863 +0.56597292j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -157.0\n",
      "episode number:  191\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -157.0\n",
      "episode number:  191\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -157.0\n",
      "episode number:  191\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -157.0\n",
      "episode number:  191\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -158.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 192 reward -158.00, Last 30ep Avg. rewards -158.00.\n",
      "episode number:  192\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.29078445-0.17299522j 0.9320966 -0.12927869j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.86470748-0.21373993j -0.45347617-0.03091226j]\n",
      "RESULT: False\n",
      "reward till now:  -158.0\n",
      "episode number:  192\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.86470748-0.21373993j -0.45347617-0.03091226j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [ 0.86470748-0.21373993j -0.45347617-0.03091226j]\n",
      "RESULT: False\n",
      "reward till now:  -158.0\n",
      "episode number:  192\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.86470748-0.21373993j -0.45347617-0.03091226j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [ 0.86470748-0.21373993j -0.45347617-0.03091226j]\n",
      "RESULT: False\n",
      "reward till now:  -158.0\n",
      "episode number:  192\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.86470748-0.21373993j -0.45347617-0.03091226j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [ 0.86470748-0.21373993j -0.45347617-0.03091226j]\n",
      "RESULT: False\n",
      "reward till now:  -158.0\n",
      "episode number:  192\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.86470748-0.21373993j -0.45347617-0.03091226j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [ 0.86470748-0.21373993j -0.45347617-0.03091226j]\n",
      "RESULT: False\n",
      "reward till now:  -159.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 193 reward -159.00, Last 30ep Avg. rewards -159.00.\n",
      "episode number:  193\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.43373139-0.47578661j -0.54472185+0.53738467j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.07848211+0.0435564j   0.69187093-0.71642028j]\n",
      "RESULT: False\n",
      "reward till now:  -159.0\n",
      "episode number:  193\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.07848211+0.0435564j   0.69187093-0.71642028j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -159.0\n",
      "episode number:  193\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -159.0\n",
      "episode number:  193\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -159.0\n",
      "episode number:  193\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -160.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 194 reward -160.00, Last 30ep Avg. rewards -160.00.\n",
      "episode number:  194\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.35737578+0.46412066j 0.22740837+0.7779203j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -160.0\n",
      "episode number:  194\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -160.0\n",
      "episode number:  194\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -160.0\n",
      "episode number:  194\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -160.0\n",
      "episode number:  194\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -161.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 195 reward -161.00, Last 30ep Avg. rewards -161.00.\n",
      "episode number:  195\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.08384412-0.63874022j  0.47342073-0.60071117j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -161.0\n",
      "episode number:  195\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -161.0\n",
      "episode number:  195\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -161.0\n",
      "episode number:  195\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -161.0\n",
      "episode number:  195\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -162.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 196 reward -162.00, Last 30ep Avg. rewards -162.00.\n",
      "episode number:  196\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.69818398+0.08360584j 0.20627846-0.68043985j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -162.0\n",
      "episode number:  196\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -162.0\n",
      "episode number:  196\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -162.0\n",
      "episode number:  196\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -162.0\n",
      "episode number:  196\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -163.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 197 reward -163.00, Last 30ep Avg. rewards -163.00.\n",
      "episode number:  197\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.15407327+0.76251571j 0.57205563-0.25996841j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -163.0\n",
      "episode number:  197\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -163.0\n",
      "episode number:  197\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.70710678+0.j          0.        -0.70710678j]\n",
      "RESULT: False\n",
      "reward till now:  -163.0\n",
      "episode number:  197\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.70710678+0.j          0.        -0.70710678j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -163.0\n",
      "episode number:  197\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -164.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 198 reward -164.00, Last 30ep Avg. rewards -164.00.\n",
      "episode number:  198\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.58421154-0.25606245j -0.4419079 +0.63075059j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -164.0\n",
      "episode number:  198\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -164.0\n",
      "episode number:  198\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -164.0\n",
      "episode number:  198\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -164.0\n",
      "episode number:  198\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -165.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 199 reward -165.00, Last 30ep Avg. rewards -165.00.\n",
      "episode number:  199\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.74680362-0.52049214j 0.05961437-0.40965645j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -165.0\n",
      "episode number:  199\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -165.0\n",
      "episode number:  199\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -165.0\n",
      "episode number:  199\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -165.0\n",
      "episode number:  199\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -166.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 200 reward -166.00, Last 30ep Avg. rewards -166.00.\n",
      "episode number:  200\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.18538985-0.10497257j -0.96972411-0.11935876j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.21548981+0.61147168j  0.75992531-0.04669103j]\n",
      "RESULT: False\n",
      "reward till now:  -166.0\n",
      "episode number:  200\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.21548981+0.61147168j  0.75992531-0.04669103j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -166.0\n",
      "episode number:  200\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -166.0\n",
      "episode number:  200\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -166.0\n",
      "episode number:  200\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -167.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 201 reward -167.00, Last 30ep Avg. rewards -167.00.\n",
      "episode number:  201\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.31547142+0.85617067j -0.31396027+0.26244718j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -167.0\n",
      "episode number:  201\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -166.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 202 reward -166.00, Last 30ep Avg. rewards -166.00.\n",
      "episode number:  202\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.16581366-0.14036474j  0.7358776 +0.64131718j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -166.0\n",
      "episode number:  202\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -166.0\n",
      "episode number:  202\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -166.0\n",
      "episode number:  202\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -166.0\n",
      "episode number:  202\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -167.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 203 reward -167.00, Last 30ep Avg. rewards -167.00.\n",
      "episode number:  203\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.58464348+0.23211085j 0.77581061-0.0493402j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.96198631+0.12923836j -0.13517557+0.19901595j]\n",
      "RESULT: False\n",
      "reward till now:  -167.0\n",
      "episode number:  203\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.96198631+0.12923836j -0.13517557+0.19901595j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.58464348+0.23211085j 0.77581061-0.0493402j ]\n",
      "RESULT: False\n",
      "reward till now:  -167.0\n",
      "episode number:  203\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.58464348+0.23211085j 0.77581061-0.0493402j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.96198631+0.12923836j -0.13517557+0.19901595j]\n",
      "RESULT: False\n",
      "reward till now:  -167.0\n",
      "episode number:  203\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.96198631+0.12923836j -0.13517557+0.19901595j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.58464348+0.23211085j 0.77581061-0.0493402j ]\n",
      "RESULT: False\n",
      "reward till now:  -167.0\n",
      "episode number:  203\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.58464348+0.23211085j 0.77581061-0.0493402j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.96198631+0.12923836j -0.13517557+0.19901595j]\n",
      "RESULT: False\n",
      "reward till now:  -168.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 204 reward -168.00, Last 30ep Avg. rewards -168.00.\n",
      "episode number:  204\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.18104424+0.55341512j -0.45114309-0.67633173j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -168.0\n",
      "episode number:  204\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -168.0\n",
      "episode number:  204\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.        +0.70710678j 0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -168.0\n",
      "episode number:  204\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.        +0.70710678j 0.70710678+0.j        ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -168.0\n",
      "episode number:  204\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -167.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 205 reward -167.00, Last 30ep Avg. rewards -167.00.\n",
      "episode number:  205\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.02999451-0.78241061j -0.06862784+0.61824282j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -167.0\n",
      "episode number:  205\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.70710678+0.j         0.        +0.70710678j]\n",
      "RESULT: False\n",
      "reward till now:  -167.0\n",
      "episode number:  205\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.70710678+0.j         0.        +0.70710678j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -167.0\n",
      "episode number:  205\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -166.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 206 reward -166.00, Last 30ep Avg. rewards -166.00.\n",
      "episode number:  206\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.14588835+0.29971547j  0.49327077+0.80347443j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -166.0\n",
      "episode number:  206\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -166.0\n",
      "episode number:  206\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -166.0\n",
      "episode number:  206\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -166.0\n",
      "episode number:  206\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -167.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 207 reward -167.00, Last 30ep Avg. rewards -167.00.\n",
      "episode number:  207\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.17374187+0.37127082j -0.14220252-0.9009718j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -167.0\n",
      "episode number:  207\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -166.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 208 reward -166.00, Last 30ep Avg. rewards -166.00.\n",
      "episode number:  208\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.60485903-0.3731494j   0.56569907+0.41819809j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -166.0\n",
      "episode number:  208\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -165.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 209 reward -165.00, Last 30ep Avg. rewards -165.00.\n",
      "episode number:  209\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.42085939-0.80291087j 0.13373531-0.40040775j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -165.0\n",
      "episode number:  209\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.70710678+0.j         0.        +0.70710678j]\n",
      "RESULT: False\n",
      "reward till now:  -165.0\n",
      "episode number:  209\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.70710678+0.j         0.        +0.70710678j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -165.0\n",
      "episode number:  209\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -164.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 210 reward -164.00, Last 30ep Avg. rewards -164.00.\n",
      "episode number:  210\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.05269194+0.01318908j 0.51267046+0.85686557j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -164.0\n",
      "episode number:  210\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -164.0\n",
      "episode number:  210\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -164.0\n",
      "episode number:  210\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -164.0\n",
      "episode number:  210\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -165.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 211 reward -165.00, Last 30ep Avg. rewards -165.00.\n",
      "episode number:  211\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.35147747-0.76102244j  0.54379449+0.03994982j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -165.0\n",
      "episode number:  211\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -165.0\n",
      "episode number:  211\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -165.0\n",
      "episode number:  211\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -165.0\n",
      "episode number:  211\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -166.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 212 reward -166.00, Last 30ep Avg. rewards -166.00.\n",
      "episode number:  212\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.74854407-0.17978542j  0.50109946-0.39529522j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -166.0\n",
      "episode number:  212\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -166.0\n",
      "episode number:  212\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -166.0\n",
      "episode number:  212\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -166.0\n",
      "episode number:  212\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -167.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 213 reward -167.00, Last 30ep Avg. rewards -167.00.\n",
      "episode number:  213\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.25624754+0.29365834j 0.40799396-0.82561668j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -167.0\n",
      "episode number:  213\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -166.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 214 reward -166.00, Last 30ep Avg. rewards -166.00.\n",
      "episode number:  214\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.01474884+0.28292603j  0.10608315+0.95314306j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -166.0\n",
      "episode number:  214\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.-1.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -166.0\n",
      "episode number:  214\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.-1.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -166.0\n",
      "episode number:  214\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -165.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 215 reward -165.00, Last 30ep Avg. rewards -165.00.\n",
      "episode number:  215\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.55617668+0.74794311j -0.35236078+0.08420503j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.1441196+0.58841759j 0.642433 +0.4693337j ]\n",
      "RESULT: False\n",
      "reward till now:  -165.0\n",
      "episode number:  215\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.1441196+0.58841759j 0.642433 +0.4693337j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -165.0\n",
      "episode number:  215\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -165.0\n",
      "episode number:  215\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -165.0\n",
      "episode number:  215\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -166.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 216 reward -166.00, Last 30ep Avg. rewards -166.00.\n",
      "episode number:  216\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.84864322+0.09373967j -0.32486249+0.40679469j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.3703689 +0.35393124j 0.82979385-0.22136333j]\n",
      "RESULT: False\n",
      "reward till now:  -166.0\n",
      "episode number:  216\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.3703689 +0.35393124j 0.82979385-0.22136333j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.84864322+0.09373967j -0.32486249+0.40679469j]\n",
      "RESULT: False\n",
      "reward till now:  -166.0\n",
      "episode number:  216\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.84864322+0.09373967j -0.32486249+0.40679469j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.3703689 +0.35393124j 0.82979385-0.22136333j]\n",
      "RESULT: False\n",
      "reward till now:  -166.0\n",
      "episode number:  216\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.3703689 +0.35393124j 0.82979385-0.22136333j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.84864322+0.09373967j -0.32486249+0.40679469j]\n",
      "RESULT: False\n",
      "reward till now:  -166.0\n",
      "episode number:  216\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.84864322+0.09373967j -0.32486249+0.40679469j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.3703689 +0.35393124j 0.82979385-0.22136333j]\n",
      "RESULT: False\n",
      "reward till now:  -167.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 217 reward -167.00, Last 30ep Avg. rewards -167.00.\n",
      "episode number:  217\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.35545179-0.79708795j  0.45336808+0.18100333j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -167.0\n",
      "episode number:  217\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -166.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 218 reward -166.00, Last 30ep Avg. rewards -166.00.\n",
      "episode number:  218\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.99265135+0.02513404j -0.02947052-0.11464317j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -166.0\n",
      "episode number:  218\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -165.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 219 reward -165.00, Last 30ep Avg. rewards -165.00.\n",
      "episode number:  219\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.0574558 -0.47031213j -0.87765264+0.07232684j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.09177018+0.28803324j 0.95315503-0.01051541j]\n",
      "RESULT: False\n",
      "reward till now:  -165.0\n",
      "episode number:  219\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.09177018+0.28803324j 0.95315503-0.01051541j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.7388737 +0.19623474j -0.60909107+0.21110578j]\n",
      "RESULT: False\n",
      "reward till now:  -165.0\n",
      "episode number:  219\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.7388737 +0.19623474j -0.60909107+0.21110578j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.67173693+0.56945134j 0.29193351+0.37318828j]\n",
      "RESULT: False\n",
      "reward till now:  -165.0\n",
      "episode number:  219\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.67173693+0.56945134j 0.29193351+0.37318828j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -165.0\n",
      "episode number:  219\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -164.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 220 reward -164.00, Last 30ep Avg. rewards -164.00.\n",
      "episode number:  220\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.63359042-0.68952822j -0.32547979-0.13106076j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.21786712-0.58024404j 0.67816505-0.39489612j]\n",
      "RESULT: False\n",
      "reward till now:  -164.0\n",
      "episode number:  220\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.21786712-0.58024404j 0.67816505-0.39489612j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -164.0\n",
      "episode number:  220\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -164.0\n",
      "episode number:  220\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.5-0.5j  0.5-0.5j]\n",
      "RESULT: False\n",
      "reward till now:  -164.0\n",
      "episode number:  220\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.5-0.5j  0.5-0.5j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -165.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 221 reward -165.00, Last 30ep Avg. rewards -165.00.\n",
      "episode number:  221\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.32440959+0.76292014j  0.03417163-0.55816088j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -165.0\n",
      "episode number:  221\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -165.0\n",
      "episode number:  221\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.5-0.5j  0.5-0.5j]\n",
      "RESULT: False\n",
      "reward till now:  -165.0\n",
      "episode number:  221\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.5-0.5j  0.5-0.5j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -165.0\n",
      "episode number:  221\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -164.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 222 reward -164.00, Last 30ep Avg. rewards -164.00.\n",
      "episode number:  222\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.77176157+0.12374114j -0.61817888+0.08322911j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.60456971+0.52461668j 0.34962028+0.48686597j]\n",
      "RESULT: False\n",
      "reward till now:  -164.0\n",
      "episode number:  222\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.60456971+0.52461668j 0.34962028+0.48686597j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -164.0\n",
      "episode number:  222\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -164.0\n",
      "episode number:  222\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -163.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 223 reward -163.00, Last 30ep Avg. rewards -163.00.\n",
      "episode number:  223\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.03370467-0.21821036j  0.401497  +0.88884667j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -163.0\n",
      "episode number:  223\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -163.0\n",
      "episode number:  223\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.5-0.5j  0.5-0.5j]\n",
      "RESULT: False\n",
      "reward till now:  -163.0\n",
      "episode number:  223\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.5-0.5j  0.5-0.5j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -163.0\n",
      "episode number:  223\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -164.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 224 reward -164.00, Last 30ep Avg. rewards -164.00.\n",
      "episode number:  224\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.08666066-0.6644378j -0.05319452-0.7403936j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -164.0\n",
      "episode number:  224\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -163.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 225 reward -163.00, Last 30ep Avg. rewards -163.00.\n",
      "episode number:  225\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.36060104-0.05649885j -0.9158218 +0.16746699j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.90256725+0.07846633j  0.39260037-0.15836776j]\n",
      "RESULT: False\n",
      "reward till now:  -163.0\n",
      "episode number:  225\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.90256725+0.07846633j  0.39260037-0.15836776j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.90256725+0.07846633j  0.39260037-0.15836776j]\n",
      "RESULT: False\n",
      "reward till now:  -163.0\n",
      "episode number:  225\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.90256725+0.07846633j  0.39260037-0.15836776j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.90256725+0.07846633j  0.39260037-0.15836776j]\n",
      "RESULT: False\n",
      "reward till now:  -163.0\n",
      "episode number:  225\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.90256725+0.07846633j  0.39260037-0.15836776j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [ 0.39260037-0.15836776j -0.90256725+0.07846633j]\n",
      "RESULT: False\n",
      "reward till now:  -163.0\n",
      "episode number:  225\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.39260037-0.15836776j -0.90256725+0.07846633j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.33309446+0.5262285j  0.75019434+0.22212631j]\n",
      "RESULT: False\n",
      "reward till now:  -164.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 226 reward -164.00, Last 30ep Avg. rewards -164.00.\n",
      "episode number:  226\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.02919192+0.18776224j  0.33286511-0.92363087j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -164.0\n",
      "episode number:  226\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -164.0\n",
      "episode number:  226\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.5-0.5j  0.5-0.5j]\n",
      "RESULT: False\n",
      "reward till now:  -164.0\n",
      "episode number:  226\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.5-0.5j  0.5-0.5j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -164.0\n",
      "episode number:  226\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -165.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 227 reward -165.00, Last 30ep Avg. rewards -165.00.\n",
      "episode number:  227\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.85195882+0.04031162j 0.16331542-0.49585201j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.7179073 -0.3221157j  0.48694442+0.37912494j]\n",
      "RESULT: False\n",
      "reward till now:  -165.0\n",
      "episode number:  227\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.7179073 -0.3221157j  0.48694442+0.37912494j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -165.0\n",
      "episode number:  227\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -165.0\n",
      "episode number:  227\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.5-0.5j  0.5-0.5j]\n",
      "RESULT: False\n",
      "reward till now:  -165.0\n",
      "episode number:  227\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.5-0.5j  0.5-0.5j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -166.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 228 reward -166.00, Last 30ep Avg. rewards -166.00.\n",
      "episode number:  228\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.14421339-0.22391321j -0.96055932+0.07994474j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.15850374+0.52088746j 0.83754856+0.04544479j]\n",
      "RESULT: False\n",
      "reward till now:  -166.0\n",
      "episode number:  228\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.15850374+0.52088746j 0.83754856+0.04544479j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70431533+0.40045737j -0.4801572 +0.33618873j]\n",
      "RESULT: False\n",
      "reward till now:  -166.0\n",
      "episode number:  228\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70431533+0.40045737j -0.4801572 +0.33618873j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.15850374+0.52088746j 0.83754856+0.04544479j]\n",
      "RESULT: False\n",
      "reward till now:  -166.0\n",
      "episode number:  228\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.15850374+0.52088746j 0.83754856+0.04544479j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70431533+0.40045737j -0.4801572 +0.33618873j]\n",
      "RESULT: False\n",
      "reward till now:  -166.0\n",
      "episode number:  228\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70431533+0.40045737j -0.4801572 +0.33618873j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.15850374+0.52088746j 0.83754856+0.04544479j]\n",
      "RESULT: False\n",
      "reward till now:  -167.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 229 reward -167.00, Last 30ep Avg. rewards -167.00.\n",
      "episode number:  229\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.1633331 -0.35796524j -0.90552288-0.1587813j ]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.00321861+0.38718172j 0.89342102+0.22776928j]\n",
      "RESULT: False\n",
      "reward till now:  -167.0\n",
      "episode number:  229\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.00321861+0.38718172j 0.89342102+0.22776928j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.63401996+0.43483602j -0.62946816+0.11272162j]\n",
      "RESULT: False\n",
      "reward till now:  -167.0\n",
      "episode number:  229\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.63401996+0.43483602j -0.62946816+0.11272162j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.52802604+0.7525767j  0.1376257 +0.36861359j]\n",
      "RESULT: False\n",
      "reward till now:  -167.0\n",
      "episode number:  229\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.52802604+0.7525767j  0.1376257 +0.36861359j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -167.0\n",
      "episode number:  229\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.-1.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -168.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 230 reward -168.00, Last 30ep Avg. rewards -168.00.\n",
      "episode number:  230\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.41495565+0.19156887j -0.82910875+0.32201221j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.29285046+0.36315666j  0.87968637-0.09223737j]\n",
      "RESULT: False\n",
      "reward till now:  -168.0\n",
      "episode number:  230\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.29285046+0.36315666j  0.87968637-0.09223737j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.29285046+0.36315666j  0.87968637-0.09223737j]\n",
      "RESULT: False\n",
      "reward till now:  -168.0\n",
      "episode number:  230\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.29285046+0.36315666j  0.87968637-0.09223737j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.29285046+0.36315666j  0.87968637-0.09223737j]\n",
      "RESULT: False\n",
      "reward till now:  -168.0\n",
      "episode number:  230\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.29285046+0.36315666j  0.87968637-0.09223737j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.29285046+0.36315666j  0.87968637-0.09223737j]\n",
      "RESULT: False\n",
      "reward till now:  -168.0\n",
      "episode number:  230\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.29285046+0.36315666j  0.87968637-0.09223737j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.29285046+0.36315666j  0.87968637-0.09223737j]\n",
      "RESULT: False\n",
      "reward till now:  -169.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 231 reward -169.00, Last 30ep Avg. rewards -169.00.\n",
      "episode number:  231\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.01663007-0.02981018j -0.67148528-0.74023125j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.46305256-0.54450152j  0.48657103+0.50234356j]\n",
      "RESULT: False\n",
      "reward till now:  -169.0\n",
      "episode number:  231\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.46305256-0.54450152j  0.48657103+0.50234356j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -169.0\n",
      "episode number:  231\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -169.0\n",
      "episode number:  231\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -169.0\n",
      "episode number:  231\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -170.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 232 reward -170.00, Last 30ep Avg. rewards -170.00.\n",
      "episode number:  232\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.37642053+0.91100976j -0.13851923-0.09581866j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -170.0\n",
      "episode number:  232\n",
      "time step:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -169.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 233 reward -169.00, Last 30ep Avg. rewards -169.00.\n",
      "episode number:  233\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.32938914+0.91879952j  0.20701262+0.06675336j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -169.0\n",
      "episode number:  233\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -168.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 234 reward -168.00, Last 30ep Avg. rewards -168.00.\n",
      "episode number:  234\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.94406597+0.21990497j -0.0148971 -0.24527399j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.65702161-0.01793861j 0.67808928+0.3289312j ]\n",
      "RESULT: False\n",
      "reward till now:  -168.0\n",
      "episode number:  234\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.65702161-0.01793861j 0.67808928+0.3289312j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.94406597+0.21990497j -0.0148971 -0.24527399j]\n",
      "RESULT: False\n",
      "reward till now:  -168.0\n",
      "episode number:  234\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.94406597+0.21990497j -0.0148971 -0.24527399j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.65702161-0.01793861j 0.67808928+0.3289312j ]\n",
      "RESULT: False\n",
      "reward till now:  -168.0\n",
      "episode number:  234\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.65702161-0.01793861j 0.67808928+0.3289312j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.94406597+0.21990497j -0.0148971 -0.24527399j]\n",
      "RESULT: False\n",
      "reward till now:  -168.0\n",
      "episode number:  234\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.94406597+0.21990497j -0.0148971 -0.24527399j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.65702161-0.01793861j 0.67808928+0.3289312j ]\n",
      "RESULT: False\n",
      "reward till now:  -169.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 235 reward -169.00, Last 30ep Avg. rewards -169.00.\n",
      "episode number:  235\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.0615048 +0.95064349j 0.01362635+0.30382304j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [ 0.30382304-0.01362635j -0.95064349+0.0615048j ]\n",
      "RESULT: False\n",
      "reward till now:  -169.0\n",
      "episode number:  235\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.30382304-0.01362635j -0.95064349+0.0615048j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.45737112+0.03385518j  0.88704179-0.05312574j]\n",
      "RESULT: False\n",
      "reward till now:  -169.0\n",
      "episode number:  235\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.45737112+0.03385518j  0.88704179-0.05312574j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.45737112+0.03385518j  0.88704179-0.05312574j]\n",
      "RESULT: False\n",
      "reward till now:  -169.0\n",
      "episode number:  235\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.45737112+0.03385518j  0.88704179-0.05312574j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.36097579-0.60329404j -0.65117249-0.28584465j]\n",
      "RESULT: False\n",
      "reward till now:  -169.0\n",
      "episode number:  235\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.36097579-0.60329404j -0.65117249-0.28584465j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.71569692-0.628716j    0.20520005-0.22447062j]\n",
      "RESULT: False\n",
      "reward till now:  -170.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 236 reward -170.00, Last 30ep Avg. rewards -170.00.\n",
      "episode number:  236\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.67434864+0.59242592j 0.15441591-0.41284521j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -170.0\n",
      "episode number:  236\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -169.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 237 reward -169.00, Last 30ep Avg. rewards -169.00.\n",
      "episode number:  237\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.30277349-0.15459821j -0.93291164-0.11875804j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.30277349-0.15459821j -0.93291164-0.11875804j]\n",
      "RESULT: False\n",
      "reward till now:  -169.0\n",
      "episode number:  237\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.30277349-0.15459821j -0.93291164-0.11875804j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.30277349-0.15459821j -0.93291164-0.11875804j]\n",
      "RESULT: False\n",
      "reward till now:  -169.0\n",
      "episode number:  237\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.30277349-0.15459821j -0.93291164-0.11875804j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.30277349-0.15459821j -0.93291164-0.11875804j]\n",
      "RESULT: False\n",
      "reward till now:  -169.0\n",
      "episode number:  237\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.30277349-0.15459821j -0.93291164-0.11875804j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.11875804+0.93291164j  0.15459821-0.30277349j]\n",
      "RESULT: False\n",
      "reward till now:  -169.0\n",
      "episode number:  237\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.11875804+0.93291164j  0.15459821-0.30277349j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -170.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 238 reward -170.00, Last 30ep Avg. rewards -170.00.\n",
      "episode number:  238\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.30433539-0.1057392j   0.49486398-0.80703707j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -170.0\n",
      "episode number:  238\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -170.0\n",
      "episode number:  238\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -170.0\n",
      "episode number:  238\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -170.0\n",
      "episode number:  238\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -171.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 239 reward -171.00, Last 30ep Avg. rewards -171.00.\n",
      "episode number:  239\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.41895577-0.57848893j -0.21363065-0.66647473j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -171.0\n",
      "episode number:  239\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -170.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 240 reward -170.00, Last 30ep Avg. rewards -170.00.\n",
      "episode number:  240\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.42543271-0.70654229j -0.35778138-0.4379469j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -170.0\n",
      "episode number:  240\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -169.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]]\n",
      "Episode 241 reward -169.00, Last 30ep Avg. rewards -169.00.\n",
      "episode number:  241\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.78136675+0.09764324j -0.0811182 +0.61102507j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -169.0\n",
      "episode number:  241\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -169.0\n",
      "episode number:  241\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -169.0\n",
      "episode number:  241\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -169.0\n",
      "episode number:  241\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -170.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 242 reward -170.00, Last 30ep Avg. rewards -170.00.\n",
      "episode number:  242\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.75738647-0.50486131j 0.3127153 +0.27145889j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -170.0\n",
      "episode number:  242\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -170.0\n",
      "episode number:  242\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -170.0\n",
      "episode number:  242\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -170.0\n",
      "episode number:  242\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -171.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 243 reward -171.00, Last 30ep Avg. rewards -171.00.\n",
      "episode number:  243\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.75757814-0.21558742j  0.43158587+0.43969428j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -171.0\n",
      "episode number:  243\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -170.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 244 reward -170.00, Last 30ep Avg. rewards -170.00.\n",
      "episode number:  244\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.94673978-0.29325387j -0.12832052-0.03492568j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.57870981-0.23205798j 0.76018243-0.18266561j]\n",
      "RESULT: False\n",
      "reward till now:  -170.0\n",
      "episode number:  244\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.57870981-0.23205798j 0.76018243-0.18266561j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.94673978-0.29325387j -0.12832052-0.03492568j]\n",
      "RESULT: False\n",
      "reward till now:  -170.0\n",
      "episode number:  244\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.94673978-0.29325387j -0.12832052-0.03492568j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.57870981-0.23205798j 0.76018243-0.18266561j]\n",
      "RESULT: False\n",
      "reward till now:  -170.0\n",
      "episode number:  244\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.57870981-0.23205798j 0.76018243-0.18266561j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.94673978-0.29325387j -0.12832052-0.03492568j]\n",
      "RESULT: False\n",
      "reward till now:  -170.0\n",
      "episode number:  244\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.94673978-0.29325387j -0.12832052-0.03492568j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.57870981-0.23205798j 0.76018243-0.18266561j]\n",
      "RESULT: False\n",
      "reward till now:  -171.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 245 reward -171.00, Last 30ep Avg. rewards -171.00.\n",
      "episode number:  245\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.70895398+0.60653269j -0.35955335-0.01495788j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.24706356+0.41830656j 0.75554878+0.4394602j ]\n",
      "RESULT: False\n",
      "reward till now:  -171.0\n",
      "episode number:  245\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.24706356+0.41830656j 0.75554878+0.4394602j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -171.0\n",
      "episode number:  245\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -171.0\n",
      "episode number:  245\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -171.0\n",
      "episode number:  245\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -172.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 246 reward -172.00, Last 30ep Avg. rewards -172.00.\n",
      "episode number:  246\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.46687041+0.02878146j -0.70331649+0.53530324j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -172.0\n",
      "episode number:  246\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -171.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 247 reward -171.00, Last 30ep Avg. rewards -171.00.\n",
      "episode number:  247\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.41095398+0.79422677j -0.07880098+0.44058038j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -171.0\n",
      "episode number:  247\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -170.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 248 reward -170.00, Last 30ep Avg. rewards -170.00.\n",
      "episode number:  248\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.80162552+0.20221674j -0.56257885+0.00315587j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.16903152+0.14522036j 0.96463816+0.14075729j]\n",
      "RESULT: False\n",
      "reward till now:  -170.0\n",
      "episode number:  248\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.16903152+0.14522036j 0.96463816+0.14075729j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.16903152+0.14522036j 0.96463816+0.14075729j]\n",
      "RESULT: False\n",
      "reward till now:  -170.0\n",
      "episode number:  248\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.16903152+0.14522036j 0.96463816+0.14075729j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.80162552+0.20221674j -0.56257885+0.00315587j]\n",
      "RESULT: False\n",
      "reward till now:  -170.0\n",
      "episode number:  248\n",
      "time step:  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL STATE:  [[[ 0.80162552+0.20221674j -0.56257885+0.00315587j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.16903152+0.14522036j 0.96463816+0.14075729j]\n",
      "RESULT: False\n",
      "reward till now:  -170.0\n",
      "episode number:  248\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.16903152+0.14522036j 0.96463816+0.14075729j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.80162552+0.20221674j -0.56257885+0.00315587j]\n",
      "RESULT: False\n",
      "reward till now:  -171.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 249 reward -171.00, Last 30ep Avg. rewards -171.00.\n",
      "episode number:  249\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.16310202+0.54388759j 0.75402823-0.33019002j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -171.0\n",
      "episode number:  249\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -170.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 250 reward -170.00, Last 30ep Avg. rewards -170.00.\n",
      "episode number:  250\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.22836075+0.07757527j -0.95173872+0.1898074j ]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.22836075+0.07757527j -0.95173872+0.1898074j ]\n",
      "RESULT: False\n",
      "reward till now:  -170.0\n",
      "episode number:  250\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.22836075+0.07757527j -0.95173872+0.1898074j ]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.22836075+0.07757527j -0.95173872+0.1898074j ]\n",
      "RESULT: False\n",
      "reward till now:  -170.0\n",
      "episode number:  250\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.22836075+0.07757527j -0.95173872+0.1898074j ]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.22836075+0.07757527j -0.95173872+0.1898074j ]\n",
      "RESULT: False\n",
      "reward till now:  -170.0\n",
      "episode number:  250\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.22836075+0.07757527j -0.95173872+0.1898074j ]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.22836075+0.07757527j -0.95173872+0.1898074j ]\n",
      "RESULT: False\n",
      "reward till now:  -170.0\n",
      "episode number:  250\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.22836075+0.07757527j -0.95173872+0.1898074j ]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.22836075+0.07757527j -0.95173872+0.1898074j ]\n",
      "RESULT: False\n",
      "reward till now:  -171.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 251 reward -171.00, Last 30ep Avg. rewards -171.00.\n",
      "episode number:  251\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.42663112-0.59784108j -0.25884033-0.62735446j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -171.0\n",
      "episode number:  251\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -170.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 252 reward -170.00, Last 30ep Avg. rewards -170.00.\n",
      "episode number:  252\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.6322788-0.27507512j -0.5724732+0.44365712j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.04228895+0.11920547j 0.85188831-0.50822044j]\n",
      "RESULT: False\n",
      "reward till now:  -170.0\n",
      "episode number:  252\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.04228895+0.11920547j 0.85188831-0.50822044j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -170.0\n",
      "episode number:  252\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -170.0\n",
      "episode number:  252\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -170.0\n",
      "episode number:  252\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -171.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 253 reward -171.00, Last 30ep Avg. rewards -171.00.\n",
      "episode number:  253\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.5569184-0.03151179j 0.0382472-0.82908748j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -171.0\n",
      "episode number:  253\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -171.0\n",
      "episode number:  253\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -171.0\n",
      "episode number:  253\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -171.0\n",
      "episode number:  253\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -172.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 254 reward -172.00, Last 30ep Avg. rewards -172.00.\n",
      "episode number:  254\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.48170907-0.15140859j -0.59621037+0.62415142j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.08096465+0.33427966j  0.76220415-0.54840374j]\n",
      "RESULT: False\n",
      "reward till now:  -172.0\n",
      "episode number:  254\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.08096465+0.33427966j  0.76220415-0.54840374j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -172.0\n",
      "episode number:  254\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -172.0\n",
      "episode number:  254\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -172.0\n",
      "episode number:  254\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -173.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 255 reward -173.00, Last 30ep Avg. rewards -173.00.\n",
      "episode number:  255\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.7821807 +0.25455235j -0.49003849+0.28854588j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.20657574+0.38402844j 0.89959482-0.02403705j]\n",
      "RESULT: False\n",
      "reward till now:  -173.0\n",
      "episode number:  255\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.20657574+0.38402844j 0.89959482-0.02403705j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.7821807 +0.25455235j -0.49003849+0.28854588j]\n",
      "RESULT: False\n",
      "reward till now:  -173.0\n",
      "episode number:  255\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.7821807 +0.25455235j -0.49003849+0.28854588j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.20657574+0.38402844j 0.89959482-0.02403705j]\n",
      "RESULT: False\n",
      "reward till now:  -173.0\n",
      "episode number:  255\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.20657574+0.38402844j 0.89959482-0.02403705j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.7821807 +0.25455235j -0.49003849+0.28854588j]\n",
      "RESULT: False\n",
      "reward till now:  -173.0\n",
      "episode number:  255\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.7821807 +0.25455235j -0.49003849+0.28854588j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.20657574+0.38402844j 0.89959482-0.02403705j]\n",
      "RESULT: False\n",
      "reward till now:  -174.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 256 reward -174.00, Last 30ep Avg. rewards -174.00.\n",
      "episode number:  256\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.00796963-0.26027534j 0.10993129+0.95922278j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -174.0\n",
      "episode number:  256\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -174.0\n",
      "episode number:  256\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -174.0\n",
      "episode number:  256\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -174.0\n",
      "episode number:  256\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -175.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 257 reward -175.00, Last 30ep Avg. rewards -175.00.\n",
      "episode number:  257\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.07272588+0.86407836j  0.15444387+0.47352574j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.07272588+0.86407836j  0.15444387+0.47352574j]\n",
      "RESULT: False\n",
      "reward till now:  -175.0\n",
      "episode number:  257\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.07272588+0.86407836j  0.15444387+0.47352574j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -175.0\n",
      "episode number:  257\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -174.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 258 reward -174.00, Last 30ep Avg. rewards -174.00.\n",
      "episode number:  258\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.3072147-0.65661086j -0.1566138-0.67078568j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -174.0\n",
      "episode number:  258\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.70710678+0.j         0.        +0.70710678j]\n",
      "RESULT: False\n",
      "reward till now:  -174.0\n",
      "episode number:  258\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.70710678+0.j         0.        +0.70710678j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -174.0\n",
      "episode number:  258\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -174.0\n",
      "episode number:  258\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -175.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 259 reward -175.00, Last 30ep Avg. rewards -175.00.\n",
      "episode number:  259\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.39067303+0.66308047j -0.2834824 +0.57213338j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -175.0\n",
      "episode number:  259\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -175.0\n",
      "episode number:  259\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -174.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 260 reward -174.00, Last 30ep Avg. rewards -174.00.\n",
      "episode number:  260\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.07523912+0.93000494j 0.35970385-0.00655942j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -174.0\n",
      "episode number:  260\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -173.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]]\n",
      "Episode 261 reward -173.00, Last 30ep Avg. rewards -173.00.\n",
      "episode number:  261\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.29044675-0.21339064j 0.38995809+0.84737112j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -173.0\n",
      "episode number:  261\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -173.0\n",
      "episode number:  261\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -173.0\n",
      "episode number:  261\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -173.0\n",
      "episode number:  261\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -174.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 262 reward -174.00, Last 30ep Avg. rewards -174.00.\n",
      "episode number:  262\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.67709302+0.18489354j -0.50199777-0.50532927j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -174.0\n",
      "episode number:  262\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -174.0\n",
      "episode number:  262\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -173.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 263 reward -173.00, Last 30ep Avg. rewards -173.00.\n",
      "episode number:  263\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.00420691+0.66130524j -0.62488372+0.41494339j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.62488372+0.41494339j -0.00420691+0.66130524j]\n",
      "RESULT: False\n",
      "reward till now:  -173.0\n",
      "episode number:  263\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.62488372+0.41494339j -0.00420691+0.66130524j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -173.0\n",
      "episode number:  263\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -172.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 264 reward -172.00, Last 30ep Avg. rewards -172.00.\n",
      "episode number:  264\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.75515106-0.09371393j  0.2998196 -0.57538925j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -172.0\n",
      "episode number:  264\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -171.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 265 reward -171.00, Last 30ep Avg. rewards -171.00.\n",
      "episode number:  265\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.12514081+0.14895518j 0.83137629-0.52054356j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -171.0\n",
      "episode number:  265\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -171.0\n",
      "episode number:  265\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -171.0\n",
      "episode number:  265\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -171.0\n",
      "episode number:  265\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -172.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 266 reward -172.00, Last 30ep Avg. rewards -172.00.\n",
      "episode number:  266\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.23272191+0.41488711j  0.62319942+0.6207509j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -172.0\n",
      "episode number:  266\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -172.0\n",
      "episode number:  266\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -172.0\n",
      "episode number:  266\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -172.0\n",
      "episode number:  266\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -173.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 267 reward -173.00, Last 30ep Avg. rewards -173.00.\n",
      "episode number:  267\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.02002356-0.28737841j 0.512406  +0.80898257j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -173.0\n",
      "episode number:  267\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -173.0\n",
      "episode number:  267\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -173.0\n",
      "episode number:  267\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -173.0\n",
      "episode number:  267\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -174.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 268 reward -174.00, Last 30ep Avg. rewards -174.00.\n",
      "episode number:  268\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.0275608 +0.39135516j 0.79076778-0.46985941j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -174.0\n",
      "episode number:  268\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -174.0\n",
      "episode number:  268\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -174.0\n",
      "episode number:  268\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -174.0\n",
      "episode number:  268\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -175.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 269 reward -175.00, Last 30ep Avg. rewards -175.00.\n",
      "episode number:  269\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.68184596-0.2320612j 0.26640578+0.6405167j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -175.0\n",
      "episode number:  269\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -175.0\n",
      "episode number:  269\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -175.0\n",
      "episode number:  269\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -175.0\n",
      "episode number:  269\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -176.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 270 reward -176.00, Last 30ep Avg. rewards -176.00.\n",
      "episode number:  270\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.29570105-0.62007757j 0.04546846+0.72525673j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -176.0\n",
      "episode number:  270\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -176.0\n",
      "episode number:  270\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -176.0\n",
      "episode number:  270\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -176.0\n",
      "episode number:  270\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -177.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 271 reward -177.00, Last 30ep Avg. rewards -177.00.\n",
      "episode number:  271\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.14803703+0.13523082j -0.94715092-0.25040528j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -177.0\n",
      "episode number:  271\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -177.0\n",
      "episode number:  271\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -177.0\n",
      "episode number:  271\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -177.0\n",
      "episode number:  271\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -178.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 272 reward -178.00, Last 30ep Avg. rewards -178.00.\n",
      "episode number:  272\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.60163181+0.57151353j  0.06989236-0.55364836j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.81690645+0.35469973j -0.45354245-0.03392943j]\n",
      "RESULT: False\n",
      "reward till now:  -178.0\n",
      "episode number:  272\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.81690645+0.35469973j -0.45354245-0.03392943j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -178.0\n",
      "episode number:  272\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -177.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 273 reward -177.00, Last 30ep Avg. rewards -177.00.\n",
      "episode number:  273\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.236948  -0.88595917j  0.39728407+0.0331265j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -177.0\n",
      "episode number:  273\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -177.0\n",
      "episode number:  273\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -177.0\n",
      "episode number:  273\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -177.0\n",
      "episode number:  273\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -178.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 274 reward -178.00, Last 30ep Avg. rewards -178.00.\n",
      "episode number:  274\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.02397004+0.644571j   -0.76415606+0.00437943j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -178.0\n",
      "episode number:  274\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -178.0\n",
      "episode number:  274\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -178.0\n",
      "episode number:  274\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -178.0\n",
      "episode number:  274\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -179.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 275 reward -179.00, Last 30ep Avg. rewards -179.00.\n",
      "episode number:  275\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.2018473 -0.8427342j  0.48795577+0.10467042j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -179.0\n",
      "episode number:  275\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -178.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 276 reward -178.00, Last 30ep Avg. rewards -178.00.\n",
      "episode number:  276\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.61448338-0.47019072j  0.63037473+0.06291713j]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.61448338-0.47019072j  0.63037473+0.06291713j]\n",
      "RESULT: False\n",
      "reward till now:  -178.0\n",
      "episode number:  276\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.61448338-0.47019072j  0.63037473+0.06291713j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.61448338-0.47019072j  0.63037473+0.06291713j]\n",
      "RESULT: False\n",
      "reward till now:  -178.0\n",
      "episode number:  276\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.61448338-0.47019072j  0.63037473+0.06291713j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.61448338-0.47019072j  0.63037473+0.06291713j]\n",
      "RESULT: False\n",
      "reward till now:  -178.0\n",
      "episode number:  276\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.61448338-0.47019072j  0.63037473+0.06291713j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.61448338-0.47019072j  0.63037473+0.06291713j]\n",
      "RESULT: False\n",
      "reward till now:  -178.0\n",
      "episode number:  276\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.61448338-0.47019072j  0.63037473+0.06291713j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.61448338-0.47019072j  0.63037473+0.06291713j]\n",
      "RESULT: False\n",
      "reward till now:  -179.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 277 reward -179.00, Last 30ep Avg. rewards -179.00.\n",
      "episode number:  277\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.24831029-0.25277405j 0.53559498+0.76654112j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -179.0\n",
      "episode number:  277\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -179.0\n",
      "episode number:  277\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -179.0\n",
      "episode number:  277\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -179.0\n",
      "episode number:  277\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -180.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 278 reward -180.00, Last 30ep Avg. rewards -180.00.\n",
      "episode number:  278\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.54791474+0.09858292j 0.81058601+0.18171727j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.96060509+0.19820217j -0.18573664-0.05878486j]\n",
      "RESULT: False\n",
      "reward till now:  -180.0\n",
      "episode number:  278\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.96060509+0.19820217j -0.18573664-0.05878486j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.54791474+0.09858292j 0.81058601+0.18171727j]\n",
      "RESULT: False\n",
      "reward till now:  -180.0\n",
      "episode number:  278\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.54791474+0.09858292j 0.81058601+0.18171727j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.96060509+0.19820217j -0.18573664-0.05878486j]\n",
      "RESULT: False\n",
      "reward till now:  -180.0\n",
      "episode number:  278\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.96060509+0.19820217j -0.18573664-0.05878486j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.54791474+0.09858292j 0.81058601+0.18171727j]\n",
      "RESULT: False\n",
      "reward till now:  -180.0\n",
      "episode number:  278\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.54791474+0.09858292j 0.81058601+0.18171727j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.96060509+0.19820217j -0.18573664-0.05878486j]\n",
      "RESULT: False\n",
      "reward till now:  -181.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 279 reward -181.00, Last 30ep Avg. rewards -181.00.\n",
      "episode number:  279\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.7496485 -0.56259212j  0.34463912-0.05235572j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -181.0\n",
      "episode number:  279\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -180.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 280 reward -180.00, Last 30ep Avg. rewards -180.00.\n",
      "episode number:  280\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.43535334+0.82163736j  0.26666047+0.25351867j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -180.0\n",
      "episode number:  280\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -179.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]]\n",
      "Episode 281 reward -179.00, Last 30ep Avg. rewards -179.00.\n",
      "episode number:  281\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.11783811-0.27397268j -0.54655669-0.78251449j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -179.0\n",
      "episode number:  281\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -179.0\n",
      "episode number:  281\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -179.0\n",
      "episode number:  281\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -179.0\n",
      "episode number:  281\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -180.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 282 reward -180.00, Last 30ep Avg. rewards -180.00.\n",
      "episode number:  282\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.75120809+0.65409465j  0.07686519+0.0440265j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -180.0\n",
      "episode number:  282\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -179.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 283 reward -179.00, Last 30ep Avg. rewards -179.00.\n",
      "episode number:  283\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.17780375-0.80431843j  0.49278885+0.28038694j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -179.0\n",
      "episode number:  283\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -179.0\n",
      "episode number:  283\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -179.0\n",
      "episode number:  283\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -179.0\n",
      "episode number:  283\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -180.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 284 reward -180.00, Last 30ep Avg. rewards -180.00.\n",
      "episode number:  284\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.04535636-0.17551244j  0.96616393+0.18348148j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.04535636-0.17551244j  0.96616393+0.18348148j]\n",
      "RESULT: False\n",
      "reward till now:  -180.0\n",
      "episode number:  284\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.04535636-0.17551244j  0.96616393+0.18348148j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.04535636-0.17551244j  0.96616393+0.18348148j]\n",
      "RESULT: False\n",
      "reward till now:  -180.0\n",
      "episode number:  284\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.04535636-0.17551244j  0.96616393+0.18348148j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.04535636-0.17551244j  0.96616393+0.18348148j]\n",
      "RESULT: False\n",
      "reward till now:  -180.0\n",
      "episode number:  284\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.04535636-0.17551244j  0.96616393+0.18348148j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.04535636-0.17551244j  0.96616393+0.18348148j]\n",
      "RESULT: False\n",
      "reward till now:  -180.0\n",
      "episode number:  284\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.04535636-0.17551244j  0.96616393+0.18348148j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.04535636-0.17551244j  0.96616393+0.18348148j]\n",
      "RESULT: False\n",
      "reward till now:  -181.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 285 reward -181.00, Last 30ep Avg. rewards -181.00.\n",
      "episode number:  285\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.77504363-0.60597705j -0.13711507+0.11531971j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -181.0\n",
      "episode number:  285\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -180.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 286 reward -180.00, Last 30ep Avg. rewards -180.00.\n",
      "episode number:  286\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.37473  -0.3298257j  -0.4678359+0.72932984j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -180.0\n",
      "episode number:  286\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -180.0\n",
      "episode number:  286\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -180.0\n",
      "episode number:  286\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -180.0\n",
      "episode number:  286\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -181.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 287 reward -181.00, Last 30ep Avg. rewards -181.00.\n",
      "episode number:  287\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.02334295-0.14415404j -0.25851082-0.95490674j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -181.0\n",
      "episode number:  287\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -181.0\n",
      "episode number:  287\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -181.0\n",
      "episode number:  287\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -181.0\n",
      "episode number:  287\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -182.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 288 reward -182.00, Last 30ep Avg. rewards -182.00.\n",
      "episode number:  288\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.10817196-0.64401942j -0.75047111-0.1016412j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -182.0\n",
      "episode number:  288\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -182.0\n",
      "episode number:  288\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -182.0\n",
      "episode number:  288\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -182.0\n",
      "episode number:  288\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -183.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 289 reward -183.00, Last 30ep Avg. rewards -183.00.\n",
      "episode number:  289\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.38804878-0.75179921j  0.18989765-0.49815156j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.49815156-0.18989765j  0.75179921-0.38804878j]\n",
      "RESULT: False\n",
      "reward till now:  -183.0\n",
      "episode number:  289\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.49815156-0.18989765j  0.75179921-0.38804878j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.49815156-0.18989765j  0.75179921-0.38804878j]\n",
      "RESULT: False\n",
      "reward till now:  -183.0\n",
      "episode number:  289\n",
      "time step:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL STATE:  [[[-0.49815156-0.18989765j  0.75179921-0.38804878j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.49815156-0.18989765j  0.75179921-0.38804878j]\n",
      "RESULT: False\n",
      "reward till now:  -183.0\n",
      "episode number:  289\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.49815156-0.18989765j  0.75179921-0.38804878j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.49815156-0.18989765j  0.75179921-0.38804878j]\n",
      "RESULT: False\n",
      "reward till now:  -183.0\n",
      "episode number:  289\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.49815156-0.18989765j  0.75179921-0.38804878j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.49815156-0.18989765j  0.75179921-0.38804878j]\n",
      "RESULT: False\n",
      "reward till now:  -184.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 290 reward -184.00, Last 30ep Avg. rewards -184.00.\n",
      "episode number:  290\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.22214832+0.36828849j  0.88181285-0.193442j  ]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.22214832+0.36828849j  0.88181285-0.193442j  ]\n",
      "RESULT: False\n",
      "reward till now:  -184.0\n",
      "episode number:  290\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.22214832+0.36828849j  0.88181285-0.193442j  ]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.22214832+0.36828849j  0.88181285-0.193442j  ]\n",
      "RESULT: False\n",
      "reward till now:  -184.0\n",
      "episode number:  290\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[-0.22214832+0.36828849j  0.88181285-0.193442j  ]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.22214832+0.36828849j  0.88181285-0.193442j  ]\n",
      "RESULT: False\n",
      "reward till now:  -184.0\n",
      "episode number:  290\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.22214832+0.36828849j  0.88181285-0.193442j  ]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.22214832+0.36828849j  0.88181285-0.193442j  ]\n",
      "RESULT: False\n",
      "reward till now:  -184.0\n",
      "episode number:  290\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[-0.22214832+0.36828849j  0.88181285-0.193442j  ]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.22214832+0.36828849j  0.88181285-0.193442j  ]\n",
      "RESULT: False\n",
      "reward till now:  -185.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 291 reward -185.00, Last 30ep Avg. rewards -185.00.\n",
      "episode number:  291\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.14101788+0.14636668j 0.54145379+0.8157932j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -185.0\n",
      "episode number:  291\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -185.0\n",
      "episode number:  291\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -184.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 292 reward -184.00, Last 30ep Avg. rewards -184.00.\n",
      "episode number:  292\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.03463251-0.97111216j -0.12995143+0.19711515j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -184.0\n",
      "episode number:  292\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -183.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 293 reward -183.00, Last 30ep Avg. rewards -183.00.\n",
      "episode number:  293\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.48706865-0.51402918j 0.2961089 -0.64098178j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -183.0\n",
      "episode number:  293\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -183.0\n",
      "episode number:  293\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -182.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 294 reward -182.00, Last 30ep Avg. rewards -182.00.\n",
      "episode number:  294\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.2028378+0.09791883j 0.4049122-0.88617991j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -182.0\n",
      "episode number:  294\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -182.0\n",
      "episode number:  294\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -181.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 295 reward -181.00, Last 30ep Avg. rewards -181.00.\n",
      "episode number:  295\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.21522421-0.23857834j 0.64070802-0.69731782j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -181.0\n",
      "episode number:  295\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -181.0\n",
      "episode number:  295\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -180.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 296 reward -180.00, Last 30ep Avg. rewards -180.00.\n",
      "episode number:  296\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.55710579+0.41475628j 0.67003604-0.26203448j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -180.0\n",
      "episode number:  296\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -180.0\n",
      "episode number:  296\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -179.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 297 reward -179.00, Last 30ep Avg. rewards -179.00.\n",
      "episode number:  297\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.08994761+0.11773705j 0.20510547+0.96746016j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -179.0\n",
      "episode number:  297\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -179.0\n",
      "episode number:  297\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -178.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 298 reward -178.00, Last 30ep Avg. rewards -178.00.\n",
      "episode number:  298\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.41705265-0.88277046j -0.15522374-0.15062863j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -178.0\n",
      "episode number:  298\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -177.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 299 reward -177.00, Last 30ep Avg. rewards -177.00.\n",
      "episode number:  299\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.46222088+0.06973426j 0.70891788-0.52813296j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -177.0\n",
      "episode number:  299\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -177.0\n",
      "episode number:  299\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -176.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 300 reward -176.00, Last 30ep Avg. rewards -176.00.\n",
      "episode number:  300\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.39479168+0.02019832j -0.68591133+0.61094779j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [ 0.61094779+0.68591133j -0.02019832-0.39479168j]\n",
      "RESULT: False\n",
      "reward till now:  -176.0\n",
      "episode number:  300\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.61094779+0.68591133j -0.02019832-0.39479168j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.41772295+0.20585268j 0.4462877 +0.76417243j]\n",
      "RESULT: False\n",
      "reward till now:  -176.0\n",
      "episode number:  300\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.41772295+0.20585268j 0.4462877 +0.76417243j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -176.0\n",
      "episode number:  300\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -176.0\n",
      "episode number:  300\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -177.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 301 reward -177.00, Last 30ep Avg. rewards -177.00.\n",
      "episode number:  301\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.26008914-0.15696295j -0.56939775+0.76387333j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -177.0\n",
      "episode number:  301\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -177.0\n",
      "episode number:  301\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -176.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 302 reward -176.00, Last 30ep Avg. rewards -176.00.\n",
      "episode number:  302\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.34356516+0.42085016j  0.66987386-0.50608016j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -176.0\n",
      "episode number:  302\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -175.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 303 reward -175.00, Last 30ep Avg. rewards -175.00.\n",
      "episode number:  303\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.83882989-0.30729492j  0.24247405+0.37833925j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -175.0\n",
      "episode number:  303\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -175.0\n",
      "episode number:  303\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -174.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 304 reward -174.00, Last 30ep Avg. rewards -174.00.\n",
      "episode number:  304\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.68482791-0.69094315j -0.1445931 +0.18083454j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -174.0\n",
      "episode number:  304\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -173.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 305 reward -173.00, Last 30ep Avg. rewards -173.00.\n",
      "episode number:  305\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.08652276+0.56862744j  0.47143622-0.66852415j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -173.0\n",
      "episode number:  305\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -173.0\n",
      "episode number:  305\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -172.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 306 reward -172.00, Last 30ep Avg. rewards -172.00.\n",
      "episode number:  306\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.7067693 +0.01707547j -0.62544717-0.33015362j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.05750343-0.22137968j 0.94201931+0.24552804j]\n",
      "RESULT: False\n",
      "reward till now:  -172.0\n",
      "episode number:  306\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.05750343-0.22137968j 0.94201931+0.24552804j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.7067693 +0.01707547j -0.62544717-0.33015362j]\n",
      "RESULT: False\n",
      "reward till now:  -172.0\n",
      "episode number:  306\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.7067693 +0.01707547j -0.62544717-0.33015362j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.05750343-0.22137968j 0.94201931+0.24552804j]\n",
      "RESULT: False\n",
      "reward till now:  -172.0\n",
      "episode number:  306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.05750343-0.22137968j 0.94201931+0.24552804j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.7067693 +0.01707547j -0.62544717-0.33015362j]\n",
      "RESULT: False\n",
      "reward till now:  -172.0\n",
      "episode number:  306\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.7067693 +0.01707547j -0.62544717-0.33015362j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.05750343-0.22137968j 0.94201931+0.24552804j]\n",
      "RESULT: False\n",
      "reward till now:  -173.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 307 reward -173.00, Last 30ep Avg. rewards -173.00.\n",
      "episode number:  307\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.40374507-0.87500867j -0.1477429 +0.22253489j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -173.0\n",
      "episode number:  307\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -172.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 308 reward -172.00, Last 30ep Avg. rewards -172.00.\n",
      "episode number:  308\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.80693099+0.50633677j  0.21050689+0.21948189j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -172.0\n",
      "episode number:  308\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -171.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 309 reward -171.00, Last 30ep Avg. rewards -171.00.\n",
      "episode number:  309\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.27197092-0.37752996j 0.05262948-0.88359102j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -171.0\n",
      "episode number:  309\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -171.0\n",
      "episode number:  309\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -170.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 310 reward -170.00, Last 30ep Avg. rewards -170.00.\n",
      "episode number:  310\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.4583767 -0.12981249j -0.19308688+0.85776278j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -170.0\n",
      "episode number:  310\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -169.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]]\n",
      "Episode 311 reward -169.00, Last 30ep Avg. rewards -169.00.\n",
      "episode number:  311\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.97013489-0.05565254j 0.15571086-0.17746894j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.79609316-0.16484178j 0.57588475+0.0861372j ]\n",
      "RESULT: False\n",
      "reward till now:  -169.0\n",
      "episode number:  311\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.79609316-0.16484178j 0.57588475+0.0861372j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -169.0\n",
      "episode number:  311\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -169.0\n",
      "episode number:  311\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -168.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 312 reward -168.00, Last 30ep Avg. rewards -168.00.\n",
      "episode number:  312\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.22277853-0.77345875j  0.48986654+0.33490605j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -168.0\n",
      "episode number:  312\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -168.0\n",
      "episode number:  312\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -167.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 313 reward -167.00, Last 30ep Avg. rewards -167.00.\n",
      "episode number:  313\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.08631004-0.62681274j  0.23911587+0.73653239j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -167.0\n",
      "episode number:  313\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -167.0\n",
      "episode number:  313\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -166.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 314 reward -166.00, Last 30ep Avg. rewards -166.00.\n",
      "episode number:  314\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.97943504-0.09107875j -0.13824489-0.11532572j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.59481125-0.14595j   0.79031906+0.0171452j]\n",
      "RESULT: False\n",
      "reward till now:  -166.0\n",
      "episode number:  314\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.59481125-0.14595j   0.79031906+0.0171452j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.97943504-0.09107875j -0.13824489-0.11532572j]\n",
      "RESULT: False\n",
      "reward till now:  -166.0\n",
      "episode number:  314\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.97943504-0.09107875j -0.13824489-0.11532572j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.59481125-0.14595j   0.79031906+0.0171452j]\n",
      "RESULT: False\n",
      "reward till now:  -166.0\n",
      "episode number:  314\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.59481125-0.14595j   0.79031906+0.0171452j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.97943504-0.09107875j -0.13824489-0.11532572j]\n",
      "RESULT: False\n",
      "reward till now:  -166.0\n",
      "episode number:  314\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.97943504-0.09107875j -0.13824489-0.11532572j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.59481125-0.14595j   0.79031906+0.0171452j]\n",
      "RESULT: False\n",
      "reward till now:  -167.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 315 reward -167.00, Last 30ep Avg. rewards -167.00.\n",
      "episode number:  315\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.11430749-0.56280942j 0.60994113-0.54603221j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -167.0\n",
      "episode number:  315\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -166.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 316 reward -166.00, Last 30ep Avg. rewards -166.00.\n",
      "episode number:  316\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.85303988+0.28550083j 0.10565361-0.42385085j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.67789867-0.09782823j 0.5284819 +0.50158739j]\n",
      "RESULT: False\n",
      "reward till now:  -166.0\n",
      "episode number:  316\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.67789867-0.09782823j 0.5284819 +0.50158739j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -166.0\n",
      "episode number:  316\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -165.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 317 reward -165.00, Last 30ep Avg. rewards -165.00.\n",
      "episode number:  317\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.6420166 +0.72568857j -0.12479605-0.21358073j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -165.0\n",
      "episode number:  317\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -164.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 318 reward -164.00, Last 30ep Avg. rewards -164.00.\n",
      "episode number:  318\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.52539699-0.43015601j -0.72246865+0.13024154j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.13935071-0.21207155j  0.88237426-0.39626091j]\n",
      "RESULT: False\n",
      "reward till now:  -164.0\n",
      "episode number:  318\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.13935071-0.21207155j  0.88237426-0.39626091j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [ 0.88237426-0.39626091j -0.13935071-0.21207155j]\n",
      "RESULT: False\n",
      "reward till now:  -164.0\n",
      "episode number:  318\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.88237426-0.39626091j -0.13935071-0.21207155j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.52539699-0.43015601j 0.72246865-0.13024154j]\n",
      "RESULT: False\n",
      "reward till now:  -164.0\n",
      "episode number:  318\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.52539699-0.43015601j 0.72246865-0.13024154j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.88237426-0.39626091j -0.13935071-0.21207155j]\n",
      "RESULT: False\n",
      "reward till now:  -164.0\n",
      "episode number:  318\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.88237426-0.39626091j -0.13935071-0.21207155j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.52539699-0.43015601j 0.72246865-0.13024154j]\n",
      "RESULT: False\n",
      "reward till now:  -165.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 319 reward -165.00, Last 30ep Avg. rewards -165.00.\n",
      "episode number:  319\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.10445594-0.91599941j 0.36123806-0.13978947j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -165.0\n",
      "episode number:  319\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -164.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 320 reward -164.00, Last 30ep Avg. rewards -164.00.\n",
      "episode number:  320\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.75113668-0.31673535j 0.11599088-0.56746677j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.61315177-0.62522531j 0.4491159 +0.17729389j]\n",
      "RESULT: False\n",
      "reward till now:  -164.0\n",
      "episode number:  320\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.61315177-0.62522531j 0.4491159 +0.17729389j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -164.0\n",
      "episode number:  320\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -163.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 321 reward -163.00, Last 30ep Avg. rewards -163.00.\n",
      "episode number:  321\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.01799783+0.73882873j 0.58634219-0.33167908j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -163.0\n",
      "episode number:  321\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -163.0\n",
      "episode number:  321\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -162.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 322 reward -162.00, Last 30ep Avg. rewards -162.00.\n",
      "episode number:  322\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.45758778-0.50158117j 0.50646345-0.53153037j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -162.0\n",
      "episode number:  322\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -161.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 323 reward -161.00, Last 30ep Avg. rewards -161.00.\n",
      "episode number:  323\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.26258574+0.4185242j  0.17453051-0.85171904j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -161.0\n",
      "episode number:  323\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -161.0\n",
      "episode number:  323\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -160.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 324 reward -160.00, Last 30ep Avg. rewards -160.00.\n",
      "episode number:  324\n",
      "time step:  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL STATE:  [[[-0.91638615+0.04871154j  0.39092556+0.07099875j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -160.0\n",
      "episode number:  324\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -159.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 325 reward -159.00, Last 30ep Avg. rewards -159.00.\n",
      "episode number:  325\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.06922149+0.53738959j -0.81183752+0.2175791j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -159.0\n",
      "episode number:  325\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -159.0\n",
      "episode number:  325\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -158.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 326 reward -158.00, Last 30ep Avg. rewards -158.00.\n",
      "episode number:  326\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.8428553 +0.51466466j 0.08677004+0.13109614j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.65734438+0.45662184j 0.53463301+0.2712239j ]\n",
      "RESULT: False\n",
      "reward till now:  -158.0\n",
      "episode number:  326\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.65734438+0.45662184j 0.53463301+0.2712239j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -158.0\n",
      "episode number:  326\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -158.0\n",
      "episode number:  326\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -157.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 327 reward -157.00, Last 30ep Avg. rewards -157.00.\n",
      "episode number:  327\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.65835391-0.09721353j  0.46611418+0.58297275j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -157.0\n",
      "episode number:  327\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -157.0\n",
      "episode number:  327\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -156.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 328 reward -156.00, Last 30ep Avg. rewards -156.00.\n",
      "episode number:  328\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.93868964+0.09375221j  0.10322801-0.31530345j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -156.0\n",
      "episode number:  328\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -155.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 329 reward -155.00, Last 30ep Avg. rewards -155.00.\n",
      "episode number:  329\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.20865199+0.21054892j  0.5833188 +0.75622263j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -155.0\n",
      "episode number:  329\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -155.0\n",
      "episode number:  329\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -154.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 330 reward -154.00, Last 30ep Avg. rewards -154.00.\n",
      "episode number:  330\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.34196992-0.38315627j 0.09703854+0.85254406j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -154.0\n",
      "episode number:  330\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -153.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]]\n",
      "Episode 331 reward -153.00, Last 30ep Avg. rewards -153.00.\n",
      "episode number:  331\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.37050709+0.54018536j -0.39318535-0.64523604j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -153.0\n",
      "episode number:  331\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -153.0\n",
      "episode number:  331\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -153.0\n",
      "episode number:  331\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -153.0\n",
      "episode number:  331\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -154.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 332 reward -154.00, Last 30ep Avg. rewards -154.00.\n",
      "episode number:  332\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.19536557-0.8846607j  -0.41964028-0.05576526j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -154.0\n",
      "episode number:  332\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -153.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 333 reward -153.00, Last 30ep Avg. rewards -153.00.\n",
      "episode number:  333\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.07416416-0.09565445j  0.98211954-0.14419125j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [ 0.98211954-0.14419125j -0.07416416-0.09565445j]\n",
      "RESULT: False\n",
      "reward till now:  -153.0\n",
      "episode number:  333\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.98211954-0.14419125j -0.07416416-0.09565445j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.6420214 -0.16959652j 0.74690537-0.0343207j ]\n",
      "RESULT: False\n",
      "reward till now:  -153.0\n",
      "episode number:  333\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.6420214 -0.16959652j 0.74690537-0.0343207j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.98211954-0.14419125j -0.07416416-0.09565445j]\n",
      "RESULT: False\n",
      "reward till now:  -153.0\n",
      "episode number:  333\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.98211954-0.14419125j -0.07416416-0.09565445j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.6420214 -0.16959652j 0.74690537-0.0343207j ]\n",
      "RESULT: False\n",
      "reward till now:  -153.0\n",
      "episode number:  333\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.6420214 -0.16959652j 0.74690537-0.0343207j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.98211954-0.14419125j -0.07416416-0.09565445j]\n",
      "RESULT: False\n",
      "reward till now:  -154.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 334 reward -154.00, Last 30ep Avg. rewards -154.00.\n",
      "episode number:  334\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.22764758-0.36481753j -0.1852444 -0.88361149j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -154.0\n",
      "episode number:  334\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -154.0\n",
      "episode number:  334\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.5+0.5j 0.5+0.5j]\n",
      "RESULT: False\n",
      "reward till now:  -154.0\n",
      "episode number:  334\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.5+0.5j 0.5+0.5j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -154.0\n",
      "episode number:  334\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -153.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 335 reward -153.00, Last 30ep Avg. rewards -153.00.\n",
      "episode number:  335\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.57968915-0.06268434j -0.27775512-0.76346791j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -153.0\n",
      "episode number:  335\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -152.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 336 reward -152.00, Last 30ep Avg. rewards -152.00.\n",
      "episode number:  336\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.09564841-0.35650958j -0.42957999+0.824144j  ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -152.0\n",
      "episode number:  336\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -152.0\n",
      "episode number:  336\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -152.0\n",
      "episode number:  336\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -152.0\n",
      "episode number:  336\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -153.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 337 reward -153.00, Last 30ep Avg. rewards -153.00.\n",
      "episode number:  337\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.0166118 -0.9910068j  -0.12314712+0.04964233j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -153.0\n",
      "episode number:  337\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -152.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 338 reward -152.00, Last 30ep Avg. rewards -152.00.\n",
      "episode number:  338\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.3417707 -0.18826729j 0.81930126+0.42011149j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.82100186+0.16393861j -0.3376651 -0.43018876j]\n",
      "RESULT: False\n",
      "reward till now:  -152.0\n",
      "episode number:  338\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.82100186+0.16393861j -0.3376651 -0.43018876j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.3417707 -0.18826729j 0.81930126+0.42011149j]\n",
      "RESULT: False\n",
      "reward till now:  -152.0\n",
      "episode number:  338\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.3417707 -0.18826729j 0.81930126+0.42011149j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.82100186+0.16393861j -0.3376651 -0.43018876j]\n",
      "RESULT: False\n",
      "reward till now:  -152.0\n",
      "episode number:  338\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.82100186+0.16393861j -0.3376651 -0.43018876j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.3417707 -0.18826729j 0.81930126+0.42011149j]\n",
      "RESULT: False\n",
      "reward till now:  -152.0\n",
      "episode number:  338\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.3417707 -0.18826729j 0.81930126+0.42011149j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.82100186+0.16393861j -0.3376651 -0.43018876j]\n",
      "RESULT: False\n",
      "reward till now:  -153.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 339 reward -153.00, Last 30ep Avg. rewards -153.00.\n",
      "episode number:  339\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.48819814+0.73387723j -0.32843072+0.33944079j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -153.0\n",
      "episode number:  339\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -153.0\n",
      "episode number:  339\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.70710678+0.j  0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -153.0\n",
      "episode number:  339\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[-0.70710678+0.j  0.70710678+0.j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.-0.70710678j 0.-0.70710678j]\n",
      "RESULT: False\n",
      "reward till now:  -153.0\n",
      "episode number:  339\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.-0.70710678j 0.-0.70710678j]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -154.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 340 reward -154.00, Last 30ep Avg. rewards -154.00.\n",
      "episode number:  340\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.09681161-0.50863619j  0.58546254-0.62381917j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -154.0\n",
      "episode number:  340\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -153.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]]\n",
      "Episode 341 reward -153.00, Last 30ep Avg. rewards -153.00.\n",
      "episode number:  341\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.51896164-0.61329163j -0.01849859-0.59515544j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -153.0\n",
      "episode number:  341\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -152.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]\n",
      " [0.95]\n",
      " [1.  ]]\n",
      "Episode 342 reward -152.00, Last 30ep Avg. rewards -152.00.\n",
      "episode number:  342\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.16096167+0.87153864j -0.25428221-0.3871076j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -152.0\n",
      "episode number:  342\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -152.0\n",
      "episode number:  342\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -152.0\n",
      "episode number:  342\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -152.0\n",
      "episode number:  342\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -153.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 343 reward -153.00, Last 30ep Avg. rewards -153.00.\n",
      "episode number:  343\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.48962311-0.81637262j -0.04126365-0.30348356j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -153.0\n",
      "episode number:  343\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -152.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 344 reward -152.00, Last 30ep Avg. rewards -152.00.\n",
      "episode number:  344\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.39987126+0.66059926j 0.14984865-0.6174601j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -152.0\n",
      "episode number:  344\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -152.0\n",
      "episode number:  344\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [0.70710678+0.j         0.        +0.70710678j]\n",
      "RESULT: False\n",
      "reward till now:  -152.0\n",
      "episode number:  344\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.70710678+0.j         0.        +0.70710678j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.5+0.5j 0.5-0.5j]\n",
      "RESULT: False\n",
      "reward till now:  -152.0\n",
      "episode number:  344\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.5+0.5j 0.5-0.5j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -153.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 345 reward -153.00, Last 30ep Avg. rewards -153.00.\n",
      "episode number:  345\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.63459734+0.70013558j 0.24882514-0.21256163j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -153.0\n",
      "episode number:  345\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -152.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 346 reward -152.00, Last 30ep Avg. rewards -152.00.\n",
      "episode number:  346\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.20427168+0.35485905j -0.8300452 -0.37864641j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.12330156+0.83785383j  0.33600734+0.41218534j]\n",
      "RESULT: False\n",
      "reward till now:  -152.0\n",
      "episode number:  346\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.12330156+0.83785383j  0.33600734+0.41218534j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -152.0\n",
      "episode number:  346\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -151.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 347 reward -151.00, Last 30ep Avg. rewards -151.00.\n",
      "episode number:  347\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.85618246-0.42843945j 0.02433785+0.28774798j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.62262188-0.09948389j 0.58820297-0.50642099j]\n",
      "RESULT: False\n",
      "reward till now:  -151.0\n",
      "episode number:  347\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.62262188-0.09948389j 0.58820297-0.50642099j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -151.0\n",
      "episode number:  347\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -151.0\n",
      "episode number:  347\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -151.0\n",
      "episode number:  347\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -152.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 348 reward -152.00, Last 30ep Avg. rewards -152.00.\n",
      "episode number:  348\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.32375208-0.91601901j  0.03230051-0.23462832j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -152.0\n",
      "episode number:  348\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -151.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 349 reward -151.00, Last 30ep Avg. rewards -151.00.\n",
      "episode number:  349\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.59452357-0.11771074j 0.63546998+0.47839712j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -151.0\n",
      "episode number:  349\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -151.0\n",
      "episode number:  349\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -151.0\n",
      "episode number:  349\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -151.0\n",
      "episode number:  349\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -152.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 350 reward -152.00, Last 30ep Avg. rewards -152.00.\n",
      "episode number:  350\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.15751616-0.56577047j 0.61240375+0.52920136j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -152.0\n",
      "episode number:  350\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -152.0\n",
      "episode number:  350\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -151.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 351 reward -151.00, Last 30ep Avg. rewards -151.00.\n",
      "episode number:  351\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.74640165+0.63731797j 0.1906215 -0.01933428j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.1906215 -0.01933428j 0.74640165+0.63731797j]\n",
      "RESULT: False\n",
      "reward till now:  -151.0\n",
      "episode number:  351\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.1906215 -0.01933428j 0.74640165+0.63731797j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -151.0\n",
      "episode number:  351\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -151.0\n",
      "episode number:  351\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -150.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 352 reward -150.00, Last 30ep Avg. rewards -150.00.\n",
      "episode number:  352\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.1052541-0.55285866j -0.6048689-0.56338484j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -150.0\n",
      "episode number:  352\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.-1.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -150.0\n",
      "episode number:  352\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.-1.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -150.0\n",
      "episode number:  352\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -149.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 353 reward -149.00, Last 30ep Avg. rewards -149.00.\n",
      "episode number:  353\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.41126031-0.10131858j -0.90449849-0.04981946j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -149.0\n",
      "episode number:  353\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -149.0\n",
      "episode number:  353\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -148.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 354 reward -148.00, Last 30ep Avg. rewards -148.00.\n",
      "episode number:  354\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.32885273-0.60006934j 0.64294812-0.34407905j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -148.0\n",
      "episode number:  354\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -148.0\n",
      "episode number:  354\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -147.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 355 reward -147.00, Last 30ep Avg. rewards -147.00.\n",
      "episode number:  355\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.05717376-0.8298675j  -0.21471891-0.51180747j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -147.0\n",
      "episode number:  355\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -146.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 356 reward -146.00, Last 30ep Avg. rewards -146.00.\n",
      "episode number:  356\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.40170876-0.41755383j 0.34670605-0.73761358j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -146.0\n",
      "episode number:  356\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward till now:  -145.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 357 reward -145.00, Last 30ep Avg. rewards -145.00.\n",
      "episode number:  357\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.14411242+0.76626004j -0.57229672-0.25407405j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -145.0\n",
      "episode number:  357\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -144.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 358 reward -144.00, Last 30ep Avg. rewards -144.00.\n",
      "episode number:  358\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.93692454+0.13486241j 0.17765667-0.26911455j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.93692454+0.13486241j 0.17765667-0.26911455j]\n",
      "RESULT: False\n",
      "reward till now:  -144.0\n",
      "episode number:  358\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.93692454+0.13486241j 0.17765667-0.26911455j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.17765667-0.26911455j 0.93692454+0.13486241j]\n",
      "RESULT: False\n",
      "reward till now:  -144.0\n",
      "episode number:  358\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.17765667-0.26911455j 0.93692454+0.13486241j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.93692454+0.13486241j 0.17765667-0.26911455j]\n",
      "RESULT: False\n",
      "reward till now:  -144.0\n",
      "episode number:  358\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.93692454+0.13486241j 0.17765667-0.26911455j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.17765667-0.26911455j 0.93692454+0.13486241j]\n",
      "RESULT: False\n",
      "reward till now:  -144.0\n",
      "episode number:  358\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.17765667-0.26911455j 0.93692454+0.13486241j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.93692454+0.13486241j 0.17765667-0.26911455j]\n",
      "RESULT: False\n",
      "reward till now:  -145.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 359 reward -145.00, Last 30ep Avg. rewards -145.00.\n",
      "episode number:  359\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.25943633+0.55954419j -0.15101308-0.77252712j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -145.0\n",
      "episode number:  359\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -145.0\n",
      "episode number:  359\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -144.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 360 reward -144.00, Last 30ep Avg. rewards -144.00.\n",
      "episode number:  360\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.04277895+0.04003326j -0.91092861-0.40838263j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -144.0\n",
      "episode number:  360\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -144.0\n",
      "episode number:  360\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -143.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 361 reward -143.00, Last 30ep Avg. rewards -143.00.\n",
      "episode number:  361\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.52935478-0.10245093j 0.46351654+0.70316409j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -143.0\n",
      "episode number:  361\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -143.0\n",
      "episode number:  361\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -142.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 362 reward -142.00, Last 30ep Avg. rewards -142.00.\n",
      "episode number:  362\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.07009088-0.88178611j -0.25849518-0.38822773j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -142.0\n",
      "episode number:  362\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -141.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 363 reward -141.00, Last 30ep Avg. rewards -141.00.\n",
      "episode number:  363\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.40692635-0.55165279j 0.12735633+0.71684762j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -141.0\n",
      "episode number:  363\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -140.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 364 reward -140.00, Last 30ep Avg. rewards -140.00.\n",
      "episode number:  364\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.01496373+0.92387564j -0.36834105-0.10273635j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -140.0\n",
      "episode number:  364\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -139.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 365 reward -139.00, Last 30ep Avg. rewards -139.00.\n",
      "episode number:  365\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 1.77159246e-04-0.81964596j -5.57170699e-01+0.13319641j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -139.0\n",
      "episode number:  365\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -139.0\n",
      "episode number:  365\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -138.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 366 reward -138.00, Last 30ep Avg. rewards -138.00.\n",
      "episode number:  366\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.78129262+0.56644132j  0.21188907-0.15436674j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -138.0\n",
      "episode number:  366\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -137.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 367 reward -137.00, Last 30ep Avg. rewards -137.00.\n",
      "episode number:  367\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.35000797-0.78182397j -0.49557322+0.14371111j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -137.0\n",
      "episode number:  367\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -136.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 368 reward -136.00, Last 30ep Avg. rewards -136.00.\n",
      "episode number:  368\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.47943184-0.65941132j  0.15177431+0.55882588j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -136.0\n",
      "episode number:  368\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -135.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 369 reward -135.00, Last 30ep Avg. rewards -135.00.\n",
      "episode number:  369\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.42031931-0.25757571j 0.5612722 -0.66480068j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -135.0\n",
      "episode number:  369\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -135.0\n",
      "episode number:  369\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -134.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 370 reward -134.00, Last 30ep Avg. rewards -134.00.\n",
      "episode number:  370\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.44311709+0.60249378j -0.63853036-0.18145929j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -134.0\n",
      "episode number:  370\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -134.0\n",
      "episode number:  370\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -133.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 371 reward -133.00, Last 30ep Avg. rewards -133.00.\n",
      "episode number:  371\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.21285406-0.51968286j -0.14658566-0.81433133j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -133.0\n",
      "episode number:  371\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -133.0\n",
      "episode number:  371\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -132.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 372 reward -132.00, Last 30ep Avg. rewards -132.00.\n",
      "episode number:  372\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.36339596+0.57445405j -0.22045812+0.69953137j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -132.0\n",
      "episode number:  372\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -132.0\n",
      "episode number:  372\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -131.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 373 reward -131.00, Last 30ep Avg. rewards -131.00.\n",
      "episode number:  373\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.69148171+0.63672371j -0.30289617-0.15713012j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -131.0\n",
      "episode number:  373\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -130.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 374 reward -130.00, Last 30ep Avg. rewards -130.00.\n",
      "episode number:  374\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.51712746+0.23862051j -0.72555422+0.38627778j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [ 0.38627778+0.72555422j -0.23862051+0.51712746j]\n",
      "RESULT: False\n",
      "reward till now:  -130.0\n",
      "episode number:  374\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.38627778+0.72555422j -0.23862051+0.51712746j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -130.0\n",
      "episode number:  374\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -129.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 375 reward -129.00, Last 30ep Avg. rewards -129.00.\n",
      "episode number:  375\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.58225623+0.28149835j -0.04164941-0.76157842j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -129.0\n",
      "episode number:  375\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -128.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 376 reward -128.00, Last 30ep Avg. rewards -128.00.\n",
      "episode number:  376\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.19792483-0.08196824j 0.9629317 -0.16391921j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.9629317 -0.16391921j 0.19792483-0.08196824j]\n",
      "RESULT: False\n",
      "reward till now:  -128.0\n",
      "episode number:  376\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.9629317 -0.16391921j 0.19792483-0.08196824j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.82084952-0.17386868j 0.54094155-0.05794809j]\n",
      "RESULT: False\n",
      "reward till now:  -128.0\n",
      "episode number:  376\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.82084952-0.17386868j 0.54094155-0.05794809j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -128.0\n",
      "episode number:  376\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -127.0\n",
      "in discounted reward\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disc_rw:  [[0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 377 reward -127.00, Last 30ep Avg. rewards -127.00.\n",
      "episode number:  377\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.0674702 +0.97177074j  0.18972165+0.12294344j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -127.0\n",
      "episode number:  377\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -126.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 378 reward -126.00, Last 30ep Avg. rewards -126.00.\n",
      "episode number:  378\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.22863955-0.75413842j 0.42628828+0.44415932j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -126.0\n",
      "episode number:  378\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -126.0\n",
      "episode number:  378\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -125.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 379 reward -125.00, Last 30ep Avg. rewards -125.00.\n",
      "episode number:  379\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.21822415+0.75698372j 0.33530092+0.51664995j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -125.0\n",
      "episode number:  379\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -124.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 380 reward -124.00, Last 30ep Avg. rewards -124.00.\n",
      "episode number:  380\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.45655548-0.50906494j -0.24720564-0.68651246j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.24720564-0.68651246j -0.45655548-0.50906494j]\n",
      "RESULT: False\n",
      "reward till now:  -124.0\n",
      "episode number:  380\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.24720564-0.68651246j -0.45655548-0.50906494j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -124.0\n",
      "episode number:  380\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -123.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 381 reward -123.00, Last 30ep Avg. rewards -123.00.\n",
      "episode number:  381\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.00102565-0.3350532j  0.14479606+0.93100612j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -123.0\n",
      "episode number:  381\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -123.0\n",
      "episode number:  381\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -123.0\n",
      "episode number:  381\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -123.0\n",
      "episode number:  381\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -124.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 382 reward -124.00, Last 30ep Avg. rewards -124.00.\n",
      "episode number:  382\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.13989066+0.87368605j -0.44368564+0.14228961j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -124.0\n",
      "episode number:  382\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -123.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 383 reward -123.00, Last 30ep Avg. rewards -123.00.\n",
      "episode number:  383\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.57600894+0.02564203j  0.26308744+0.77352516j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -123.0\n",
      "episode number:  383\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -122.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 384 reward -122.00, Last 30ep Avg. rewards -122.00.\n",
      "episode number:  384\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.8012379 +0.21832643j -0.02468652+0.55654467j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.54910475+0.54791661j 0.58401676-0.23915641j]\n",
      "RESULT: False\n",
      "reward till now:  -122.0\n",
      "episode number:  384\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.54910475+0.54791661j 0.58401676-0.23915641j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -122.0\n",
      "episode number:  384\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -121.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 385 reward -121.00, Last 30ep Avg. rewards -121.00.\n",
      "episode number:  385\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.17202337-0.70953616j  0.67302079+0.11836134j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -121.0\n",
      "episode number:  385\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -120.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 386 reward -120.00, Last 30ep Avg. rewards -120.00.\n",
      "episode number:  386\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.28452257-0.07753402j -0.80905358+0.50839717j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -120.0\n",
      "episode number:  386\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -120.0\n",
      "episode number:  386\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -120.0\n",
      "episode number:  386\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -120.0\n",
      "episode number:  386\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -121.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 387 reward -121.00, Last 30ep Avg. rewards -121.00.\n",
      "episode number:  387\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.32538766+0.17692071j -0.68042439+0.63233265j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -121.0\n",
      "episode number:  387\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -121.0\n",
      "episode number:  387\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -121.0\n",
      "episode number:  387\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -121.0\n",
      "episode number:  387\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -122.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 388 reward -122.00, Last 30ep Avg. rewards -122.00.\n",
      "episode number:  388\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.5997647 -0.64734336j -0.41427065+0.22273012j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -122.0\n",
      "episode number:  388\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -121.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 389 reward -121.00, Last 30ep Avg. rewards -121.00.\n",
      "episode number:  389\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.15568392+0.73024199j -0.46729776+0.47343632j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -121.0\n",
      "episode number:  389\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -120.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 390 reward -120.00, Last 30ep Avg. rewards -120.00.\n",
      "episode number:  390\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.73753658-0.52790793j  0.35767435+0.22231075j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -120.0\n",
      "episode number:  390\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -119.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]]\n",
      "Episode 391 reward -119.00, Last 30ep Avg. rewards -119.00.\n",
      "episode number:  391\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.54472389+0.09230246j  0.8104115 -0.19490856j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -119.0\n",
      "episode number:  391\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -118.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]\n",
      " [0.95]\n",
      " [1.  ]]\n",
      "Episode 392 reward -118.00, Last 30ep Avg. rewards -118.00.\n",
      "episode number:  392\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.3725379 -0.51557964j -0.16304791-0.75419396j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -118.0\n",
      "episode number:  392\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -117.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]\n",
      " [0.95]\n",
      " [1.  ]\n",
      " [0.95]\n",
      " [1.  ]]\n",
      "Episode 393 reward -117.00, Last 30ep Avg. rewards -117.00.\n",
      "episode number:  393\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.54498139-0.17327396j -0.6833673 +0.45385081j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [-0.09785362+0.19839779j  0.86857369-0.44344417j]\n",
      "RESULT: False\n",
      "reward till now:  -117.0\n",
      "episode number:  393\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.09785362+0.19839779j  0.86857369-0.44344417j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -117.0\n",
      "episode number:  393\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -116.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 394 reward -116.00, Last 30ep Avg. rewards -116.00.\n",
      "episode number:  394\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.47763679+0.08459022j -0.64127888-0.59453258j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -116.0\n",
      "episode number:  394\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -116.0\n",
      "episode number:  394\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -116.0\n",
      "episode number:  394\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -116.0\n",
      "episode number:  394\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -117.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 395 reward -117.00, Last 30ep Avg. rewards -117.00.\n",
      "episode number:  395\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.58452449-0.31225662j  0.51755644+0.54125988j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -117.0\n",
      "episode number:  395\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -116.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 396 reward -116.00, Last 30ep Avg. rewards -116.00.\n",
      "episode number:  396\n",
      "time step:  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL STATE:  [[[0.48679502+0.10047924j 0.18666934+0.84740137j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -116.0\n",
      "episode number:  396\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -115.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 397 reward -115.00, Last 30ep Avg. rewards -115.00.\n",
      "episode number:  397\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.13654728-0.19907853j 0.49538086-0.83445814j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -115.0\n",
      "episode number:  397\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -115.0\n",
      "episode number:  397\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -115.0\n",
      "episode number:  397\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -115.0\n",
      "episode number:  397\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -116.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 398 reward -116.00, Last 30ep Avg. rewards -116.00.\n",
      "episode number:  398\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.4854332+0.78534696j -0.3391678+0.18041608j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -116.0\n",
      "episode number:  398\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -115.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 399 reward -115.00, Last 30ep Avg. rewards -115.00.\n",
      "episode number:  399\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.07729656-0.13839677j  0.95927017+0.23382111j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -115.0\n",
      "episode number:  399\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -115.0\n",
      "episode number:  399\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -115.0\n",
      "episode number:  399\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -115.0\n",
      "episode number:  399\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -116.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 400 reward -116.00, Last 30ep Avg. rewards -116.00.\n",
      "episode number:  400\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.92635884+0.35982017j  0.06593686-0.08967209j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -116.0\n",
      "episode number:  400\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -115.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]]\n",
      "Episode 401 reward -115.00, Last 30ep Avg. rewards -115.00.\n",
      "episode number:  401\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.10952834+0.53133364j -0.23260633-0.80720654j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -115.0\n",
      "episode number:  401\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -114.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]\n",
      " [0.95]\n",
      " [1.  ]]\n",
      "Episode 402 reward -114.00, Last 30ep Avg. rewards -114.00.\n",
      "episode number:  402\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.93399238+0.07921198j 0.06381288-0.34250784j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70555486-0.18617829j 0.61530982+0.29820094j]\n",
      "RESULT: False\n",
      "reward till now:  -114.0\n",
      "episode number:  402\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.70555486-0.18617829j 0.61530982+0.29820094j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -114.0\n",
      "episode number:  402\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -114.0\n",
      "episode number:  402\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -114.0\n",
      "episode number:  402\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -115.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 403 reward -115.00, Last 30ep Avg. rewards -115.00.\n",
      "episode number:  403\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.64731012-0.35918025j -0.38975327-0.54778787j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.1821202 -0.64132331j 0.73331455+0.13336573j]\n",
      "RESULT: False\n",
      "reward till now:  -115.0\n",
      "episode number:  403\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.1821202 -0.64132331j 0.73331455+0.13336573j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -115.0\n",
      "episode number:  403\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -115.0\n",
      "episode number:  403\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -115.0\n",
      "episode number:  403\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -116.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 404 reward -116.00, Last 30ep Avg. rewards -116.00.\n",
      "episode number:  404\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.6120748 -0.35965069j  0.66091912-0.24331408j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -116.0\n",
      "episode number:  404\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -116.0\n",
      "episode number:  404\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -116.0\n",
      "episode number:  404\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -116.0\n",
      "episode number:  404\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -117.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 405 reward -117.00, Last 30ep Avg. rewards -117.00.\n",
      "episode number:  405\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.41252009+0.65449668j 0.46054732-0.43515221j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -117.0\n",
      "episode number:  405\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -116.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 406 reward -116.00, Last 30ep Avg. rewards -116.00.\n",
      "episode number:  406\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.39311247+0.27501869j 0.7054478 -0.52169983j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -116.0\n",
      "episode number:  406\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -116.0\n",
      "episode number:  406\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -116.0\n",
      "episode number:  406\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -116.0\n",
      "episode number:  406\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -117.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 407 reward -117.00, Last 30ep Avg. rewards -117.00.\n",
      "episode number:  407\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.39965295+0.63421797j -0.55852124-0.35510436j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -117.0\n",
      "episode number:  407\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -116.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 408 reward -116.00, Last 30ep Avg. rewards -116.00.\n",
      "episode number:  408\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.16438516-0.36600169j -0.51972029-0.75426196j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -116.0\n",
      "episode number:  408\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -116.0\n",
      "episode number:  408\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -116.0\n",
      "episode number:  408\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -116.0\n",
      "episode number:  408\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -117.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 409 reward -117.00, Last 30ep Avg. rewards -117.00.\n",
      "episode number:  409\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.91019409-0.02128548j -0.41076115-0.0486716j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.35315242-0.04946712j 0.93405641+0.01936491j]\n",
      "RESULT: False\n",
      "reward till now:  -117.0\n",
      "episode number:  409\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.35315242-0.04946712j 0.93405641+0.01936491j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.91019409-0.02128548j -0.41076115-0.0486716j ]\n",
      "RESULT: False\n",
      "reward till now:  -117.0\n",
      "episode number:  409\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.91019409-0.02128548j -0.41076115-0.0486716j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.35315242-0.04946712j 0.93405641+0.01936491j]\n",
      "RESULT: False\n",
      "reward till now:  -117.0\n",
      "episode number:  409\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.35315242-0.04946712j 0.93405641+0.01936491j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.91019409-0.02128548j -0.41076115-0.0486716j ]\n",
      "RESULT: False\n",
      "reward till now:  -117.0\n",
      "episode number:  409\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.91019409-0.02128548j -0.41076115-0.0486716j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.35315242-0.04946712j 0.93405641+0.01936491j]\n",
      "RESULT: False\n",
      "reward till now:  -118.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 410 reward -118.00, Last 30ep Avg. rewards -118.00.\n",
      "episode number:  410\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.15322472-0.11553099j -0.5525477 -0.81108928j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -118.0\n",
      "episode number:  410\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -118.0\n",
      "episode number:  410\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -118.0\n",
      "episode number:  410\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward till now:  -118.0\n",
      "episode number:  410\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.+0.70710678j 0.+0.70710678j]\n",
      "RESULT: False\n",
      "reward till now:  -119.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 411 reward -119.00, Last 30ep Avg. rewards -119.00.\n",
      "episode number:  411\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.07593488+0.16965703j -0.44564474+0.87570038j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -119.0\n",
      "episode number:  411\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -119.0\n",
      "episode number:  411\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -119.0\n",
      "episode number:  411\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -119.0\n",
      "episode number:  411\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -120.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 412 reward -120.00, Last 30ep Avg. rewards -120.00.\n",
      "episode number:  412\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.33478533+0.07767745j 0.9191767 +0.19235175j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.88668505+0.19093947j -0.4132271 -0.08108698j]\n",
      "RESULT: False\n",
      "reward till now:  -120.0\n",
      "episode number:  412\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.88668505+0.19093947j -0.4132271 -0.08108698j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.33478533+0.07767745j 0.9191767 +0.19235175j]\n",
      "RESULT: False\n",
      "reward till now:  -120.0\n",
      "episode number:  412\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.33478533+0.07767745j 0.9191767 +0.19235175j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.88668505+0.19093947j -0.4132271 -0.08108698j]\n",
      "RESULT: False\n",
      "reward till now:  -120.0\n",
      "episode number:  412\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.88668505+0.19093947j -0.4132271 -0.08108698j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.33478533+0.07767745j 0.9191767 +0.19235175j]\n",
      "RESULT: False\n",
      "reward till now:  -120.0\n",
      "episode number:  412\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.33478533+0.07767745j 0.9191767 +0.19235175j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.88668505+0.19093947j -0.4132271 -0.08108698j]\n",
      "RESULT: False\n",
      "reward till now:  -121.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 413 reward -121.00, Last 30ep Avg. rewards -121.00.\n",
      "episode number:  413\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.22836293-0.28939305j  0.6081867 +0.70300141j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -121.0\n",
      "episode number:  413\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -121.0\n",
      "episode number:  413\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.+0.70710678j 0.+0.70710678j]\n",
      "RESULT: False\n",
      "reward till now:  -121.0\n",
      "episode number:  413\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.70710678j 0.+0.70710678j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -121.0\n",
      "episode number:  413\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -120.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 414 reward -120.00, Last 30ep Avg. rewards -120.00.\n",
      "episode number:  414\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.7869779 -0.22382426j 0.57305097-0.04670193j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -120.0\n",
      "episode number:  414\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -120.0\n",
      "episode number:  414\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -120.0\n",
      "episode number:  414\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -120.0\n",
      "episode number:  414\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -121.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 415 reward -121.00, Last 30ep Avg. rewards -121.00.\n",
      "episode number:  415\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.92823335-0.25525454j -0.26287697+0.06421573j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -121.0\n",
      "episode number:  415\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -120.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 416 reward -120.00, Last 30ep Avg. rewards -120.00.\n",
      "episode number:  416\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.72063638-0.40407656j 0.38692137+0.40950848j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -120.0\n",
      "episode number:  416\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -120.0\n",
      "episode number:  416\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -120.0\n",
      "episode number:  416\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -120.0\n",
      "episode number:  416\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -121.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 417 reward -121.00, Last 30ep Avg. rewards -121.00.\n",
      "episode number:  417\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.13055797+0.0037057j  -0.95535978+0.26500676j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -121.0\n",
      "episode number:  417\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -121.0\n",
      "episode number:  417\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -121.0\n",
      "episode number:  417\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -121.0\n",
      "episode number:  417\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -122.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 418 reward -122.00, Last 30ep Avg. rewards -122.00.\n",
      "episode number:  418\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.53002306-0.67520339j -0.40628232-0.31322615j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -122.0\n",
      "episode number:  418\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -122.0\n",
      "episode number:  418\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -122.0\n",
      "episode number:  418\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -122.0\n",
      "episode number:  418\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -123.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 419 reward -123.00, Last 30ep Avg. rewards -123.00.\n",
      "episode number:  419\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.60177452-0.31485005j  0.30826792-0.66611392j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -123.0\n",
      "episode number:  419\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -122.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 420 reward -122.00, Last 30ep Avg. rewards -122.00.\n",
      "episode number:  420\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.9418875 -0.27930943j 0.18458925-0.02758603j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.79653935-0.21700786j 0.53549073-0.17799532j]\n",
      "RESULT: False\n",
      "reward till now:  -122.0\n",
      "episode number:  420\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.79653935-0.21700786j 0.53549073-0.17799532j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -122.0\n",
      "episode number:  420\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -122.0\n",
      "episode number:  420\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -122.0\n",
      "episode number:  420\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -123.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 421 reward -123.00, Last 30ep Avg. rewards -123.00.\n",
      "episode number:  421\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.37756269+0.40454282j -0.76812227+0.32214856j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -123.0\n",
      "episode number:  421\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -123.0\n",
      "episode number:  421\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -123.0\n",
      "episode number:  421\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -123.0\n",
      "episode number:  421\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -124.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 422 reward -124.00, Last 30ep Avg. rewards -124.00.\n",
      "episode number:  422\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.23720848-0.65865923j  0.46940655+0.53810561j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -124.0\n",
      "episode number:  422\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -123.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 423 reward -123.00, Last 30ep Avg. rewards -123.00.\n",
      "episode number:  423\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.50767857-0.61163399j -0.59721482-0.1072417j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -123.0\n",
      "episode number:  423\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -122.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 424 reward -122.00, Last 30ep Avg. rewards -122.00.\n",
      "episode number:  424\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.28453672+0.48151958j 0.30404786-0.77118911j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -122.0\n",
      "episode number:  424\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -122.0\n",
      "episode number:  424\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -122.0\n",
      "episode number:  424\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -122.0\n",
      "episode number:  424\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -123.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 425 reward -123.00, Last 30ep Avg. rewards -123.00.\n",
      "episode number:  425\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.89937554-0.13018387j -0.41393451+0.05323557j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -123.0\n",
      "episode number:  425\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -122.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 426 reward -122.00, Last 30ep Avg. rewards -122.00.\n",
      "episode number:  426\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.02883632+0.73534769j  0.43457318-0.51920939j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -122.0\n",
      "episode number:  426\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -121.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 427 reward -121.00, Last 30ep Avg. rewards -121.00.\n",
      "episode number:  427\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.22083862+0.04007261j -0.88335778-0.4114651j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -121.0\n",
      "episode number:  427\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -121.0\n",
      "episode number:  427\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -121.0\n",
      "episode number:  427\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -121.0\n",
      "episode number:  427\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -122.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 428 reward -122.00, Last 30ep Avg. rewards -122.00.\n",
      "episode number:  428\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.93835715-0.27199025j -0.21245848+0.01919777j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.51328787-0.17875128j 0.81374954-0.20590103j]\n",
      "RESULT: False\n",
      "reward till now:  -122.0\n",
      "episode number:  428\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.51328787-0.17875128j 0.81374954-0.20590103j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -122.0\n",
      "episode number:  428\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -122.0\n",
      "episode number:  428\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -122.0\n",
      "episode number:  428\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -123.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 429 reward -123.00, Last 30ep Avg. rewards -123.00.\n",
      "episode number:  429\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.91601211-0.04620908j 0.38862704+0.08806561j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.92251919+0.02959704j 0.37291756-0.09494654j]\n",
      "RESULT: False\n",
      "reward till now:  -123.0\n",
      "episode number:  429\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.92251919+0.02959704j 0.37291756-0.09494654j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.91601211-0.04620908j 0.38862704+0.08806561j]\n",
      "RESULT: False\n",
      "reward till now:  -123.0\n",
      "episode number:  429\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.91601211-0.04620908j 0.38862704+0.08806561j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.92251919+0.02959704j 0.37291756-0.09494654j]\n",
      "RESULT: False\n",
      "reward till now:  -123.0\n",
      "episode number:  429\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.92251919+0.02959704j 0.37291756-0.09494654j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.91601211-0.04620908j 0.38862704+0.08806561j]\n",
      "RESULT: False\n",
      "reward till now:  -123.0\n",
      "episode number:  429\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.91601211-0.04620908j 0.38862704+0.08806561j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.92251919+0.02959704j 0.37291756-0.09494654j]\n",
      "RESULT: False\n",
      "reward till now:  -124.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 430 reward -124.00, Last 30ep Avg. rewards -124.00.\n",
      "episode number:  430\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.51392092+0.427182j   0.16640775-0.72505813j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -124.0\n",
      "episode number:  430\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -123.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]]\n",
      "Episode 431 reward -123.00, Last 30ep Avg. rewards -123.00.\n",
      "episode number:  431\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.98618684-0.15143888j -0.05479511+0.03872043j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.65859341-0.07970398j 0.7360854 -0.13446294j]\n",
      "RESULT: False\n",
      "reward till now:  -123.0\n",
      "episode number:  431\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.65859341-0.07970398j 0.7360854 -0.13446294j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -123.0\n",
      "episode number:  431\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -122.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 432 reward -122.00, Last 30ep Avg. rewards -122.00.\n",
      "episode number:  432\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.17499927+0.13850851j  0.86567111-0.4481118j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -122.0\n",
      "episode number:  432\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -122.0\n",
      "episode number:  432\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -122.0\n",
      "episode number:  432\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -122.0\n",
      "episode number:  432\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -123.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 433 reward -123.00, Last 30ep Avg. rewards -123.00.\n",
      "episode number:  433\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.79213865-0.04981825j -0.57427255-0.20061291j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.57427255-0.20061291j  0.79213865-0.04981825j]\n",
      "RESULT: False\n",
      "reward till now:  -123.0\n",
      "episode number:  433\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.57427255-0.20061291j  0.79213865-0.04981825j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -123.0\n",
      "episode number:  433\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -122.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 434 reward -122.00, Last 30ep Avg. rewards -122.00.\n",
      "episode number:  434\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.80963302+0.08634983j 0.3299351 +0.47768286j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.3299351 +0.47768286j 0.80963302+0.08634983j]\n",
      "RESULT: False\n",
      "reward till now:  -122.0\n",
      "episode number:  434\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.3299351 +0.47768286j 0.80963302+0.08634983j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -122.0\n",
      "episode number:  434\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -122.0\n",
      "episode number:  434\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -122.0\n",
      "episode number:  434\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -123.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 435 reward -123.00, Last 30ep Avg. rewards -123.00.\n",
      "episode number:  435\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.35871847-0.62000268j -0.10219695+0.69027061j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -123.0\n",
      "episode number:  435\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -122.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 436 reward -122.00, Last 30ep Avg. rewards -122.00.\n",
      "episode number:  436\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.00667752-0.18046471j 0.77066007-0.6111227j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -122.0\n",
      "episode number:  436\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -122.0\n",
      "episode number:  436\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -122.0\n",
      "episode number:  436\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -122.0\n",
      "episode number:  436\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -123.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 437 reward -123.00, Last 30ep Avg. rewards -123.00.\n",
      "episode number:  437\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.66947254-0.08306721j -0.30218195-0.67349271j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -123.0\n",
      "episode number:  437\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -122.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 438 reward -122.00, Last 30ep Avg. rewards -122.00.\n",
      "episode number:  438\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.38086077+0.41027406j -0.82352035+0.09183954j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -122.0\n",
      "episode number:  438\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -121.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 439 reward -121.00, Last 30ep Avg. rewards -121.00.\n",
      "episode number:  439\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.45680115+0.83413451j 0.04191489-0.30626047j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -121.0\n",
      "episode number:  439\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -120.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 440 reward -120.00, Last 30ep Avg. rewards -120.00.\n",
      "episode number:  440\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.03181205+0.17132936j -0.98191569-0.07399884j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -120.0\n",
      "episode number:  440\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -120.0\n",
      "episode number:  440\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -120.0\n",
      "episode number:  440\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -119.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 441 reward -119.00, Last 30ep Avg. rewards -119.00.\n",
      "episode number:  441\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.66061852+0.55851781j -0.03808162-0.50019077j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.03808162-0.50019077j  0.66061852+0.55851781j]\n",
      "RESULT: False\n",
      "reward till now:  -119.0\n",
      "episode number:  441\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.03808162-0.50019077j  0.66061852+0.55851781j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -119.0\n",
      "episode number:  441\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -118.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 442 reward -118.00, Last 30ep Avg. rewards -118.00.\n",
      "episode number:  442\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.52288834+0.70096113j -0.48348621-0.03850145j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -118.0\n",
      "episode number:  442\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -117.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 443 reward -117.00, Last 30ep Avg. rewards -117.00.\n",
      "episode number:  443\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.31204375+0.43713566j 0.38881961-0.74857226j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -117.0\n",
      "episode number:  443\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.-1.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -117.0\n",
      "episode number:  443\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.-1.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -117.0\n",
      "episode number:  443\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -116.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 444 reward -116.00, Last 30ep Avg. rewards -116.00.\n",
      "episode number:  444\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.48459119-0.37885165j 0.16317881+0.77137247j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -116.0\n",
      "episode number:  444\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -115.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 445 reward -115.00, Last 30ep Avg. rewards -115.00.\n",
      "episode number:  445\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.04343307+0.55645132j  0.27093519-0.78426375j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -115.0\n",
      "episode number:  445\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -115.0\n",
      "episode number:  445\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -114.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 446 reward -114.00, Last 30ep Avg. rewards -114.00.\n",
      "episode number:  446\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.66932949-0.18188067j -0.69089826-0.20390451j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -114.0\n",
      "episode number:  446\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -114.0\n",
      "episode number:  446\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -113.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 447 reward -113.00, Last 30ep Avg. rewards -113.00.\n",
      "episode number:  447\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.30865933-0.60614132j 0.0097567 -0.73295765j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -113.0\n",
      "episode number:  447\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -112.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 448 reward -112.00, Last 30ep Avg. rewards -112.00.\n",
      "episode number:  448\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.54475762+0.55336696j -0.61608513-0.1321486j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -112.0\n",
      "episode number:  448\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -111.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 449 reward -111.00, Last 30ep Avg. rewards -111.00.\n",
      "episode number:  449\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.50466057+0.5244929j  -0.20242799-0.65517007j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -111.0\n",
      "episode number:  449\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -110.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 450 reward -110.00, Last 30ep Avg. rewards -110.00.\n",
      "episode number:  450\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.6367727 -0.24695007j -0.62474866-0.3784512j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -110.0\n",
      "episode number:  450\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -109.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]]\n",
      "Episode 451 reward -109.00, Last 30ep Avg. rewards -109.00.\n",
      "episode number:  451\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.53441673-0.00314393j 0.81849626+0.21083823j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.81849626+0.21083823j 0.53441673-0.00314393j]\n",
      "RESULT: False\n",
      "reward till now:  -109.0\n",
      "episode number:  451\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.81849626+0.21083823j 0.53441673-0.00314393j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.53441673-0.00314393j 0.81849626+0.21083823j]\n",
      "RESULT: False\n",
      "reward till now:  -109.0\n",
      "episode number:  451\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.53441673-0.00314393j 0.81849626+0.21083823j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.81849626+0.21083823j 0.53441673-0.00314393j]\n",
      "RESULT: False\n",
      "reward till now:  -109.0\n",
      "episode number:  451\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.81849626+0.21083823j 0.53441673-0.00314393j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.53441673-0.00314393j 0.81849626+0.21083823j]\n",
      "RESULT: False\n",
      "reward till now:  -109.0\n",
      "episode number:  451\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.53441673-0.00314393j 0.81849626+0.21083823j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.81849626+0.21083823j 0.53441673-0.00314393j]\n",
      "RESULT: False\n",
      "reward till now:  -110.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 452 reward -110.00, Last 30ep Avg. rewards -110.00.\n",
      "episode number:  452\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.58629174-0.01685271j -0.70760549+0.39404625j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -110.0\n",
      "episode number:  452\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -109.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 453 reward -109.00, Last 30ep Avg. rewards -109.00.\n",
      "episode number:  453\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.18399409-0.90308425j  0.30718681-0.23711024j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -109.0\n",
      "episode number:  453\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -108.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 454 reward -108.00, Last 30ep Avg. rewards -108.00.\n",
      "episode number:  454\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.11594156-0.1282101j 0.61085999+0.7726382j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -108.0\n",
      "episode number:  454\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -108.0\n",
      "episode number:  454\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -107.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 455 reward -107.00, Last 30ep Avg. rewards -107.00.\n",
      "episode number:  455\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.23928203+0.35536469j -0.68640023-0.58763489j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -107.0\n",
      "episode number:  455\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -107.0\n",
      "episode number:  455\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -106.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 456 reward -106.00, Last 30ep Avg. rewards -106.00.\n",
      "episode number:  456\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.38606226-0.90078224j -0.01540256+0.19826763j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -106.0\n",
      "episode number:  456\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -105.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 457 reward -105.00, Last 30ep Avg. rewards -105.00.\n",
      "episode number:  457\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.41815327+0.65310926j 0.32658429+0.54031365j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -105.0\n",
      "episode number:  457\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -104.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 458 reward -104.00, Last 30ep Avg. rewards -104.00.\n",
      "episode number:  458\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.71285577-0.23806831j  0.15087868+0.64218047j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -104.0\n",
      "episode number:  458\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -104.0\n",
      "episode number:  458\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -103.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 459 reward -103.00, Last 30ep Avg. rewards -103.00.\n",
      "episode number:  459\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.01619253-0.79589632j 0.58282776-0.16309093j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -103.0\n",
      "episode number:  459\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -102.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 460 reward -102.00, Last 30ep Avg. rewards -102.00.\n",
      "episode number:  460\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.45157086+0.31717076j -0.03490043+0.83322772j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -102.0\n",
      "episode number:  460\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -101.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]]\n",
      "Episode 461 reward -101.00, Last 30ep Avg. rewards -101.00.\n",
      "episode number:  461\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.59931354-0.12439993j  0.47891372-0.62927704j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -101.0\n",
      "episode number:  461\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -101.0\n",
      "episode number:  461\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -100.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 462 reward -100.00, Last 30ep Avg. rewards -100.00.\n",
      "episode number:  462\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.04743502-0.22905464j -0.94359211-0.23434552j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -100.0\n",
      "episode number:  462\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -100.0\n",
      "episode number:  462\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -99.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 463 reward -99.00, Last 30ep Avg. rewards -99.00.\n",
      "episode number:  463\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.27664765-0.80444243j  0.21472999+0.47982236j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -99.0\n",
      "episode number:  463\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -98.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 464 reward -98.00, Last 30ep Avg. rewards -98.00.\n",
      "episode number:  464\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.71218028-0.17020089j  0.34026141+0.58996023j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -98.0\n",
      "episode number:  464\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -98.0\n",
      "episode number:  464\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -97.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 465 reward -97.00, Last 30ep Avg. rewards -97.00.\n",
      "episode number:  465\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.33402817-0.43785423j -0.60250073+0.57766922j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -97.0\n",
      "episode number:  465\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -97.0\n",
      "episode number:  465\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -96.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 466 reward -96.00, Last 30ep Avg. rewards -96.00.\n",
      "episode number:  466\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.08770964-0.68514004j -0.15617136+0.70604579j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -96.0\n",
      "episode number:  466\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -95.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 467 reward -95.00, Last 30ep Avg. rewards -95.00.\n",
      "episode number:  467\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.7702636 -0.59713725j -0.14405326+0.17137601j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -95.0\n",
      "episode number:  467\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -94.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 468 reward -94.00, Last 30ep Avg. rewards -94.00.\n",
      "episode number:  468\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.07978971+0.86673961j 0.41641192-0.26267311j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -94.0\n",
      "episode number:  468\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -93.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 469 reward -93.00, Last 30ep Avg. rewards -93.00.\n",
      "episode number:  469\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.01640763-0.01422413j  0.10997549+0.99369706j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -93.0\n",
      "episode number:  469\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -93.0\n",
      "episode number:  469\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -92.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 470 reward -92.00, Last 30ep Avg. rewards -92.00.\n",
      "episode number:  470\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.4823353 +0.56155055j -0.61257956-0.27705581j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -92.0\n",
      "episode number:  470\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -92.0\n",
      "episode number:  470\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -92.0\n",
      "episode number:  470\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -92.0\n",
      "episode number:  470\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -93.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 471 reward -93.00, Last 30ep Avg. rewards -93.00.\n",
      "episode number:  471\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.43911828+0.32546804j -0.59470291-0.58955418j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -93.0\n",
      "episode number:  471\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -93.0\n",
      "episode number:  471\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -93.0\n",
      "episode number:  471\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -93.0\n",
      "episode number:  471\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -94.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 472 reward -94.00, Last 30ep Avg. rewards -94.00.\n",
      "episode number:  472\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.1141487 +0.46653073j 0.22679557+0.84727972j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -94.0\n",
      "episode number:  472\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -94.0\n",
      "episode number:  472\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -94.0\n",
      "episode number:  472\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -94.0\n",
      "episode number:  472\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -95.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 473 reward -95.00, Last 30ep Avg. rewards -95.00.\n",
      "episode number:  473\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.55106277+0.50474845j -0.60508508-0.27464682j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -95.0\n",
      "episode number:  473\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -95.0\n",
      "episode number:  473\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -95.0\n",
      "episode number:  473\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -95.0\n",
      "episode number:  473\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -96.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 474 reward -96.00, Last 30ep Avg. rewards -96.00.\n",
      "episode number:  474\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.13558172-0.53188616j 0.37062736-0.74923299j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -96.0\n",
      "episode number:  474\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -96.0\n",
      "episode number:  474\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -96.0\n",
      "episode number:  474\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -96.0\n",
      "episode number:  474\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -97.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 475 reward -97.00, Last 30ep Avg. rewards -97.00.\n",
      "episode number:  475\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.24891537+0.21237786j 0.85681185-0.39851001j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.78186722-0.1316153j  -0.42984772+0.43196296j]\n",
      "RESULT: False\n",
      "reward till now:  -97.0\n",
      "episode number:  475\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.78186722-0.1316153j  -0.42984772+0.43196296j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.24891537+0.21237786j 0.85681185-0.39851001j]\n",
      "RESULT: False\n",
      "reward till now:  -97.0\n",
      "episode number:  475\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.24891537+0.21237786j 0.85681185-0.39851001j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.78186722-0.1316153j  -0.42984772+0.43196296j]\n",
      "RESULT: False\n",
      "reward till now:  -97.0\n",
      "episode number:  475\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.78186722-0.1316153j  -0.42984772+0.43196296j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.24891537+0.21237786j 0.85681185-0.39851001j]\n",
      "RESULT: False\n",
      "reward till now:  -97.0\n",
      "episode number:  475\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.24891537+0.21237786j 0.85681185-0.39851001j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.78186722-0.1316153j  -0.42984772+0.43196296j]\n",
      "RESULT: False\n",
      "reward till now:  -98.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 476 reward -98.00, Last 30ep Avg. rewards -98.00.\n",
      "episode number:  476\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.68518432-0.20896293j -0.33240921-0.61348273j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -98.0\n",
      "episode number:  476\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -98.0\n",
      "episode number:  476\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -98.0\n",
      "episode number:  476\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -98.0\n",
      "episode number:  476\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -99.0\n",
      "in discounted reward\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 477 reward -99.00, Last 30ep Avg. rewards -99.00.\n",
      "episode number:  477\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.66204043-0.30494911j 0.56023529+0.3935034j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -99.0\n",
      "episode number:  477\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -98.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 478 reward -98.00, Last 30ep Avg. rewards -98.00.\n",
      "episode number:  478\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.28088012-0.49895767j  0.49374684+0.65449344j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -98.0\n",
      "episode number:  478\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -98.0\n",
      "episode number:  478\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -98.0\n",
      "episode number:  478\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -98.0\n",
      "episode number:  478\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -99.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 479 reward -99.00, Last 30ep Avg. rewards -99.00.\n",
      "episode number:  479\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.52846203+0.24596705j -0.35160703-0.73253026j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -99.0\n",
      "episode number:  479\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -99.0\n",
      "episode number:  479\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -99.0\n",
      "episode number:  479\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -99.0\n",
      "episode number:  479\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -100.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 480 reward -100.00, Last 30ep Avg. rewards -100.00.\n",
      "episode number:  480\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.07866498+0.34777475j  0.79766494+0.48641051j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -100.0\n",
      "episode number:  480\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -100.0\n",
      "episode number:  480\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -100.0\n",
      "episode number:  480\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -99.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 481 reward -99.00, Last 30ep Avg. rewards -99.00.\n",
      "episode number:  481\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.34230379+0.42910425j  0.17079978+0.81824513j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -99.0\n",
      "episode number:  481\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -99.0\n",
      "episode number:  481\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -99.0\n",
      "episode number:  481\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -99.0\n",
      "episode number:  481\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -100.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 482 reward -100.00, Last 30ep Avg. rewards -100.00.\n",
      "episode number:  482\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.21898035+0.35047389j  0.04142617-0.90967001j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -100.0\n",
      "episode number:  482\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -100.0\n",
      "episode number:  482\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -100.0\n",
      "episode number:  482\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -99.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 483 reward -99.00, Last 30ep Avg. rewards -99.00.\n",
      "episode number:  483\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.20036165+0.94182682j -0.11542689+0.24391409j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -99.0\n",
      "episode number:  483\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -98.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 484 reward -98.00, Last 30ep Avg. rewards -98.00.\n",
      "episode number:  484\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.57166327+0.54799458j  0.17931728-0.58373655j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -98.0\n",
      "episode number:  484\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -97.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 485 reward -97.00, Last 30ep Avg. rewards -97.00.\n",
      "episode number:  485\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.63669591+0.63234875j 0.19357849-0.39658637j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.58709266+0.16670918j 0.31333134+0.727567j  ]\n",
      "RESULT: False\n",
      "reward till now:  -97.0\n",
      "episode number:  485\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.58709266+0.16670918j 0.31333134+0.727567j  ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -97.0\n",
      "episode number:  485\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -97.0\n",
      "episode number:  485\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -97.0\n",
      "episode number:  485\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -96.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 486 reward -96.00, Last 30ep Avg. rewards -96.00.\n",
      "episode number:  486\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.54978127-0.6392215j  -0.50719677+0.17857173j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -96.0\n",
      "episode number:  486\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -95.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 487 reward -95.00, Last 30ep Avg. rewards -95.00.\n",
      "episode number:  487\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.29151499+0.79110781j -0.23461044+0.48386504j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -95.0\n",
      "episode number:  487\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -94.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 488 reward -94.00, Last 30ep Avg. rewards -94.00.\n",
      "episode number:  488\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.35445069+0.60585791j -0.31784816+0.63739584j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -94.0\n",
      "episode number:  488\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -93.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 489 reward -93.00, Last 30ep Avg. rewards -93.00.\n",
      "episode number:  489\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.06917828+0.4381313j  0.71119031-0.54540231j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -93.0\n",
      "episode number:  489\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -93.0\n",
      "episode number:  489\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -93.0\n",
      "episode number:  489\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -92.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 490 reward -92.00, Last 30ep Avg. rewards -92.00.\n",
      "episode number:  490\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.07572108+0.46872491j -0.87620384-0.08264447j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -92.0\n",
      "episode number:  490\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -92.0\n",
      "episode number:  490\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -92.0\n",
      "episode number:  490\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -91.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 491 reward -91.00, Last 30ep Avg. rewards -91.00.\n",
      "episode number:  491\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.24734526+0.02679359j 0.83108751-0.49738917j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -91.0\n",
      "episode number:  491\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -91.0\n",
      "episode number:  491\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -91.0\n",
      "episode number:  491\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -91.0\n",
      "episode number:  491\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -92.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 492 reward -92.00, Last 30ep Avg. rewards -92.00.\n",
      "episode number:  492\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.0555302-0.49601843j  0.6403601+0.58379882j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [-0.0555302-0.49601843j  0.6403601+0.58379882j]\n",
      "RESULT: False\n",
      "reward till now:  -92.0\n",
      "episode number:  492\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.0555302-0.49601843j  0.6403601+0.58379882j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -92.0\n",
      "episode number:  492\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -92.0\n",
      "episode number:  492\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -92.0\n",
      "episode number:  492\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -91.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 493 reward -91.00, Last 30ep Avg. rewards -91.00.\n",
      "episode number:  493\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.11670791+0.64512553j -0.20394932-0.72704676j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -91.0\n",
      "episode number:  493\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -91.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode number:  493\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -91.0\n",
      "episode number:  493\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -90.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 494 reward -90.00, Last 30ep Avg. rewards -90.00.\n",
      "episode number:  494\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.06918341+0.90976884j 0.06697422-0.40379298j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -90.0\n",
      "episode number:  494\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -89.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 495 reward -89.00, Last 30ep Avg. rewards -89.00.\n",
      "episode number:  495\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.2544212 -0.57238325j  0.73307935-0.26503196j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -89.0\n",
      "episode number:  495\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -88.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 496 reward -88.00, Last 30ep Avg. rewards -88.00.\n",
      "episode number:  496\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.64884749-0.07118893j -0.41407969-0.63440293j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -88.0\n",
      "episode number:  496\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -88.0\n",
      "episode number:  496\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -88.0\n",
      "episode number:  496\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -88.0\n",
      "episode number:  496\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -89.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 497 reward -89.00, Last 30ep Avg. rewards -89.00.\n",
      "episode number:  497\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.43017171+0.4734941j  0.49461131+0.58831563j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -89.0\n",
      "episode number:  497\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -88.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 498 reward -88.00, Last 30ep Avg. rewards -88.00.\n",
      "episode number:  498\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.36186576+0.24779423j -0.73715641-0.51405409j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -88.0\n",
      "episode number:  498\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -88.0\n",
      "episode number:  498\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -88.0\n",
      "episode number:  498\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -88.0\n",
      "episode number:  498\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -89.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 499 reward -89.00, Last 30ep Avg. rewards -89.00.\n",
      "episode number:  499\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.4448802 +0.0219289j  0.07622473+0.89207092j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -89.0\n",
      "episode number:  499\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -89.0\n",
      "episode number:  499\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -89.0\n",
      "episode number:  499\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -88.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 500 reward -88.00, Last 30ep Avg. rewards -88.00.\n",
      "episode number:  500\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.49530113-0.59682332j -0.3554526 +0.52166288j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -88.0\n",
      "episode number:  500\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -87.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]]\n",
      "Episode 501 reward -87.00, Last 30ep Avg. rewards -87.00.\n",
      "episode number:  501\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.38774082+0.74301376j 0.46804993-0.28020862j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -87.0\n",
      "episode number:  501\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -86.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]\n",
      " [0.95]\n",
      " [1.  ]]\n",
      "Episode 502 reward -86.00, Last 30ep Avg. rewards -86.00.\n",
      "episode number:  502\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.16730014+0.2059176j   0.96278939+0.05143153j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.56249584+0.18197331j -0.79909397+0.10923815j]\n",
      "RESULT: False\n",
      "reward till now:  -86.0\n",
      "episode number:  502\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.56249584+0.18197331j -0.79909397+0.10923815j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -86.0\n",
      "episode number:  502\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -86.0\n",
      "episode number:  502\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -86.0\n",
      "episode number:  502\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -87.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 503 reward -87.00, Last 30ep Avg. rewards -87.00.\n",
      "episode number:  503\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.11815316-0.072485j   -0.97185466-0.19048433j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -87.0\n",
      "episode number:  503\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -87.0\n",
      "episode number:  503\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -87.0\n",
      "episode number:  503\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -87.0\n",
      "episode number:  503\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -88.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 504 reward -88.00, Last 30ep Avg. rewards -88.00.\n",
      "episode number:  504\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.04689505-0.7562204j  0.49813077+0.42166018j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -88.0\n",
      "episode number:  504\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -88.0\n",
      "episode number:  504\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -88.0\n",
      "episode number:  504\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -87.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 505 reward -87.00, Last 30ep Avg. rewards -87.00.\n",
      "episode number:  505\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.51276505-0.31341242j -0.66586216+0.44212243j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -87.0\n",
      "episode number:  505\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -87.0\n",
      "episode number:  505\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -87.0\n",
      "episode number:  505\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -87.0\n",
      "episode number:  505\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -88.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 506 reward -88.00, Last 30ep Avg. rewards -88.00.\n",
      "episode number:  506\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.53232339-0.69433035j -0.43626651+0.21025869j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -88.0\n",
      "episode number:  506\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -87.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 507 reward -87.00, Last 30ep Avg. rewards -87.00.\n",
      "episode number:  507\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.08640226-0.07511554j -0.92247112-0.36869953j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -87.0\n",
      "episode number:  507\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -87.0\n",
      "episode number:  507\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -87.0\n",
      "episode number:  507\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -86.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 508 reward -86.00, Last 30ep Avg. rewards -86.00.\n",
      "episode number:  508\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.0615539-0.46983402j -0.6020833-0.64262182j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -86.0\n",
      "episode number:  508\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -86.0\n",
      "episode number:  508\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -86.0\n",
      "episode number:  508\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -85.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 509 reward -85.00, Last 30ep Avg. rewards -85.00.\n",
      "episode number:  509\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.27942781-0.60436071j 0.51632784-0.53858499j]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -85.0\n",
      "episode number:  509\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -85.0\n",
      "episode number:  509\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -85.0\n",
      "episode number:  509\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -84.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 510 reward -84.00, Last 30ep Avg. rewards -84.00.\n",
      "episode number:  510\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.60690706-0.21122178j 0.50303283+0.57793352j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -84.0\n",
      "episode number:  510\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -84.0\n",
      "episode number:  510\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -84.0\n",
      "episode number:  510\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -83.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 511 reward -83.00, Last 30ep Avg. rewards -83.00.\n",
      "episode number:  511\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.09324308-0.68106851j 0.33337038-0.64522524j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -83.0\n",
      "episode number:  511\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -82.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 512 reward -82.00, Last 30ep Avg. rewards -82.00.\n",
      "episode number:  512\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.43287698+0.23115097j -0.30836012-0.81492379j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -82.0\n",
      "episode number:  512\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -81.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 513 reward -81.00, Last 30ep Avg. rewards -81.00.\n",
      "episode number:  513\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.15848867-0.18998133j -0.54909408-0.79830077j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -81.0\n",
      "episode number:  513\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -81.0\n",
      "episode number:  513\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -81.0\n",
      "episode number:  513\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -80.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 514 reward -80.00, Last 30ep Avg. rewards -80.00.\n",
      "episode number:  514\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.58909668-0.13461107j -0.79576997+0.03993887j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -80.0\n",
      "episode number:  514\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -80.0\n",
      "episode number:  514\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -80.0\n",
      "episode number:  514\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -79.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 515 reward -79.00, Last 30ep Avg. rewards -79.00.\n",
      "episode number:  515\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.27476735-0.62006417j -0.37312485+0.63308859j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -79.0\n",
      "episode number:  515\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -78.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 516 reward -78.00, Last 30ep Avg. rewards -78.00.\n",
      "episode number:  516\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.35395606+0.49487436j -0.27164557-0.7456696j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -78.0\n",
      "episode number:  516\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -78.0\n",
      "episode number:  516\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -78.0\n",
      "episode number:  516\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -77.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 517 reward -77.00, Last 30ep Avg. rewards -77.00.\n",
      "episode number:  517\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.41297994+0.09750652j 0.85161984-0.30770717j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.89420708-0.1486343j  -0.31016524+0.28652935j]\n",
      "RESULT: False\n",
      "reward till now:  -77.0\n",
      "episode number:  517\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.89420708-0.1486343j  -0.31016524+0.28652935j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.41297994+0.09750652j 0.85161984-0.30770717j]\n",
      "RESULT: False\n",
      "reward till now:  -77.0\n",
      "episode number:  517\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.41297994+0.09750652j 0.85161984-0.30770717j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.89420708-0.1486343j  -0.31016524+0.28652935j]\n",
      "RESULT: False\n",
      "reward till now:  -77.0\n",
      "episode number:  517\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.89420708-0.1486343j  -0.31016524+0.28652935j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.41297994+0.09750652j 0.85161984-0.30770717j]\n",
      "RESULT: False\n",
      "reward till now:  -77.0\n",
      "episode number:  517\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.41297994+0.09750652j 0.85161984-0.30770717j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.89420708-0.1486343j  -0.31016524+0.28652935j]\n",
      "RESULT: False\n",
      "reward till now:  -78.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 518 reward -78.00, Last 30ep Avg. rewards -78.00.\n",
      "episode number:  518\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.13136118-0.17466106j -0.32628502-0.91966072j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -78.0\n",
      "episode number:  518\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -78.0\n",
      "episode number:  518\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -78.0\n",
      "episode number:  518\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -77.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 519 reward -77.00, Last 30ep Avg. rewards -77.00.\n",
      "episode number:  519\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.08574407+0.28966933j 0.94781655+0.10189903j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.73083773+0.27688065j -0.6095773 +0.13277365j]\n",
      "RESULT: False\n",
      "reward till now:  -77.0\n",
      "episode number:  519\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.73083773+0.27688065j -0.6095773 +0.13277365j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -77.0\n",
      "episode number:  519\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -77.0\n",
      "episode number:  519\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -77.0\n",
      "episode number:  519\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -78.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 520 reward -78.00, Last 30ep Avg. rewards -78.00.\n",
      "episode number:  520\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.52275469-0.54869635j -0.19923493+0.62126105j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.22876301+0.051311j   0.51052376-0.82728481j]\n",
      "RESULT: False\n",
      "reward till now:  -78.0\n",
      "episode number:  520\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.22876301+0.051311j   0.51052376-0.82728481j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -78.0\n",
      "episode number:  520\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -78.0\n",
      "episode number:  520\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -78.0\n",
      "episode number:  520\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -77.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.81450625]\n",
      " [0.857375  ]\n",
      " [0.9025    ]\n",
      " [0.95      ]\n",
      " [1.        ]]\n",
      "Episode 521 reward -77.00, Last 30ep Avg. rewards -77.00.\n",
      "episode number:  521\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.28457494+0.81514668j -0.42181222-0.27681662j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -77.0\n",
      "episode number:  521\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -76.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.81450625]\n",
      " [0.857375  ]\n",
      " [0.9025    ]\n",
      " [0.95      ]\n",
      " [1.        ]\n",
      " [0.95      ]\n",
      " [1.        ]]\n",
      "Episode 522 reward -76.00, Last 30ep Avg. rewards -76.00.\n",
      "episode number:  522\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.63129298-0.37995772j 0.41062274+0.5371129j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -76.0\n",
      "episode number:  522\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -76.0\n",
      "episode number:  522\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -76.0\n",
      "episode number:  522\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -75.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.81450625]\n",
      " [0.857375  ]\n",
      " [0.9025    ]\n",
      " [0.95      ]\n",
      " [1.        ]\n",
      " [0.95      ]\n",
      " [1.        ]\n",
      " [0.857375  ]\n",
      " [0.9025    ]\n",
      " [0.95      ]\n",
      " [1.        ]]\n",
      "Episode 523 reward -75.00, Last 30ep Avg. rewards -75.00.\n",
      "episode number:  523\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.85411546-0.03439999j -0.10036323+0.50914697j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  523\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -74.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.81450625]\n",
      " [0.857375  ]\n",
      " [0.9025    ]\n",
      " [0.95      ]\n",
      " [1.        ]\n",
      " [0.95      ]\n",
      " [1.        ]\n",
      " [0.857375  ]\n",
      " [0.9025    ]\n",
      " [0.95      ]\n",
      " [1.        ]\n",
      " [0.95      ]\n",
      " [1.        ]]\n",
      "Episode 524 reward -74.00, Last 30ep Avg. rewards -74.00.\n",
      "episode number:  524\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.47352309-0.4873868j  -0.21479185-0.70149444j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  524\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  524\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  524\n",
      "time step:  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  524\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 525 reward -75.00, Last 30ep Avg. rewards -75.00.\n",
      "episode number:  525\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.816893 -0.34201102j -0.2840213-0.36748631j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  525\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -74.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 526 reward -74.00, Last 30ep Avg. rewards -74.00.\n",
      "episode number:  526\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.18462204-0.43872671j 0.83231416+0.28405406j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  526\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  526\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  526\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  526\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 527 reward -75.00, Last 30ep Avg. rewards -75.00.\n",
      "episode number:  527\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.76723511+0.54647817j -0.20970523-0.26217477j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  527\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  527\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  527\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  527\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -76.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 528 reward -76.00, Last 30ep Avg. rewards -76.00.\n",
      "episode number:  528\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.40879537-0.50518735j  0.72615675-0.2244292j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -76.0\n",
      "episode number:  528\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -75.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 529 reward -75.00, Last 30ep Avg. rewards -75.00.\n",
      "episode number:  529\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.75621452+0.51722933j  0.18493521+0.3555452j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  529\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.+0.j 0.+1.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  529\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 0.+1.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  529\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  529\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -76.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 530 reward -76.00, Last 30ep Avg. rewards -76.00.\n",
      "episode number:  530\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.59195385+0.18902842j -0.39165489+0.67857597j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -76.0\n",
      "episode number:  530\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -76.0\n",
      "episode number:  530\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -76.0\n",
      "episode number:  530\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -76.0\n",
      "episode number:  530\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -77.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 531 reward -77.00, Last 30ep Avg. rewards -77.00.\n",
      "episode number:  531\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.30162075+0.04938526j -0.76911207+0.5612955j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -77.0\n",
      "episode number:  531\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -77.0\n",
      "episode number:  531\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -77.0\n",
      "episode number:  531\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -76.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 532 reward -76.00, Last 30ep Avg. rewards -76.00.\n",
      "episode number:  532\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.11752889-0.60737315j  0.04676474+0.78428176j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -76.0\n",
      "episode number:  532\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -76.0\n",
      "episode number:  532\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -75.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 533 reward -75.00, Last 30ep Avg. rewards -75.00.\n",
      "episode number:  533\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.03298558-0.68810139j  0.42664032+0.58600892j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  533\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  533\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  533\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -74.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 534 reward -74.00, Last 30ep Avg. rewards -74.00.\n",
      "episode number:  534\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.03770054+0.02683738j  0.90135442-0.43060263j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.61069552-0.28550515j -0.66401213+0.32345893j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  534\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.61069552-0.28550515j -0.66401213+0.32345893j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  534\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  534\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  534\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 535 reward -75.00, Last 30ep Avg. rewards -75.00.\n",
      "episode number:  535\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.87413948-0.1439574j 0.0460245 +0.4615606j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.65065419+0.22457937j 0.58556572-0.42816588j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  535\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.65065419+0.22457937j 0.58556572-0.42816588j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  535\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -74.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 536 reward -74.00, Last 30ep Avg. rewards -74.00.\n",
      "episode number:  536\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.34050727+0.04591673j -0.84478794-0.41021921j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  536\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  536\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  536\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  536\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 537 reward -75.00, Last 30ep Avg. rewards -75.00.\n",
      "episode number:  537\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.60271493-0.41298273j 0.43250234+0.5283197j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  537\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -74.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 538 reward -74.00, Last 30ep Avg. rewards -74.00.\n",
      "episode number:  538\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.74607986-0.52619728j  0.40294471-0.06416248j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  538\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -73.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 539 reward -73.00, Last 30ep Avg. rewards -73.00.\n",
      "episode number:  539\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.35940785+0.1280193j   0.55638768-0.73815297j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -73.0\n",
      "episode number:  539\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -73.0\n",
      "episode number:  539\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -73.0\n",
      "episode number:  539\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULT: False\n",
      "reward till now:  -73.0\n",
      "episode number:  539\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 540 reward -74.00, Last 30ep Avg. rewards -74.00.\n",
      "episode number:  540\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.13632465-0.14932919j -0.96935496-0.1395254j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  540\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  540\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  540\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  540\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 541 reward -75.00, Last 30ep Avg. rewards -75.00.\n",
      "episode number:  541\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.71741824-0.43804243j  0.03689289+0.54043391j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  541\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -74.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 542 reward -74.00, Last 30ep Avg. rewards -74.00.\n",
      "episode number:  542\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.09180853-0.69638581j -0.49960179+0.5069675j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  542\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  542\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  542\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  542\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 543 reward -75.00, Last 30ep Avg. rewards -75.00.\n",
      "episode number:  543\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.93660177-0.0314579j   0.23799853-0.25523365j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  543\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -74.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 544 reward -74.00, Last 30ep Avg. rewards -74.00.\n",
      "episode number:  544\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.16916288+0.02782517j  0.93732516+0.30336647j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.54317276+0.23418786j -0.7824052 -0.19483713j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  544\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.54317276+0.23418786j -0.7824052 -0.19483713j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  544\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  544\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  544\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 545 reward -75.00, Last 30ep Avg. rewards -75.00.\n",
      "episode number:  545\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.29564832+0.52624397j  0.72486212-0.33201545j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  545\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  545\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  545\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  545\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -76.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 546 reward -76.00, Last 30ep Avg. rewards -76.00.\n",
      "episode number:  546\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.52312265-0.2623593j  -0.24773545+0.77209937j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -76.0\n",
      "episode number:  546\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -76.0\n",
      "episode number:  546\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -76.0\n",
      "episode number:  546\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -76.0\n",
      "episode number:  546\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -75.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 547 reward -75.00, Last 30ep Avg. rewards -75.00.\n",
      "episode number:  547\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.29350431-0.67961188j -0.19131684-0.64450041j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  547\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  547\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  547\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  547\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -76.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 548 reward -76.00, Last 30ep Avg. rewards -76.00.\n",
      "episode number:  548\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.51255383+0.7617065j   0.38136411-0.10794994j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -76.0\n",
      "episode number:  548\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -75.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 549 reward -75.00, Last 30ep Avg. rewards -75.00.\n",
      "episode number:  549\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.83973146-0.21403062j -0.37192943+0.33273182j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  549\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  549\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  549\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  549\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -76.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 550 reward -76.00, Last 30ep Avg. rewards -76.00.\n",
      "episode number:  550\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.89471054-0.32988245j  0.2244045 -0.20078156j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -76.0\n",
      "episode number:  550\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -75.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]]\n",
      "Episode 551 reward -75.00, Last 30ep Avg. rewards -75.00.\n",
      "episode number:  551\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.51856995+0.43300848j 0.3463243 -0.65088274j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  551\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -74.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]\n",
      " [0.95]\n",
      " [1.  ]]\n",
      "Episode 552 reward -74.00, Last 30ep Avg. rewards -74.00.\n",
      "episode number:  552\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.01732073-0.79796741j  0.33946153+0.49770861j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  552\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -73.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]\n",
      " [0.95]\n",
      " [1.  ]\n",
      " [0.95]\n",
      " [1.  ]]\n",
      "Episode 553 reward -73.00, Last 30ep Avg. rewards -73.00.\n",
      "episode number:  553\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.84338045+0.38348017j  0.11966317-0.35683763j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -73.0\n",
      "episode number:  553\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -72.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]\n",
      " [0.95]\n",
      " [1.  ]\n",
      " [0.95]\n",
      " [1.  ]\n",
      " [0.95]\n",
      " [1.  ]]\n",
      "Episode 554 reward -72.00, Last 30ep Avg. rewards -72.00.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode number:  554\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.70868494-0.11067353j 0.59713702+0.3590883j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.92335556+0.17565576j 0.07887628-0.33217177j]\n",
      "RESULT: False\n",
      "reward till now:  -72.0\n",
      "episode number:  554\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.92335556+0.17565576j 0.07887628-0.33217177j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70868494-0.11067353j 0.59713702+0.3590883j ]\n",
      "RESULT: False\n",
      "reward till now:  -72.0\n",
      "episode number:  554\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.70868494-0.11067353j 0.59713702+0.3590883j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.92335556+0.17565576j 0.07887628-0.33217177j]\n",
      "RESULT: False\n",
      "reward till now:  -72.0\n",
      "episode number:  554\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.92335556+0.17565576j 0.07887628-0.33217177j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70868494-0.11067353j 0.59713702+0.3590883j ]\n",
      "RESULT: False\n",
      "reward till now:  -72.0\n",
      "episode number:  554\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.70868494-0.11067353j 0.59713702+0.3590883j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.92335556+0.17565576j 0.07887628-0.33217177j]\n",
      "RESULT: False\n",
      "reward till now:  -73.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 555 reward -73.00, Last 30ep Avg. rewards -73.00.\n",
      "episode number:  555\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.56822719-0.67563934j 0.02257168+0.46916933j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.41775789-0.14599634j 0.38583672-0.80950197j]\n",
      "RESULT: False\n",
      "reward till now:  -73.0\n",
      "episode number:  555\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.41775789-0.14599634j 0.38583672-0.80950197j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -73.0\n",
      "episode number:  555\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -72.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 556 reward -72.00, Last 30ep Avg. rewards -72.00.\n",
      "episode number:  556\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.88028529+0.34449151j -0.31517779+0.08418058j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -72.0\n",
      "episode number:  556\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -71.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 557 reward -71.00, Last 30ep Avg. rewards -71.00.\n",
      "episode number:  557\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.69999782-0.23145668j -0.16672462+0.65470127j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.37708109+0.29927913j 0.61286532-0.6266083j ]\n",
      "RESULT: False\n",
      "reward till now:  -71.0\n",
      "episode number:  557\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.37708109+0.29927913j 0.61286532-0.6266083j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -71.0\n",
      "episode number:  557\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -71.0\n",
      "episode number:  557\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -71.0\n",
      "episode number:  557\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -72.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 558 reward -72.00, Last 30ep Avg. rewards -72.00.\n",
      "episode number:  558\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.44544988-0.66670552j  0.41902446-0.42602424j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -72.0\n",
      "episode number:  558\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -71.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 559 reward -71.00, Last 30ep Avg. rewards -71.00.\n",
      "episode number:  559\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.17129898+0.78568169j  0.31805793+0.50219528j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -71.0\n",
      "episode number:  559\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -71.0\n",
      "episode number:  559\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -71.0\n",
      "episode number:  559\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -71.0\n",
      "episode number:  559\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -72.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 560 reward -72.00, Last 30ep Avg. rewards -72.00.\n",
      "episode number:  560\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.24320924-0.62738805j -0.73962565-0.01368898j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -72.0\n",
      "episode number:  560\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -72.0\n",
      "episode number:  560\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -72.0\n",
      "episode number:  560\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -72.0\n",
      "episode number:  560\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -73.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 561 reward -73.00, Last 30ep Avg. rewards -73.00.\n",
      "episode number:  561\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.80269874-0.23044349j -0.53564282+0.12512912j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -73.0\n",
      "episode number:  561\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -72.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 562 reward -72.00, Last 30ep Avg. rewards -72.00.\n",
      "episode number:  562\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.19887011-0.41410649j -0.2111562 +0.86277434j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -72.0\n",
      "episode number:  562\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -71.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 563 reward -71.00, Last 30ep Avg. rewards -71.00.\n",
      "episode number:  563\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.60902119+1.62554415e-05j 0.09975555+7.86855783e-01j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.50118084+0.55640255j 0.36010519-0.55637957j]\n",
      "RESULT: False\n",
      "reward till now:  -71.0\n",
      "episode number:  563\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.50118084+0.55640255j 0.36010519-0.55637957j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -71.0\n",
      "episode number:  563\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -70.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 564 reward -70.00, Last 30ep Avg. rewards -70.00.\n",
      "episode number:  564\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.25704721-0.02248363j -0.03257571+0.96558792j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -70.0\n",
      "episode number:  564\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -70.0\n",
      "episode number:  564\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -70.0\n",
      "episode number:  564\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -70.0\n",
      "episode number:  564\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -71.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 565 reward -71.00, Last 30ep Avg. rewards -71.00.\n",
      "episode number:  565\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.98566538-0.02143911j -0.08936966-0.14148209j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -71.0\n",
      "episode number:  565\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -71.0\n",
      "episode number:  565\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -71.0\n",
      "episode number:  565\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -71.0\n",
      "episode number:  565\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -72.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 566 reward -72.00, Last 30ep Avg. rewards -72.00.\n",
      "episode number:  566\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.00094951-0.149523j   0.09766575-0.98392244j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -72.0\n",
      "episode number:  566\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -72.0\n",
      "episode number:  566\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -72.0\n",
      "episode number:  566\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -72.0\n",
      "episode number:  566\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -73.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 567 reward -73.00, Last 30ep Avg. rewards -73.00.\n",
      "episode number:  567\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.4671094 +0.00209178j -0.84427304-0.26269273j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -73.0\n",
      "episode number:  567\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -73.0\n",
      "episode number:  567\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -73.0\n",
      "episode number:  567\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -73.0\n",
      "episode number:  567\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 568 reward -74.00, Last 30ep Avg. rewards -74.00.\n",
      "episode number:  568\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.85398878-0.03355911j  0.45506601-0.24998376j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  568\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -73.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 569 reward -73.00, Last 30ep Avg. rewards -73.00.\n",
      "episode number:  569\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.22129847-0.3375017j  -0.78227853+0.47451016j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -73.0\n",
      "episode number:  569\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -73.0\n",
      "episode number:  569\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -73.0\n",
      "episode number:  569\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -73.0\n",
      "episode number:  569\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 570 reward -74.00, Last 30ep Avg. rewards -74.00.\n",
      "episode number:  570\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.39121953+0.3019439j  0.81387059+0.30560074j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.8521274 +0.42959894j -0.29885943-0.00258578j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  570\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.8521274 +0.42959894j -0.29885943-0.00258578j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.39121953+0.3019439j  0.81387059+0.30560074j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  570\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.39121953+0.3019439j  0.81387059+0.30560074j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.8521274 +0.42959894j -0.29885943-0.00258578j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  570\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.8521274 +0.42959894j -0.29885943-0.00258578j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.39121953+0.3019439j  0.81387059+0.30560074j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  570\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.39121953+0.3019439j  0.81387059+0.30560074j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.8521274 +0.42959894j -0.29885943-0.00258578j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 571 reward -75.00, Last 30ep Avg. rewards -75.00.\n",
      "episode number:  571\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.91443682-0.08595158j -0.34889083-0.18626009j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.3999014 -0.19248272j 0.89330755+0.07092883j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  571\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.3999014 -0.19248272j 0.89330755+0.07092883j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.91443682-0.08595158j -0.34889083-0.18626009j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  571\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.91443682-0.08595158j -0.34889083-0.18626009j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.3999014 -0.19248272j 0.89330755+0.07092883j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  571\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.3999014 -0.19248272j 0.89330755+0.07092883j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.91443682-0.08595158j -0.34889083-0.18626009j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  571\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.91443682-0.08595158j -0.34889083-0.18626009j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.3999014 -0.19248272j 0.89330755+0.07092883j]\n",
      "RESULT: False\n",
      "reward till now:  -76.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 572 reward -76.00, Last 30ep Avg. rewards -76.00.\n",
      "episode number:  572\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.25767628+0.44731824j -0.49665625+0.69774056j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -76.0\n",
      "episode number:  572\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -76.0\n",
      "episode number:  572\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -76.0\n",
      "episode number:  572\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -76.0\n",
      "episode number:  572\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -77.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 573 reward -77.00, Last 30ep Avg. rewards -77.00.\n",
      "episode number:  573\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.03928748-0.9026186j  -0.4272133 +0.03499938j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -77.0\n",
      "episode number:  573\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -77.0\n",
      "episode number:  573\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -77.0\n",
      "episode number:  573\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -77.0\n",
      "episode number:  573\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -78.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 574 reward -78.00, Last 30ep Avg. rewards -78.00.\n",
      "episode number:  574\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.15605367+0.14124679j 0.34955341+0.91296715j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -78.0\n",
      "episode number:  574\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -78.0\n",
      "episode number:  574\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -78.0\n",
      "episode number:  574\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -78.0\n",
      "episode number:  574\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -79.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 575 reward -79.00, Last 30ep Avg. rewards -79.00.\n",
      "episode number:  575\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.63668407-0.69300672j -0.28283881-0.18541114j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.25020639-0.62113522j 0.65020086-0.35892427j]\n",
      "RESULT: False\n",
      "reward till now:  -79.0\n",
      "episode number:  575\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.25020639-0.62113522j 0.65020086-0.35892427j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -79.0\n",
      "episode number:  575\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -78.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 576 reward -78.00, Last 30ep Avg. rewards -78.00.\n",
      "episode number:  576\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.42769719+0.8479721j   0.11240418-0.29220494j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -78.0\n",
      "episode number:  576\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -77.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 577 reward -77.00, Last 30ep Avg. rewards -77.00.\n",
      "episode number:  577\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.75411035-0.48505582j 0.3783077 -0.23004719j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.80074049-0.50565419j 0.2657326 -0.18031834j]\n",
      "RESULT: False\n",
      "reward till now:  -77.0\n",
      "episode number:  577\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.80074049-0.50565419j 0.2657326 -0.18031834j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.75411035-0.48505582j 0.3783077 -0.23004719j]\n",
      "RESULT: False\n",
      "reward till now:  -77.0\n",
      "episode number:  577\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.75411035-0.48505582j 0.3783077 -0.23004719j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.80074049-0.50565419j 0.2657326 -0.18031834j]\n",
      "RESULT: False\n",
      "reward till now:  -77.0\n",
      "episode number:  577\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.80074049-0.50565419j 0.2657326 -0.18031834j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.75411035-0.48505582j 0.3783077 -0.23004719j]\n",
      "RESULT: False\n",
      "reward till now:  -77.0\n",
      "episode number:  577\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.75411035-0.48505582j 0.3783077 -0.23004719j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.80074049-0.50565419j 0.2657326 -0.18031834j]\n",
      "RESULT: False\n",
      "reward till now:  -78.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 578 reward -78.00, Last 30ep Avg. rewards -78.00.\n",
      "episode number:  578\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.60720541-0.14224954j 0.75031528+0.21930262j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.95991208+0.05448476j -0.10119396-0.25565599j]\n",
      "RESULT: False\n",
      "reward till now:  -78.0\n",
      "episode number:  578\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.95991208+0.05448476j -0.10119396-0.25565599j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.60720541-0.14224954j 0.75031528+0.21930262j]\n",
      "RESULT: False\n",
      "reward till now:  -78.0\n",
      "episode number:  578\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.60720541-0.14224954j 0.75031528+0.21930262j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.95991208+0.05448476j -0.10119396-0.25565599j]\n",
      "RESULT: False\n",
      "reward till now:  -78.0\n",
      "episode number:  578\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.95991208+0.05448476j -0.10119396-0.25565599j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.60720541-0.14224954j 0.75031528+0.21930262j]\n",
      "RESULT: False\n",
      "reward till now:  -78.0\n",
      "episode number:  578\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.60720541-0.14224954j 0.75031528+0.21930262j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.95991208+0.05448476j -0.10119396-0.25565599j]\n",
      "RESULT: False\n",
      "reward till now:  -79.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 579 reward -79.00, Last 30ep Avg. rewards -79.00.\n",
      "episode number:  579\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.13025444+0.55478837j  0.27634524+0.77387141j]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -79.0\n",
      "episode number:  579\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -79.0\n",
      "episode number:  579\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -79.0\n",
      "episode number:  579\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -79.0\n",
      "episode number:  579\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -80.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 580 reward -80.00, Last 30ep Avg. rewards -80.00.\n",
      "episode number:  580\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.20610557-0.01196903j -0.90421982-0.3738499j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -80.0\n",
      "episode number:  580\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -80.0\n",
      "episode number:  580\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -80.0\n",
      "episode number:  580\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -80.0\n",
      "episode number:  580\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -81.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 581 reward -81.00, Last 30ep Avg. rewards -81.00.\n",
      "episode number:  581\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.08818518-0.07017228j 0.94583294-0.30446556j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -81.0\n",
      "episode number:  581\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -81.0\n",
      "episode number:  581\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -81.0\n",
      "episode number:  581\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -81.0\n",
      "episode number:  581\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -82.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 582 reward -82.00, Last 30ep Avg. rewards -82.00.\n",
      "episode number:  582\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.07312478+0.47576929j  0.34027863-0.80777893j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -82.0\n",
      "episode number:  582\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -82.0\n",
      "episode number:  582\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -82.0\n",
      "episode number:  582\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -82.0\n",
      "episode number:  582\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -83.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 583 reward -83.00, Last 30ep Avg. rewards -83.00.\n",
      "episode number:  583\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.04055304+0.67495868j  0.6172309 +0.40225893j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -83.0\n",
      "episode number:  583\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -83.0\n",
      "episode number:  583\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -83.0\n",
      "episode number:  583\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -83.0\n",
      "episode number:  583\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -84.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 584 reward -84.00, Last 30ep Avg. rewards -84.00.\n",
      "episode number:  584\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.57335813+0.08850302j 0.79871235+0.15964416j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -84.0\n",
      "episode number:  584\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -83.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 585 reward -83.00, Last 30ep Avg. rewards -83.00.\n",
      "episode number:  585\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.4056477 -0.16610279j 0.87303771+0.21369363j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -83.0\n",
      "episode number:  585\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -83.0\n",
      "episode number:  585\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -83.0\n",
      "episode number:  585\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -83.0\n",
      "episode number:  585\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -84.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 586 reward -84.00, Last 30ep Avg. rewards -84.00.\n",
      "episode number:  586\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.19236857-0.15127489j  0.65920091-0.71103052j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -84.0\n",
      "episode number:  586\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -84.0\n",
      "episode number:  586\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -84.0\n",
      "episode number:  586\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -84.0\n",
      "episode number:  586\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -85.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 587 reward -85.00, Last 30ep Avg. rewards -85.00.\n",
      "episode number:  587\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.4302283 -0.65539083j 0.62000321-0.03102419j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -85.0\n",
      "episode number:  587\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -84.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 588 reward -84.00, Last 30ep Avg. rewards -84.00.\n",
      "episode number:  588\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.33138815-0.10208147j 0.92640567-0.14674402j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -84.0\n",
      "episode number:  588\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -84.0\n",
      "episode number:  588\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [ 0.        -0.70710678j -0.70710678+0.j        ]\n",
      "RESULT: False\n",
      "reward till now:  -84.0\n",
      "episode number:  588\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.        -0.70710678j -0.70710678+0.j        ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -84.0\n",
      "episode number:  588\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -85.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 589 reward -85.00, Last 30ep Avg. rewards -85.00.\n",
      "episode number:  589\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.26522931+0.1780715j   0.19442925+0.92743799j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -85.0\n",
      "episode number:  589\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -85.0\n",
      "episode number:  589\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -85.0\n",
      "episode number:  589\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -85.0\n",
      "episode number:  589\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -86.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 590 reward -86.00, Last 30ep Avg. rewards -86.00.\n",
      "episode number:  590\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.4284346 -0.70281665j -0.55409165+0.12439851j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -86.0\n",
      "episode number:  590\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -85.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]]\n",
      "Episode 591 reward -85.00, Last 30ep Avg. rewards -85.00.\n",
      "episode number:  591\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.41350913+0.429061j   -0.46302747-0.65614208j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -85.0\n",
      "episode number:  591\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -84.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]\n",
      " [0.95]\n",
      " [1.  ]]\n",
      "Episode 592 reward -84.00, Last 30ep Avg. rewards -84.00.\n",
      "episode number:  592\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.51161817-0.50027054j -0.44433258+0.53902207j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -84.0\n",
      "episode number:  592\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -83.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]\n",
      " [0.95]\n",
      " [1.  ]\n",
      " [0.95]\n",
      " [1.  ]]\n",
      "Episode 593 reward -83.00, Last 30ep Avg. rewards -83.00.\n",
      "episode number:  593\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.88044884-0.33678077j 0.26296239+0.2055221j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.80851383-0.09281389j 0.43662886-0.38346603j]\n",
      "RESULT: False\n",
      "reward till now:  -83.0\n",
      "episode number:  593\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.80851383-0.09281389j 0.43662886-0.38346603j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.88044884-0.33678077j 0.26296239+0.2055221j ]\n",
      "RESULT: False\n",
      "reward till now:  -83.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode number:  593\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.88044884-0.33678077j 0.26296239+0.2055221j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.80851383-0.09281389j 0.43662886-0.38346603j]\n",
      "RESULT: False\n",
      "reward till now:  -83.0\n",
      "episode number:  593\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.80851383-0.09281389j 0.43662886-0.38346603j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.88044884-0.33678077j 0.26296239+0.2055221j ]\n",
      "RESULT: False\n",
      "reward till now:  -83.0\n",
      "episode number:  593\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.88044884-0.33678077j 0.26296239+0.2055221j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.80851383-0.09281389j 0.43662886-0.38346603j]\n",
      "RESULT: False\n",
      "reward till now:  -84.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 594 reward -84.00, Last 30ep Avg. rewards -84.00.\n",
      "episode number:  594\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.60885561-0.17740847j 0.2704233 -0.72435649j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.08167145-0.31666488j -0.06577141+0.94272331j]\n",
      "RESULT: False\n",
      "reward till now:  -84.0\n",
      "episode number:  594\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.08167145-0.31666488j -0.06577141+0.94272331j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -84.0\n",
      "episode number:  594\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -84.0\n",
      "episode number:  594\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -84.0\n",
      "episode number:  594\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -85.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 595 reward -85.00, Last 30ep Avg. rewards -85.00.\n",
      "episode number:  595\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.32752094-0.86178839j -0.25102169+0.29502358j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -85.0\n",
      "episode number:  595\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -84.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 596 reward -84.00, Last 30ep Avg. rewards -84.00.\n",
      "episode number:  596\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.40715282+0.56043498j 0.02690891+0.72070461j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -84.0\n",
      "episode number:  596\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -84.0\n",
      "episode number:  596\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -84.0\n",
      "episode number:  596\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -84.0\n",
      "episode number:  596\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -85.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 597 reward -85.00, Last 30ep Avg. rewards -85.00.\n",
      "episode number:  597\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.36413952+0.22240133j -0.63846533+0.64054826j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -85.0\n",
      "episode number:  597\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -84.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 598 reward -84.00, Last 30ep Avg. rewards -84.00.\n",
      "episode number:  598\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.76014176+0.62282984j  0.03338744+0.18207906j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -84.0\n",
      "episode number:  598\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -83.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 599 reward -83.00, Last 30ep Avg. rewards -83.00.\n",
      "episode number:  599\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.64189046-0.73354913j -0.1720314 +0.14243425j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.33224052-0.41798134j 0.57552966-0.61941379j]\n",
      "RESULT: False\n",
      "reward till now:  -83.0\n",
      "episode number:  599\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.33224052-0.41798134j 0.57552966-0.61941379j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -83.0\n",
      "episode number:  599\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -83.0\n",
      "episode number:  599\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -83.0\n",
      "episode number:  599\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -84.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 600 reward -84.00, Last 30ep Avg. rewards -84.00.\n",
      "episode number:  600\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.46044854+0.2893027j  -0.6731793 +0.50111947j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -84.0\n",
      "episode number:  600\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -83.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]]\n",
      "Episode 601 reward -83.00, Last 30ep Avg. rewards -83.00.\n",
      "episode number:  601\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.12303536-0.59588822j 0.29123179+0.73821648j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -83.0\n",
      "episode number:  601\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -82.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]\n",
      " [0.95]\n",
      " [1.  ]]\n",
      "Episode 602 reward -82.00, Last 30ep Avg. rewards -82.00.\n",
      "episode number:  602\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.71011512-0.27045276j -0.64982895-0.01772443j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -82.0\n",
      "episode number:  602\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -81.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]\n",
      " [0.95]\n",
      " [1.  ]\n",
      " [0.95]\n",
      " [1.  ]]\n",
      "Episode 603 reward -81.00, Last 30ep Avg. rewards -81.00.\n",
      "episode number:  603\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.16621548+0.55965513j 0.70972046-0.39427835j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -81.0\n",
      "episode number:  603\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -81.0\n",
      "episode number:  603\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -81.0\n",
      "episode number:  603\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -81.0\n",
      "episode number:  603\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -82.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 604 reward -82.00, Last 30ep Avg. rewards -82.00.\n",
      "episode number:  604\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.36065717+0.49719105j -0.47604923+0.62936841j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -82.0\n",
      "episode number:  604\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -82.0\n",
      "episode number:  604\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -82.0\n",
      "episode number:  604\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -82.0\n",
      "episode number:  604\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -83.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 605 reward -83.00, Last 30ep Avg. rewards -83.00.\n",
      "episode number:  605\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.005408  +0.89696578j -0.24183666-0.37005158j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -83.0\n",
      "episode number:  605\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -82.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 606 reward -82.00, Last 30ep Avg. rewards -82.00.\n",
      "episode number:  606\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.79960217+0.03045506j -0.38734552+0.45789989j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -82.0\n",
      "episode number:  606\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -81.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 607 reward -81.00, Last 30ep Avg. rewards -81.00.\n",
      "episode number:  607\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.16300919-0.74921575j  0.62049883-0.16457509j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -81.0\n",
      "episode number:  607\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -81.0\n",
      "episode number:  607\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -81.0\n",
      "episode number:  607\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [0.-1.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -81.0\n",
      "episode number:  607\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.-1.j 0.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -82.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 608 reward -82.00, Last 30ep Avg. rewards -82.00.\n",
      "episode number:  608\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.67358867+0.05508183j -0.25305473+0.69224822j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.25305473+0.69224822j -0.67358867+0.05508183j]\n",
      "RESULT: False\n",
      "reward till now:  -82.0\n",
      "episode number:  608\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.25305473+0.69224822j -0.67358867+0.05508183j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -82.0\n",
      "episode number:  608\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -82.0\n",
      "episode number:  608\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -82.0\n",
      "episode number:  608\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -83.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 609 reward -83.00, Last 30ep Avg. rewards -83.00.\n",
      "episode number:  609\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.48810185+0.4880091j  -0.67741355+0.25439061j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -83.0\n",
      "episode number:  609\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -83.0\n",
      "episode number:  609\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -83.0\n",
      "episode number:  609\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -83.0\n",
      "episode number:  609\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -84.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 610 reward -84.00, Last 30ep Avg. rewards -84.00.\n",
      "episode number:  610\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.2313075 +0.29774592j 0.85383466-0.3589019j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -84.0\n",
      "episode number:  610\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -84.0\n",
      "episode number:  610\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -84.0\n",
      "episode number:  610\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -84.0\n",
      "episode number:  610\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -85.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 611 reward -85.00, Last 30ep Avg. rewards -85.00.\n",
      "episode number:  611\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.28355485-0.06514603j  0.02006152-0.95653028j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -85.0\n",
      "episode number:  611\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -85.0\n",
      "episode number:  611\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -85.0\n",
      "episode number:  611\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -85.0\n",
      "episode number:  611\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -86.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 612 reward -86.00, Last 30ep Avg. rewards -86.00.\n",
      "episode number:  612\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.38179959-0.32013989j  0.86698898+0.00834499j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -86.0\n",
      "episode number:  612\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -86.0\n",
      "episode number:  612\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -86.0\n",
      "episode number:  612\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -86.0\n",
      "episode number:  612\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -87.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 613 reward -87.00, Last 30ep Avg. rewards -87.00.\n",
      "episode number:  613\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.34245081+0.17104301j 0.78322105-0.48993522j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -87.0\n",
      "episode number:  613\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -87.0\n",
      "episode number:  613\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -87.0\n",
      "episode number:  613\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -87.0\n",
      "episode number:  613\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -88.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 614 reward -88.00, Last 30ep Avg. rewards -88.00.\n",
      "episode number:  614\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.79986364+0.56304295j 0.08312975-0.19049997j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.62437061+0.26342767j 0.5068074 +0.5328353j ]\n",
      "RESULT: False\n",
      "reward till now:  -88.0\n",
      "episode number:  614\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.62437061+0.26342767j 0.5068074 +0.5328353j ]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.5068074 +0.5328353j  0.62437061+0.26342767j]\n",
      "RESULT: False\n",
      "reward till now:  -88.0\n",
      "episode number:  614\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.5068074 +0.5328353j  0.62437061+0.26342767j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -88.0\n",
      "episode number:  614\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -88.0\n",
      "episode number:  614\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -89.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 615 reward -89.00, Last 30ep Avg. rewards -89.00.\n",
      "episode number:  615\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.34903361-0.42565978j -0.49395968+0.67304764j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -89.0\n",
      "episode number:  615\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -89.0\n",
      "episode number:  615\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -89.0\n",
      "episode number:  615\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -89.0\n",
      "episode number:  615\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -90.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 616 reward -90.00, Last 30ep Avg. rewards -90.00.\n",
      "episode number:  616\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.05095398-0.64393583j -0.50729954+0.57043625j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -90.0\n",
      "episode number:  616\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -90.0\n",
      "episode number:  616\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -90.0\n",
      "episode number:  616\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -90.0\n",
      "episode number:  616\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -91.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 617 reward -91.00, Last 30ep Avg. rewards -91.00.\n",
      "episode number:  617\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.14884417+0.88606851j 0.34742915+0.26836728j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -91.0\n",
      "episode number:  617\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -90.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 618 reward -90.00, Last 30ep Avg. rewards -90.00.\n",
      "episode number:  618\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.0245622+0.54879883j 0.367298 +0.75053895j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -90.0\n",
      "episode number:  618\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -90.0\n",
      "episode number:  618\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -90.0\n",
      "episode number:  618\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -90.0\n",
      "episode number:  618\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -91.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 619 reward -91.00, Last 30ep Avg. rewards -91.00.\n",
      "episode number:  619\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.43803231+0.87386736j -0.17565385+0.11674445j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -91.0\n",
      "episode number:  619\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -90.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 620 reward -90.00, Last 30ep Avg. rewards -90.00.\n",
      "episode number:  620\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.30049305-0.81156771j -0.2804376 +0.4152307j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -90.0\n",
      "episode number:  620\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -89.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]]\n",
      "Episode 621 reward -89.00, Last 30ep Avg. rewards -89.00.\n",
      "episode number:  621\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.06897271-0.72683426j 0.22284735-0.64598281j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -89.0\n",
      "episode number:  621\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -88.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]\n",
      " [0.95]\n",
      " [1.  ]]\n",
      "Episode 622 reward -88.00, Last 30ep Avg. rewards -88.00.\n",
      "episode number:  622\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.55702535+0.10300703j -0.66525419+0.48636322j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -88.0\n",
      "episode number:  622\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -88.0\n",
      "episode number:  622\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -87.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 623 reward -87.00, Last 30ep Avg. rewards -87.00.\n",
      "episode number:  623\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.61694378-0.27883692j  0.72078353-0.14866551j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -87.0\n",
      "episode number:  623\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -86.0\n",
      "in discounted reward\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disc_rw:  [[0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 624 reward -86.00, Last 30ep Avg. rewards -86.00.\n",
      "episode number:  624\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.89192285-0.30675524j  0.30828277-0.12384098j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -86.0\n",
      "episode number:  624\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -86.0\n",
      "episode number:  624\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -85.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 625 reward -85.00, Last 30ep Avg. rewards -85.00.\n",
      "episode number:  625\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.0302816 +0.17066007j -0.98485514+0.00430401j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -85.0\n",
      "episode number:  625\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -85.0\n",
      "episode number:  625\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -84.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 626 reward -84.00, Last 30ep Avg. rewards -84.00.\n",
      "episode number:  626\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.31630523-0.32885079j -0.15008906+0.8770869j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -84.0\n",
      "episode number:  626\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -84.0\n",
      "episode number:  626\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -83.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 627 reward -83.00, Last 30ep Avg. rewards -83.00.\n",
      "episode number:  627\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.19710239-0.22707704j -0.64498948-0.7025491j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -83.0\n",
      "episode number:  627\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -83.0\n",
      "episode number:  627\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -82.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 628 reward -82.00, Last 30ep Avg. rewards -82.00.\n",
      "episode number:  628\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.46958525+0.40016081j  0.35778357+0.700965j  ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -82.0\n",
      "episode number:  628\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -82.0\n",
      "episode number:  628\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -81.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 629 reward -81.00, Last 30ep Avg. rewards -81.00.\n",
      "episode number:  629\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.86040635+0.04696036j  0.30184587-0.40790282j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -81.0\n",
      "episode number:  629\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -80.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 630 reward -80.00, Last 30ep Avg. rewards -80.00.\n",
      "episode number:  630\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.35572511+0.26885385j -0.68935167+0.57093916j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -80.0\n",
      "episode number:  630\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -80.0\n",
      "episode number:  630\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -79.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 631 reward -79.00, Last 30ep Avg. rewards -79.00.\n",
      "episode number:  631\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.95467491-0.27093445j 0.07175514-0.10020753j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.72579564-0.26243701j 0.62431856-0.12072217j]\n",
      "RESULT: False\n",
      "reward till now:  -79.0\n",
      "episode number:  631\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.72579564-0.26243701j 0.62431856-0.12072217j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.62431856-0.12072217j 0.72579564-0.26243701j]\n",
      "RESULT: False\n",
      "reward till now:  -79.0\n",
      "episode number:  631\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.62431856-0.12072217j 0.72579564-0.26243701j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.72579564-0.26243701j 0.62431856-0.12072217j]\n",
      "RESULT: False\n",
      "reward till now:  -79.0\n",
      "episode number:  631\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.72579564-0.26243701j 0.62431856-0.12072217j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.62431856-0.12072217j 0.72579564-0.26243701j]\n",
      "RESULT: False\n",
      "reward till now:  -79.0\n",
      "episode number:  631\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.62431856-0.12072217j 0.72579564-0.26243701j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.72579564-0.26243701j 0.62431856-0.12072217j]\n",
      "RESULT: False\n",
      "reward till now:  -80.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 632 reward -80.00, Last 30ep Avg. rewards -80.00.\n",
      "episode number:  632\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.73127392+0.256723j   -0.12346697-0.61974806j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -80.0\n",
      "episode number:  632\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -79.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 633 reward -79.00, Last 30ep Avg. rewards -79.00.\n",
      "episode number:  633\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.78219666+0.60376055j -0.0573517 +0.14266171j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -79.0\n",
      "episode number:  633\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -78.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 634 reward -78.00, Last 30ep Avg. rewards -78.00.\n",
      "episode number:  634\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.43707265+0.78489304j -0.35489292-0.25876909j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -78.0\n",
      "episode number:  634\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -77.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 635 reward -77.00, Last 30ep Avg. rewards -77.00.\n",
      "episode number:  635\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.35379255-0.4387025j  -0.82039267+0.09657546j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -77.0\n",
      "episode number:  635\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -76.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 636 reward -76.00, Last 30ep Avg. rewards -76.00.\n",
      "episode number:  636\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.99981823-0.00830389j  0.00756216-0.01540685j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -76.0\n",
      "episode number:  636\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -75.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 637 reward -75.00, Last 30ep Avg. rewards -75.00.\n",
      "episode number:  637\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.45093071-0.86811625j 0.09483662-0.18450389j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.09483662-0.18450389j 0.45093071-0.86811625j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  637\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.09483662-0.18450389j 0.45093071-0.86811625j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  637\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  637\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -74.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 638 reward -74.00, Last 30ep Avg. rewards -74.00.\n",
      "episode number:  638\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.39238815-0.4596234j  0.70746494+0.36643038j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.70746494+0.36643038j 0.39238815-0.4596234j ]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  638\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.70746494+0.36643038j 0.39238815-0.4596234j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.77771358-0.06589742j 0.22279293+0.58410823j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  638\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.77771358-0.06589742j 0.22279293+0.58410823j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70746494+0.36643038j 0.39238815-0.4596234j ]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  638\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.70746494+0.36643038j 0.39238815-0.4596234j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.77771358-0.06589742j 0.22279293+0.58410823j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  638\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.77771358-0.06589742j 0.22279293+0.58410823j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70746494+0.36643038j 0.39238815-0.4596234j ]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 639 reward -75.00, Last 30ep Avg. rewards -75.00.\n",
      "episode number:  639\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.52234028-0.26982389j 0.77331026-0.23737511j]]]\n",
      "Action:  bit_flip_Y(new_state)\n",
      "NEW STATE:  [-0.23737511-0.77331026j  0.26982389+0.52234028j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  639\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.23737511-0.77331026j  0.26982389+0.52234028j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  639\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  639\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -74.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 640 reward -74.00, Last 30ep Avg. rewards -74.00.\n",
      "episode number:  640\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.55440757+0.28701715j 0.73545301+0.26336719j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.73545301+0.26336719j 0.55440757+0.28701715j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  640\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.73545301+0.26336719j 0.55440757+0.28701715j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.91206916+0.38918049j 0.12801846-0.01672305j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  640\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.91206916+0.38918049j 0.12801846-0.01672305j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.73545301+0.26336719j 0.55440757+0.28701715j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  640\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.73545301+0.26336719j 0.55440757+0.28701715j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.91206916+0.38918049j 0.12801846-0.01672305j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  640\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.91206916+0.38918049j 0.12801846-0.01672305j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.73545301+0.26336719j 0.55440757+0.28701715j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 641 reward -75.00, Last 30ep Avg. rewards -75.00.\n",
      "episode number:  641\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.02425845+0.95000953j -0.23861608-0.19988947j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  641\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -74.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 642 reward -74.00, Last 30ep Avg. rewards -74.00.\n",
      "episode number:  642\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.84639323+0.0444888j   0.09637473+0.52187275j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  642\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -73.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 643 reward -73.00, Last 30ep Avg. rewards -73.00.\n",
      "episode number:  643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.00584801+0.70904253j 0.02642623-0.70464611j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -73.0\n",
      "episode number:  643\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -72.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 644 reward -72.00, Last 30ep Avg. rewards -72.00.\n",
      "episode number:  644\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.88271307+0.04084788j -0.46789193+0.01504124j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.29332284+0.03951958j 0.95502195+0.01824805j]\n",
      "RESULT: False\n",
      "reward till now:  -72.0\n",
      "episode number:  644\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.29332284+0.03951958j 0.95502195+0.01824805j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.95502195+0.01824805j 0.29332284+0.03951958j]\n",
      "RESULT: False\n",
      "reward till now:  -72.0\n",
      "episode number:  644\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.95502195+0.01824805j 0.29332284+0.03951958j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.88271307+0.04084788j 0.46789193-0.01504124j]\n",
      "RESULT: False\n",
      "reward till now:  -72.0\n",
      "episode number:  644\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.88271307+0.04084788j 0.46789193-0.01504124j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.95502195+0.01824805j 0.29332284+0.03951958j]\n",
      "RESULT: False\n",
      "reward till now:  -72.0\n",
      "episode number:  644\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.95502195+0.01824805j 0.29332284+0.03951958j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.88271307+0.04084788j 0.46789193-0.01504124j]\n",
      "RESULT: False\n",
      "reward till now:  -73.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 645 reward -73.00, Last 30ep Avg. rewards -73.00.\n",
      "episode number:  645\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.27673918+0.63233952j -0.01680066-0.72338088j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.01680066-0.72338088j  0.27673918+0.63233952j]\n",
      "RESULT: False\n",
      "reward till now:  -73.0\n",
      "episode number:  645\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.01680066-0.72338088j  0.27673918+0.63233952j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -73.0\n",
      "episode number:  645\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -72.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 646 reward -72.00, Last 30ep Avg. rewards -72.00.\n",
      "episode number:  646\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.02007978-0.43365897j  0.89539253-0.09904001j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [ 0.89539253-0.09904001j -0.02007978-0.43365897j]\n",
      "RESULT: False\n",
      "reward till now:  -72.0\n",
      "episode number:  646\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.89539253-0.09904001j -0.02007978-0.43365897j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.61893958-0.37667506j 0.64733668+0.23661133j]\n",
      "RESULT: False\n",
      "reward till now:  -72.0\n",
      "episode number:  646\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.61893958-0.37667506j 0.64733668+0.23661133j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.64733668+0.23661133j 0.61893958-0.37667506j]\n",
      "RESULT: False\n",
      "reward till now:  -72.0\n",
      "episode number:  646\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.64733668+0.23661133j 0.61893958-0.37667506j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.61893958-0.37667506j 0.64733668+0.23661133j]\n",
      "RESULT: False\n",
      "reward till now:  -72.0\n",
      "episode number:  646\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.61893958-0.37667506j 0.64733668+0.23661133j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.64733668+0.23661133j 0.61893958-0.37667506j]\n",
      "RESULT: False\n",
      "reward till now:  -73.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 647 reward -73.00, Last 30ep Avg. rewards -73.00.\n",
      "episode number:  647\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.34599429+0.60464253j -0.00767335-0.71738168j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.00767335-0.71738168j  0.34599429+0.60464253j]\n",
      "RESULT: False\n",
      "reward till now:  -73.0\n",
      "episode number:  647\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.00767335-0.71738168j  0.34599429+0.60464253j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -73.0\n",
      "episode number:  647\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -73.0\n",
      "episode number:  647\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -72.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 648 reward -72.00, Last 30ep Avg. rewards -72.00.\n",
      "episode number:  648\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.00262392+0.02405199j -0.88779948-0.45959407j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -72.0\n",
      "episode number:  648\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -72.0\n",
      "episode number:  648\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -71.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 649 reward -71.00, Last 30ep Avg. rewards -71.00.\n",
      "episode number:  649\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.94748407+0.00966147j  0.28514759-0.14446954j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -71.0\n",
      "episode number:  649\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -70.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 650 reward -70.00, Last 30ep Avg. rewards -70.00.\n",
      "episode number:  650\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.19582731+0.95968092j 0.01865374+0.20078902j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -70.0\n",
      "episode number:  650\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -69.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]]\n",
      "Episode 651 reward -69.00, Last 30ep Avg. rewards -69.00.\n",
      "episode number:  651\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.52170413+0.19720695j -0.79104932-0.25134674j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.79104932-0.25134674j  0.52170413+0.19720695j]\n",
      "RESULT: False\n",
      "reward till now:  -69.0\n",
      "episode number:  651\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.79104932-0.25134674j  0.52170413+0.19720695j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -69.0\n",
      "episode number:  651\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -69.0\n",
      "episode number:  651\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -68.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 652 reward -68.00, Last 30ep Avg. rewards -68.00.\n",
      "episode number:  652\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.15862751-0.74247394j  0.58983877+0.2750636j ]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [ 0.58983877+0.2750636j  -0.15862751-0.74247394j]\n",
      "RESULT: False\n",
      "reward till now:  -68.0\n",
      "episode number:  652\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.58983877+0.2750636j  -0.15862751-0.74247394j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.30491241-0.33050902j 0.52924558+0.71950769j]\n",
      "RESULT: False\n",
      "reward till now:  -68.0\n",
      "episode number:  652\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.30491241-0.33050902j 0.52924558+0.71950769j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.52924558+0.71950769j 0.30491241-0.33050902j]\n",
      "RESULT: False\n",
      "reward till now:  -68.0\n",
      "episode number:  652\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.52924558+0.71950769j 0.30491241-0.33050902j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.30491241-0.33050902j 0.52924558+0.71950769j]\n",
      "RESULT: False\n",
      "reward till now:  -68.0\n",
      "episode number:  652\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.30491241-0.33050902j 0.52924558+0.71950769j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.52924558+0.71950769j 0.30491241-0.33050902j]\n",
      "RESULT: False\n",
      "reward till now:  -69.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 653 reward -69.00, Last 30ep Avg. rewards -69.00.\n",
      "episode number:  653\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.61902561+0.04705591j  0.78024962+0.07618111j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -69.0\n",
      "episode number:  653\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -68.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 654 reward -68.00, Last 30ep Avg. rewards -68.00.\n",
      "episode number:  654\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.93757385-0.2170199j  -0.25048826+0.10541952j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -68.0\n",
      "episode number:  654\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -67.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 655 reward -67.00, Last 30ep Avg. rewards -67.00.\n",
      "episode number:  655\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.98630251-0.15941178j -0.0396114 +0.01503922j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.66941171-0.10208682j 0.72543068-0.12335549j]\n",
      "RESULT: False\n",
      "reward till now:  -67.0\n",
      "episode number:  655\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.66941171-0.10208682j 0.72543068-0.12335549j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.72543068-0.12335549j 0.66941171-0.10208682j]\n",
      "RESULT: False\n",
      "reward till now:  -67.0\n",
      "episode number:  655\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.72543068-0.12335549j 0.66941171-0.10208682j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.98630251-0.15941178j 0.0396114 -0.01503922j]\n",
      "RESULT: False\n",
      "reward till now:  -67.0\n",
      "episode number:  655\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.98630251-0.15941178j 0.0396114 -0.01503922j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.72543068-0.12335549j 0.66941171-0.10208682j]\n",
      "RESULT: False\n",
      "reward till now:  -67.0\n",
      "episode number:  655\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.72543068-0.12335549j 0.66941171-0.10208682j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.98630251-0.15941178j 0.0396114 -0.01503922j]\n",
      "RESULT: False\n",
      "reward till now:  -68.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 656 reward -68.00, Last 30ep Avg. rewards -68.00.\n",
      "episode number:  656\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.20929455+0.17582038j -0.63339036+0.72394726j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -68.0\n",
      "episode number:  656\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -68.0\n",
      "episode number:  656\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -67.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 657 reward -67.00, Last 30ep Avg. rewards -67.00.\n",
      "episode number:  657\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.18139306-0.40469134j  0.29905183-0.84491981j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -67.0\n",
      "episode number:  657\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -67.0\n",
      "episode number:  657\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -66.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 658 reward -66.00, Last 30ep Avg. rewards -66.00.\n",
      "episode number:  658\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.30313499+0.11840926j  0.41381065+0.85020537j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -66.0\n",
      "episode number:  658\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -66.0\n",
      "episode number:  658\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -65.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 659 reward -65.00, Last 30ep Avg. rewards -65.00.\n",
      "episode number:  659\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.30739062-0.77017335j -0.55570772+0.05943865j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -65.0\n",
      "episode number:  659\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -65.0\n",
      "episode number:  659\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -64.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 660 reward -64.00, Last 30ep Avg. rewards -64.00.\n",
      "episode number:  660\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.1577113 -0.4037005j  -0.74057632+0.51351705j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -64.0\n",
      "episode number:  660\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -63.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]]\n",
      "Episode 661 reward -63.00, Last 30ep Avg. rewards -63.00.\n",
      "episode number:  661\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.44782191-0.23099809j 0.76149518-0.40770149j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.76149518-0.40770149j 0.44782191-0.23099809j]\n",
      "RESULT: False\n",
      "reward till now:  -63.0\n",
      "episode number:  661\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.76149518-0.40770149j 0.44782191-0.23099809j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.85511632-0.45162881j 0.2218005 -0.12494817j]\n",
      "RESULT: False\n",
      "reward till now:  -63.0\n",
      "episode number:  661\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.85511632-0.45162881j 0.2218005 -0.12494817j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.76149518-0.40770149j 0.44782191-0.23099809j]\n",
      "RESULT: False\n",
      "reward till now:  -63.0\n",
      "episode number:  661\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.76149518-0.40770149j 0.44782191-0.23099809j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.85511632-0.45162881j 0.2218005 -0.12494817j]\n",
      "RESULT: False\n",
      "reward till now:  -63.0\n",
      "episode number:  661\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.85511632-0.45162881j 0.2218005 -0.12494817j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.76149518-0.40770149j 0.44782191-0.23099809j]\n",
      "RESULT: False\n",
      "reward till now:  -64.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 662 reward -64.00, Last 30ep Avg. rewards -64.00.\n",
      "episode number:  662\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.07344923+0.48214318j  0.60384299-0.63048934j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [ 0.60384299-0.63048934j -0.07344923+0.48214318j]\n",
      "RESULT: False\n",
      "reward till now:  -64.0\n",
      "episode number:  662\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.60384299-0.63048934j -0.07344923+0.48214318j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.37504503-0.10489658j 0.47891792-0.78675j   ]\n",
      "RESULT: False\n",
      "reward till now:  -64.0\n",
      "episode number:  662\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.37504503-0.10489658j 0.47891792-0.78675j   ]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.47891792-0.78675j    0.37504503-0.10489658j]\n",
      "RESULT: False\n",
      "reward till now:  -64.0\n",
      "episode number:  662\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.47891792-0.78675j    0.37504503-0.10489658j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.37504503-0.10489658j 0.47891792-0.78675j   ]\n",
      "RESULT: False\n",
      "reward till now:  -64.0\n",
      "episode number:  662\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.37504503-0.10489658j 0.47891792-0.78675j   ]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.47891792-0.78675j    0.37504503-0.10489658j]\n",
      "RESULT: False\n",
      "reward till now:  -65.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 663 reward -65.00, Last 30ep Avg. rewards -65.00.\n",
      "episode number:  663\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.56174432+0.54591244j  0.29066686+0.54948676j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -65.0\n",
      "episode number:  663\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -65.0\n",
      "episode number:  663\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -64.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 664 reward -64.00, Last 30ep Avg. rewards -64.00.\n",
      "episode number:  664\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.11976715-0.19332223j -0.97010211+0.08475991j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -64.0\n",
      "episode number:  664\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -64.0\n",
      "episode number:  664\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -63.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 665 reward -63.00, Last 30ep Avg. rewards -63.00.\n",
      "episode number:  665\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.86237392-0.1074784j  -0.36492822-0.33404643j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.35174723-0.3122052j  0.86783366+0.16020779j]\n",
      "RESULT: False\n",
      "reward till now:  -63.0\n",
      "episode number:  665\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.35174723-0.3122052j  0.86783366+0.16020779j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.86783366+0.16020779j 0.35174723-0.3122052j ]\n",
      "RESULT: False\n",
      "reward till now:  -63.0\n",
      "episode number:  665\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.86783366+0.16020779j 0.35174723-0.3122052j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.86237392-0.1074784j  0.36492822+0.33404643j]\n",
      "RESULT: False\n",
      "reward till now:  -63.0\n",
      "episode number:  665\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.86237392-0.1074784j  0.36492822+0.33404643j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.86783366+0.16020779j 0.35174723-0.3122052j ]\n",
      "RESULT: False\n",
      "reward till now:  -63.0\n",
      "episode number:  665\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.86783366+0.16020779j 0.35174723-0.3122052j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.86237392-0.1074784j  0.36492822+0.33404643j]\n",
      "RESULT: False\n",
      "reward till now:  -64.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 666 reward -64.00, Last 30ep Avg. rewards -64.00.\n",
      "episode number:  666\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.75183125-5.35107861e-01j  0.38523909+4.36685091e-04j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -64.0\n",
      "episode number:  666\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -63.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 667 reward -63.00, Last 30ep Avg. rewards -63.00.\n",
      "episode number:  667\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.27806125-0.77028693j -0.57385856+0.00513171j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -63.0\n",
      "episode number:  667\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -62.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 668 reward -62.00, Last 30ep Avg. rewards -62.00.\n",
      "episode number:  668\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.02563293+0.30846276j  0.95060933-0.02314239j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [ 0.95060933-0.02314239j -0.02563293+0.30846276j]\n",
      "RESULT: False\n",
      "reward till now:  -62.0\n",
      "episode number:  668\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.95060933-0.02314239j -0.02563293+0.30846276j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.65405709+0.20175197j 0.69030753-0.23448025j]\n",
      "RESULT: False\n",
      "reward till now:  -62.0\n",
      "episode number:  668\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.65405709+0.20175197j 0.69030753-0.23448025j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.95060933-0.02314239j -0.02563293+0.30846276j]\n",
      "RESULT: False\n",
      "reward till now:  -62.0\n",
      "episode number:  668\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.95060933-0.02314239j -0.02563293+0.30846276j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.65405709+0.20175197j 0.69030753-0.23448025j]\n",
      "RESULT: False\n",
      "reward till now:  -62.0\n",
      "episode number:  668\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.65405709+0.20175197j 0.69030753-0.23448025j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.95060933-0.02314239j -0.02563293+0.30846276j]\n",
      "RESULT: False\n",
      "reward till now:  -63.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 669 reward -63.00, Last 30ep Avg. rewards -63.00.\n",
      "episode number:  669\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.34565983-0.47858149j 0.48320005+0.64652668j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.48320005+0.64652668j 0.34565983-0.47858149j]\n",
      "RESULT: False\n",
      "reward till now:  -63.0\n",
      "episode number:  669\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.48320005+0.64652668j 0.34565983-0.47858149j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.34565983-0.47858149j 0.48320005+0.64652668j]\n",
      "RESULT: False\n",
      "reward till now:  -63.0\n",
      "episode number:  669\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.34565983-0.47858149j 0.48320005+0.64652668j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.48320005+0.64652668j 0.34565983-0.47858149j]\n",
      "RESULT: False\n",
      "reward till now:  -63.0\n",
      "episode number:  669\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.48320005+0.64652668j 0.34565983-0.47858149j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.34565983-0.47858149j 0.48320005+0.64652668j]\n",
      "RESULT: False\n",
      "reward till now:  -63.0\n",
      "episode number:  669\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.34565983-0.47858149j 0.48320005+0.64652668j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.48320005+0.64652668j 0.34565983-0.47858149j]\n",
      "RESULT: False\n",
      "reward till now:  -64.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 670 reward -64.00, Last 30ep Avg. rewards -64.00.\n",
      "episode number:  670\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.01757604+0.64022622j  0.45901743+0.6157146j ]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [ 0.45901743+0.6157146j  -0.01757604+0.64022622j]\n",
      "RESULT: False\n",
      "reward till now:  -64.0\n",
      "episode number:  670\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.45901743+0.6157146j  -0.01757604+0.64022622j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.3121462 +0.88808427j 0.33700247-0.01733234j]\n",
      "RESULT: False\n",
      "reward till now:  -64.0\n",
      "episode number:  670\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.3121462 +0.88808427j 0.33700247-0.01733234j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.33700247-0.01733234j 0.3121462 +0.88808427j]\n",
      "RESULT: False\n",
      "reward till now:  -64.0\n",
      "episode number:  670\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.33700247-0.01733234j 0.3121462 +0.88808427j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.3121462 +0.88808427j 0.33700247-0.01733234j]\n",
      "RESULT: False\n",
      "reward till now:  -64.0\n",
      "episode number:  670\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.3121462 +0.88808427j 0.33700247-0.01733234j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.33700247-0.01733234j 0.3121462 +0.88808427j]\n",
      "RESULT: False\n",
      "reward till now:  -65.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 671 reward -65.00, Last 30ep Avg. rewards -65.00.\n",
      "episode number:  671\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.79715203+0.1417986j  -0.41508903-0.41490107j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.27015934-0.19311261j 0.85718387+0.39364611j]\n",
      "RESULT: False\n",
      "reward till now:  -65.0\n",
      "episode number:  671\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.27015934-0.19311261j 0.85718387+0.39364611j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.85718387+0.39364611j 0.27015934-0.19311261j]\n",
      "RESULT: False\n",
      "reward till now:  -65.0\n",
      "episode number:  671\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.85718387+0.39364611j 0.27015934-0.19311261j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.79715203+0.1417986j  0.41508903+0.41490107j]\n",
      "RESULT: False\n",
      "reward till now:  -65.0\n",
      "episode number:  671\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.79715203+0.1417986j  0.41508903+0.41490107j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.85718387+0.39364611j 0.27015934-0.19311261j]\n",
      "RESULT: False\n",
      "reward till now:  -65.0\n",
      "episode number:  671\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.85718387+0.39364611j 0.27015934-0.19311261j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.79715203+0.1417986j  0.41508903+0.41490107j]\n",
      "RESULT: False\n",
      "reward till now:  -66.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 672 reward -66.00, Last 30ep Avg. rewards -66.00.\n",
      "episode number:  672\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.27021069-0.60143987j -0.66979235-0.3415179j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -66.0\n",
      "episode number:  672\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -66.0\n",
      "episode number:  672\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -66.0\n",
      "episode number:  672\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -66.0\n",
      "episode number:  672\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -67.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 673 reward -67.00, Last 30ep Avg. rewards -67.00.\n",
      "episode number:  673\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.5321482 +0.35551643j 0.54554884-0.54111258j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.54554884-0.54111258j 0.5321482 +0.35551643j]\n",
      "RESULT: False\n",
      "reward till now:  -67.0\n",
      "episode number:  673\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.54554884-0.54111258j 0.5321482 +0.35551643j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.5321482 +0.35551643j 0.54554884-0.54111258j]\n",
      "RESULT: False\n",
      "reward till now:  -67.0\n",
      "episode number:  673\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.5321482 +0.35551643j 0.54554884-0.54111258j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.54554884-0.54111258j 0.5321482 +0.35551643j]\n",
      "RESULT: False\n",
      "reward till now:  -67.0\n",
      "episode number:  673\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.54554884-0.54111258j 0.5321482 +0.35551643j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.5321482 +0.35551643j 0.54554884-0.54111258j]\n",
      "RESULT: False\n",
      "reward till now:  -67.0\n",
      "episode number:  673\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.5321482 +0.35551643j 0.54554884-0.54111258j]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.54554884-0.54111258j 0.5321482 +0.35551643j]\n",
      "RESULT: False\n",
      "reward till now:  -68.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 674 reward -68.00, Last 30ep Avg. rewards -68.00.\n",
      "episode number:  674\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.551047  +0.10968865j 0.26470761-0.78373815j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.57682561-0.47662497j 0.20247252+0.63174815j]\n",
      "RESULT: False\n",
      "reward till now:  -68.0\n",
      "episode number:  674\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.57682561-0.47662497j 0.20247252+0.63174815j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.551047  +0.10968865j 0.26470761-0.78373815j]\n",
      "RESULT: False\n",
      "reward till now:  -68.0\n",
      "episode number:  674\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.551047  +0.10968865j 0.26470761-0.78373815j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.57682561-0.47662497j 0.20247252+0.63174815j]\n",
      "RESULT: False\n",
      "reward till now:  -68.0\n",
      "episode number:  674\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.57682561-0.47662497j 0.20247252+0.63174815j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.551047  +0.10968865j 0.26470761-0.78373815j]\n",
      "RESULT: False\n",
      "reward till now:  -68.0\n",
      "episode number:  674\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.551047  +0.10968865j 0.26470761-0.78373815j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.57682561-0.47662497j 0.20247252+0.63174815j]\n",
      "RESULT: False\n",
      "reward till now:  -69.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 675 reward -69.00, Last 30ep Avg. rewards -69.00.\n",
      "episode number:  675\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.0592623 +0.72475948j -0.00652579-0.68641758j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -69.0\n",
      "episode number:  675\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -68.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 676 reward -68.00, Last 30ep Avg. rewards -68.00.\n",
      "episode number:  676\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.61099472-0.1979776j  -0.1070942 +0.75896058j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.35631147+0.39667487j 0.50776554-0.67665747j]\n",
      "RESULT: False\n",
      "reward till now:  -68.0\n",
      "episode number:  676\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.35631147+0.39667487j 0.50776554-0.67665747j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.50776554-0.67665747j 0.35631147+0.39667487j]\n",
      "RESULT: False\n",
      "reward till now:  -68.0\n",
      "episode number:  676\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.50776554-0.67665747j 0.35631147+0.39667487j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.35631147+0.39667487j 0.50776554-0.67665747j]\n",
      "RESULT: False\n",
      "reward till now:  -68.0\n",
      "episode number:  676\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.35631147+0.39667487j 0.50776554-0.67665747j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.50776554-0.67665747j 0.35631147+0.39667487j]\n",
      "RESULT: False\n",
      "reward till now:  -68.0\n",
      "episode number:  676\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.50776554-0.67665747j 0.35631147+0.39667487j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.35631147+0.39667487j 0.50776554-0.67665747j]\n",
      "RESULT: False\n",
      "reward till now:  -69.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 677 reward -69.00, Last 30ep Avg. rewards -69.00.\n",
      "episode number:  677\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.94252116+0.14763688j  0.02379531+0.29881598j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -69.0\n",
      "episode number:  677\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -69.0\n",
      "episode number:  677\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -69.0\n",
      "episode number:  677\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -69.0\n",
      "episode number:  677\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -70.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 678 reward -70.00, Last 30ep Avg. rewards -70.00.\n",
      "episode number:  678\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.41894136+0.90459587j  0.07827268-0.00823683j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -70.0\n",
      "episode number:  678\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -69.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 679 reward -69.00, Last 30ep Avg. rewards -69.00.\n",
      "episode number:  679\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.64396552+0.62092071j 0.3475826 +0.28098436j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.7011304 +0.63774319j 0.20957438+0.2403713j ]\n",
      "RESULT: False\n",
      "reward till now:  -69.0\n",
      "episode number:  679\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.7011304 +0.63774319j 0.20957438+0.2403713j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.64396552+0.62092071j 0.3475826 +0.28098436j]\n",
      "RESULT: False\n",
      "reward till now:  -69.0\n",
      "episode number:  679\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.64396552+0.62092071j 0.3475826 +0.28098436j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.7011304 +0.63774319j 0.20957438+0.2403713j ]\n",
      "RESULT: False\n",
      "reward till now:  -69.0\n",
      "episode number:  679\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.7011304 +0.63774319j 0.20957438+0.2403713j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.64396552+0.62092071j 0.3475826 +0.28098436j]\n",
      "RESULT: False\n",
      "reward till now:  -69.0\n",
      "episode number:  679\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.64396552+0.62092071j 0.3475826 +0.28098436j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.7011304 +0.63774319j 0.20957438+0.2403713j ]\n",
      "RESULT: False\n",
      "reward till now:  -70.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 680 reward -70.00, Last 30ep Avg. rewards -70.00.\n",
      "episode number:  680\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.46327864-0.09095246j  0.87638542+0.09512698j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [ 0.87638542+0.09512698j -0.46327864-0.09095246j]\n",
      "RESULT: False\n",
      "reward till now:  -70.0\n",
      "episode number:  680\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.87638542+0.09512698j -0.46327864-0.09095246j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.29211061+0.00295183j 0.94728554+0.13157803j]\n",
      "RESULT: False\n",
      "reward till now:  -70.0\n",
      "episode number:  680\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.29211061+0.00295183j 0.94728554+0.13157803j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.87638542+0.09512698j -0.46327864-0.09095246j]\n",
      "RESULT: False\n",
      "reward till now:  -70.0\n",
      "episode number:  680\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.87638542+0.09512698j -0.46327864-0.09095246j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.29211061+0.00295183j 0.94728554+0.13157803j]\n",
      "RESULT: False\n",
      "reward till now:  -70.0\n",
      "episode number:  680\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.29211061+0.00295183j 0.94728554+0.13157803j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.87638542+0.09512698j -0.46327864-0.09095246j]\n",
      "RESULT: False\n",
      "reward till now:  -71.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 681 reward -71.00, Last 30ep Avg. rewards -71.00.\n",
      "episode number:  681\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.36700512-0.76175913j -0.25276745+0.47025407j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -71.0\n",
      "episode number:  681\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -71.0\n",
      "episode number:  681\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -71.0\n",
      "episode number:  681\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -71.0\n",
      "episode number:  681\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -72.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 682 reward -72.00, Last 30ep Avg. rewards -72.00.\n",
      "episode number:  682\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.96636768+0.03426468j -0.25249711-0.03470801j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.50478273-0.00031348j 0.86186756+0.04877105j]\n",
      "RESULT: False\n",
      "reward till now:  -72.0\n",
      "episode number:  682\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.50478273-0.00031348j 0.86186756+0.04877105j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.96636768+0.03426468j -0.25249711-0.03470801j]\n",
      "RESULT: False\n",
      "reward till now:  -72.0\n",
      "episode number:  682\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.96636768+0.03426468j -0.25249711-0.03470801j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.50478273-0.00031348j 0.86186756+0.04877105j]\n",
      "RESULT: False\n",
      "reward till now:  -72.0\n",
      "episode number:  682\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.50478273-0.00031348j 0.86186756+0.04877105j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.96636768+0.03426468j -0.25249711-0.03470801j]\n",
      "RESULT: False\n",
      "reward till now:  -72.0\n",
      "episode number:  682\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.96636768+0.03426468j -0.25249711-0.03470801j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.50478273-0.00031348j 0.86186756+0.04877105j]\n",
      "RESULT: False\n",
      "reward till now:  -73.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 683 reward -73.00, Last 30ep Avg. rewards -73.00.\n",
      "episode number:  683\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.67806814-0.01825463j -0.52888276-0.51007195j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.52888276-0.51007195j  0.67806814-0.01825463j]\n",
      "RESULT: False\n",
      "reward till now:  -73.0\n",
      "episode number:  683\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.52888276-0.51007195j  0.67806814-0.01825463j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -73.0\n",
      "episode number:  683\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -73.0\n",
      "episode number:  683\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -73.0\n",
      "episode number:  683\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 684 reward -74.00, Last 30ep Avg. rewards -74.00.\n",
      "episode number:  684\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.77338544-0.02779787j  0.05516015-0.63091964j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  684\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -73.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 685 reward -73.00, Last 30ep Avg. rewards -73.00.\n",
      "episode number:  685\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.04206757-0.22082275j -0.68333275-0.69463946j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -73.0\n",
      "episode number:  685\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -73.0\n",
      "episode number:  685\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -73.0\n",
      "episode number:  685\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -73.0\n",
      "episode number:  685\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 686 reward -74.00, Last 30ep Avg. rewards -74.00.\n",
      "episode number:  686\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.24252265-0.27401481j -0.31256728-0.87658447j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  686\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  686\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  686\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  686\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 687 reward -75.00, Last 30ep Avg. rewards -75.00.\n",
      "episode number:  687\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.47701703+0.09867433j  0.84938686+0.20312579j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [ 0.84938686+0.20312579j -0.47701703+0.09867433j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  687\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.84938686+0.20312579j -0.47701703+0.09867433j]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.26330524+0.21340491j 0.93790919+0.07385833j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  687\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.26330524+0.21340491j 0.93790919+0.07385833j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.84938686+0.20312579j -0.47701703+0.09867433j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  687\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.84938686+0.20312579j -0.47701703+0.09867433j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.26330524+0.21340491j 0.93790919+0.07385833j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  687\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.26330524+0.21340491j 0.93790919+0.07385833j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.84938686+0.20312579j -0.47701703+0.09867433j]\n",
      "RESULT: False\n",
      "reward till now:  -76.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 688 reward -76.00, Last 30ep Avg. rewards -76.00.\n",
      "episode number:  688\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.0643406 -0.96412916j 0.25011567+0.06129767j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -76.0\n",
      "episode number:  688\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -76.0\n",
      "episode number:  688\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -76.0\n",
      "episode number:  688\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -76.0\n",
      "episode number:  688\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -77.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 689 reward -77.00, Last 30ep Avg. rewards -77.00.\n",
      "episode number:  689\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.46228855+0.19681215j  0.70429436-0.50152141j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -77.0\n",
      "episode number:  689\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -77.0\n",
      "episode number:  689\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -77.0\n",
      "episode number:  689\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -77.0\n",
      "episode number:  689\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -78.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 690 reward -78.00, Last 30ep Avg. rewards -78.00.\n",
      "episode number:  690\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.62477872+0.10725654j  0.76402604-0.12004912j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -78.0\n",
      "episode number:  690\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -77.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]]\n",
      "Episode 691 reward -77.00, Last 30ep Avg. rewards -77.00.\n",
      "episode number:  691\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.10670231-0.73548839j -0.58071323+0.33233054j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -77.0\n",
      "episode number:  691\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -77.0\n",
      "episode number:  691\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -76.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 692 reward -76.00, Last 30ep Avg. rewards -76.00.\n",
      "episode number:  692\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.67511609+0.09222782j  0.61971273-0.38944631j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -76.0\n",
      "episode number:  692\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -75.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 693 reward -75.00, Last 30ep Avg. rewards -75.00.\n",
      "episode number:  693\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.66545434-0.54544411j -0.50241544-0.08508804j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.50241544-0.08508804j  0.66545434-0.54544411j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  693\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.50241544-0.08508804j  0.66545434-0.54544411j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  693\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  693\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -74.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 694 reward -74.00, Last 30ep Avg. rewards -74.00.\n",
      "episode number:  694\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.53511115-0.82321819j 0.14917106-0.11711473j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.48386059-0.66491578j 0.27290085-0.49929055j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  694\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.48386059-0.66491578j 0.27290085-0.49929055j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.27290085-0.49929055j 0.48386059-0.66491578j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  694\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.27290085-0.49929055j 0.48386059-0.66491578j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.48386059-0.66491578j 0.27290085-0.49929055j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  694\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.48386059-0.66491578j 0.27290085-0.49929055j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.27290085-0.49929055j 0.48386059-0.66491578j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  694\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.27290085-0.49929055j 0.48386059-0.66491578j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.48386059-0.66491578j 0.27290085-0.49929055j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 695 reward -75.00, Last 30ep Avg. rewards -75.00.\n",
      "episode number:  695\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.69511938-0.47186135j  0.50212084-0.2050136j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  695\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  695\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -74.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 696 reward -74.00, Last 30ep Avg. rewards -74.00.\n",
      "episode number:  696\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.06552352+0.19899369j 0.89234748-0.39978013j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.89234748-0.39978013j 0.06552352+0.19899369j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  696\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.89234748-0.39978013j 0.06552352+0.19899369j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.67731708-0.14197745j 0.58465283-0.42339703j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  696\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.67731708-0.14197745j 0.58465283-0.42339703j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.89234748-0.39978013j 0.06552352+0.19899369j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  696\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.89234748-0.39978013j 0.06552352+0.19899369j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.67731708-0.14197745j 0.58465283-0.42339703j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  696\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.67731708-0.14197745j 0.58465283-0.42339703j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.89234748-0.39978013j 0.06552352+0.19899369j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 697 reward -75.00, Last 30ep Avg. rewards -75.00.\n",
      "episode number:  697\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.58046186-0.05976437j -0.62584357+0.51750562j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  697\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  697\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -74.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 698 reward -74.00, Last 30ep Avg. rewards -74.00.\n",
      "episode number:  698\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.15043179+0.29687316j  0.80922959-0.4841323j ]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [ 0.80922959-0.4841323j  -0.15043179+0.29687316j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  698\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.80922959-0.4841323j  -0.15043179+0.29687316j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.46584039-0.1324122j  0.67858307-0.55225426j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  698\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.46584039-0.1324122j  0.67858307-0.55225426j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.67858307-0.55225426j 0.46584039-0.1324122j ]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  698\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.67858307-0.55225426j 0.46584039-0.1324122j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.80922959-0.4841323j  0.15043179-0.29687316j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  698\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.80922959-0.4841323j  0.15043179-0.29687316j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.67858307-0.55225426j 0.46584039-0.1324122j ]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 699 reward -75.00, Last 30ep Avg. rewards -75.00.\n",
      "episode number:  699\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.07153795-0.58945503j  0.59973271-0.5364194j ]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [ 0.59973271-0.5364194j  -0.07153795-0.58945503j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  699\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.59973271-0.5364194j  -0.07153795-0.58945503j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.3734901 -0.79611344j 0.47466003+0.03750185j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  699\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.3734901 -0.79611344j 0.47466003+0.03750185j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.47466003+0.03750185j 0.3734901 -0.79611344j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  699\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.47466003+0.03750185j 0.3734901 -0.79611344j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.3734901 -0.79611344j 0.47466003+0.03750185j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  699\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.3734901 -0.79611344j 0.47466003+0.03750185j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.47466003+0.03750185j 0.3734901 -0.79611344j]\n",
      "RESULT: False\n",
      "reward till now:  -76.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 700 reward -76.00, Last 30ep Avg. rewards -76.00.\n",
      "episode number:  700\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.62720983-0.14459795j -0.60977012-0.46247125j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -76.0\n",
      "episode number:  700\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -75.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]]\n",
      "Episode 701 reward -75.00, Last 30ep Avg. rewards -75.00.\n",
      "episode number:  701\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.74956505-0.29785529j -0.16194325-0.56851459j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -75.0\n",
      "episode number:  701\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -74.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]\n",
      " [0.95]\n",
      " [1.  ]]\n",
      "Episode 702 reward -74.00, Last 30ep Avg. rewards -74.00.\n",
      "episode number:  702\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.38845213-0.43481893j  0.28117282+0.76221997j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  702\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -73.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]\n",
      " [0.95]\n",
      " [1.  ]\n",
      " [0.95]\n",
      " [1.  ]]\n",
      "Episode 703 reward -73.00, Last 30ep Avg. rewards -73.00.\n",
      "episode number:  703\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.123595  +0.78827203j 0.54427106+0.25907623j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.54427106+0.25907623j 0.123595  +0.78827203j]\n",
      "RESULT: False\n",
      "reward till now:  -73.0\n",
      "episode number:  703\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.54427106+0.25907623j 0.123595  +0.78827203j]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.47225262+0.74058706j 0.29746289-0.37419794j]\n",
      "RESULT: False\n",
      "reward till now:  -73.0\n",
      "episode number:  703\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.47225262+0.74058706j 0.29746289-0.37419794j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.29746289-0.37419794j 0.47225262+0.74058706j]\n",
      "RESULT: False\n",
      "reward till now:  -73.0\n",
      "episode number:  703\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.29746289-0.37419794j 0.47225262+0.74058706j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.47225262+0.74058706j 0.29746289-0.37419794j]\n",
      "RESULT: False\n",
      "reward till now:  -73.0\n",
      "episode number:  703\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.47225262+0.74058706j 0.29746289-0.37419794j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.29746289-0.37419794j 0.47225262+0.74058706j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 704 reward -74.00, Last 30ep Avg. rewards -74.00.\n",
      "episode number:  704\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.65854372-0.08825708j -0.58501835+0.46506385j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -74.0\n",
      "episode number:  704\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -73.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 705 reward -73.00, Last 30ep Avg. rewards -73.00.\n",
      "episode number:  705\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.47871868+0.0277542j -0.73146401-0.4847871j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -73.0\n",
      "episode number:  705\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -72.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 706 reward -72.00, Last 30ep Avg. rewards -72.00.\n",
      "episode number:  706\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.6706174 +0.38927003j -0.23083879-0.58775386j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -72.0\n",
      "episode number:  706\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -71.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 707 reward -71.00, Last 30ep Avg. rewards -71.00.\n",
      "episode number:  707\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.93841104+0.28947475j -0.18658788+0.02782194j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.53161925+0.22436264j 0.79549437+0.18501647j]\n",
      "RESULT: False\n",
      "reward till now:  -71.0\n",
      "episode number:  707\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.53161925+0.22436264j 0.79549437+0.18501647j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.79549437+0.18501647j 0.53161925+0.22436264j]\n",
      "RESULT: False\n",
      "reward till now:  -71.0\n",
      "episode number:  707\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.79549437+0.18501647j 0.53161925+0.22436264j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.93841104+0.28947475j 0.18658788-0.02782194j]\n",
      "RESULT: False\n",
      "reward till now:  -71.0\n",
      "episode number:  707\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.93841104+0.28947475j 0.18658788-0.02782194j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.79549437+0.18501647j 0.53161925+0.22436264j]\n",
      "RESULT: False\n",
      "reward till now:  -71.0\n",
      "episode number:  707\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.79549437+0.18501647j 0.53161925+0.22436264j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.93841104+0.28947475j 0.18658788-0.02782194j]\n",
      "RESULT: False\n",
      "reward till now:  -72.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 708 reward -72.00, Last 30ep Avg. rewards -72.00.\n",
      "episode number:  708\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.53509915+0.17490537j  0.30373025+0.76865138j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -72.0\n",
      "episode number:  708\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -71.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 709 reward -71.00, Last 30ep Avg. rewards -71.00.\n",
      "episode number:  709\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.22832711+0.48176738j -0.70860325-0.46222111j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -71.0\n",
      "episode number:  709\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -71.0\n",
      "episode number:  709\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -70.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 710 reward -70.00, Last 30ep Avg. rewards -70.00.\n",
      "episode number:  710\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.22662049-0.2355342j   0.87955416-0.34576187j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [ 0.87955416-0.34576187j -0.22662049-0.2355342j ]\n",
      "RESULT: False\n",
      "reward till now:  -70.0\n",
      "episode number:  710\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.87955416-0.34576187j -0.22662049-0.2355342j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.46169382-0.41103839j 0.7821836 -0.07794273j]\n",
      "RESULT: False\n",
      "reward till now:  -70.0\n",
      "episode number:  710\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.46169382-0.41103839j 0.7821836 -0.07794273j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.7821836 -0.07794273j 0.46169382-0.41103839j]\n",
      "RESULT: False\n",
      "reward till now:  -70.0\n",
      "episode number:  710\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.7821836 -0.07794273j 0.46169382-0.41103839j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.87955416-0.34576187j 0.22662049+0.2355342j ]\n",
      "RESULT: False\n",
      "reward till now:  -70.0\n",
      "episode number:  710\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.87955416-0.34576187j 0.22662049+0.2355342j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.7821836 -0.07794273j 0.46169382-0.41103839j]\n",
      "RESULT: False\n",
      "reward till now:  -71.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 711 reward -71.00, Last 30ep Avg. rewards -71.00.\n",
      "episode number:  711\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.46814375+0.41913788j -0.12290566+0.76815302j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.12290566+0.76815302j  0.46814375+0.41913788j]\n",
      "RESULT: False\n",
      "reward till now:  -71.0\n",
      "episode number:  711\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.12290566+0.76815302j  0.46814375+0.41913788j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -71.0\n",
      "episode number:  711\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -71.0\n",
      "episode number:  711\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -70.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 712 reward -70.00, Last 30ep Avg. rewards -70.00.\n",
      "episode number:  712\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.21774372+0.4853138j  0.27913851+0.79946225j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -70.0\n",
      "episode number:  712\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -69.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 713 reward -69.00, Last 30ep Avg. rewards -69.00.\n",
      "episode number:  713\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.21942516-0.52624355j 0.82140158-0.01482494j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.82140158-0.01482494j 0.21942516-0.52624355j]\n",
      "RESULT: False\n",
      "reward till now:  -69.0\n",
      "episode number:  713\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.82140158-0.01482494j 0.21942516-0.52624355j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.73597564-0.3825932j  0.42566161+0.36162757j]\n",
      "RESULT: False\n",
      "reward till now:  -69.0\n",
      "episode number:  713\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.73597564-0.3825932j  0.42566161+0.36162757j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.82140158-0.01482494j 0.21942516-0.52624355j]\n",
      "RESULT: False\n",
      "reward till now:  -69.0\n",
      "episode number:  713\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.82140158-0.01482494j 0.21942516-0.52624355j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.73597564-0.3825932j  0.42566161+0.36162757j]\n",
      "RESULT: False\n",
      "reward till now:  -69.0\n",
      "episode number:  713\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.73597564-0.3825932j  0.42566161+0.36162757j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.82140158-0.01482494j 0.21942516-0.52624355j]\n",
      "RESULT: False\n",
      "reward till now:  -70.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 714 reward -70.00, Last 30ep Avg. rewards -70.00.\n",
      "episode number:  714\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.60339761-0.19809491j 0.43065994-0.64125015j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.7311891 -0.59350659j 0.12214398+0.31335807j]\n",
      "RESULT: False\n",
      "reward till now:  -70.0\n",
      "episode number:  714\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.7311891 -0.59350659j 0.12214398+0.31335807j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.60339761-0.19809491j 0.43065994-0.64125015j]\n",
      "RESULT: False\n",
      "reward till now:  -70.0\n",
      "episode number:  714\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.60339761-0.19809491j 0.43065994-0.64125015j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.7311891 -0.59350659j 0.12214398+0.31335807j]\n",
      "RESULT: False\n",
      "reward till now:  -70.0\n",
      "episode number:  714\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.7311891 -0.59350659j 0.12214398+0.31335807j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.60339761-0.19809491j 0.43065994-0.64125015j]\n",
      "RESULT: False\n",
      "reward till now:  -70.0\n",
      "episode number:  714\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.60339761-0.19809491j 0.43065994-0.64125015j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.7311891 -0.59350659j 0.12214398+0.31335807j]\n",
      "RESULT: False\n",
      "reward till now:  -71.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 715 reward -71.00, Last 30ep Avg. rewards -71.00.\n",
      "episode number:  715\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.26588132-0.14599728j  0.35450399+0.8844879j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -71.0\n",
      "episode number:  715\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -71.0\n",
      "episode number:  715\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -70.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 716 reward -70.00, Last 30ep Avg. rewards -70.00.\n",
      "episode number:  716\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.05046697+0.58337251j -0.48744235-0.64771101j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -70.0\n",
      "episode number:  716\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -69.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 717 reward -69.00, Last 30ep Avg. rewards -69.00.\n",
      "episode number:  717\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.37401958-0.49081238j 0.67147941-0.41027793j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.67147941-0.41027793j 0.37401958-0.49081238j]\n",
      "RESULT: False\n",
      "reward till now:  -69.0\n",
      "episode number:  717\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.67147941-0.41027793j 0.37401958-0.49081238j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.73927942-0.63716706j 0.21033586+0.05694646j]\n",
      "RESULT: False\n",
      "reward till now:  -69.0\n",
      "episode number:  717\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.73927942-0.63716706j 0.21033586+0.05694646j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.67147941-0.41027793j 0.37401958-0.49081238j]\n",
      "RESULT: False\n",
      "reward till now:  -69.0\n",
      "episode number:  717\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.67147941-0.41027793j 0.37401958-0.49081238j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.73927942-0.63716706j 0.21033586+0.05694646j]\n",
      "RESULT: False\n",
      "reward till now:  -69.0\n",
      "episode number:  717\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.73927942-0.63716706j 0.21033586+0.05694646j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.67147941-0.41027793j 0.37401958-0.49081238j]\n",
      "RESULT: False\n",
      "reward till now:  -70.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 718 reward -70.00, Last 30ep Avg. rewards -70.00.\n",
      "episode number:  718\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.68512357+0.11018577j  0.57624697+0.43174556j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -70.0\n",
      "episode number:  718\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -69.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 719 reward -69.00, Last 30ep Avg. rewards -69.00.\n",
      "episode number:  719\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.84385095-0.07841426j -0.34186159-0.40607564j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.35496008-0.34258609j 0.83842538+0.23169158j]\n",
      "RESULT: False\n",
      "reward till now:  -69.0\n",
      "episode number:  719\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.35496008-0.34258609j 0.83842538+0.23169158j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.83842538+0.23169158j 0.35496008-0.34258609j]\n",
      "RESULT: False\n",
      "reward till now:  -69.0\n",
      "episode number:  719\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.83842538+0.23169158j 0.35496008-0.34258609j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.84385095-0.07841426j 0.34186159+0.40607564j]\n",
      "RESULT: False\n",
      "reward till now:  -69.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode number:  719\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.84385095-0.07841426j 0.34186159+0.40607564j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.83842538+0.23169158j 0.35496008-0.34258609j]\n",
      "RESULT: False\n",
      "reward till now:  -69.0\n",
      "episode number:  719\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.83842538+0.23169158j 0.35496008-0.34258609j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.84385095-0.07841426j 0.34186159+0.40607564j]\n",
      "RESULT: False\n",
      "reward till now:  -70.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 720 reward -70.00, Last 30ep Avg. rewards -70.00.\n",
      "episode number:  720\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.28423372+0.44348626j  0.69086644+0.49521177j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -70.0\n",
      "episode number:  720\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -69.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]]\n",
      "Episode 721 reward -69.00, Last 30ep Avg. rewards -69.00.\n",
      "episode number:  721\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.23505786-0.82271453j 0.3614813 +0.370432j  ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -69.0\n",
      "episode number:  721\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -68.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]\n",
      " [0.95]\n",
      " [1.  ]]\n",
      "Episode 722 reward -68.00, Last 30ep Avg. rewards -68.00.\n",
      "episode number:  722\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.74915127+0.20406039j -0.62192391-0.10169749j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -68.0\n",
      "episode number:  722\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -67.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]\n",
      " [0.95]\n",
      " [1.  ]\n",
      " [0.95]\n",
      " [1.  ]]\n",
      "Episode 723 reward -67.00, Last 30ep Avg. rewards -67.00.\n",
      "episode number:  723\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.65295232-0.63330104j -0.4123765 +0.05028599j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -67.0\n",
      "episode number:  723\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -66.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]\n",
      " [0.95]\n",
      " [1.  ]\n",
      " [0.95]\n",
      " [1.  ]\n",
      " [0.95]\n",
      " [1.  ]]\n",
      "Episode 724 reward -66.00, Last 30ep Avg. rewards -66.00.\n",
      "episode number:  724\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.37561562+0.30482317j 0.72074019+0.49651719j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.72074019+0.49651719j 0.37561562+0.30482317j]\n",
      "RESULT: False\n",
      "reward till now:  -66.0\n",
      "episode number:  724\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.72074019+0.49651719j 0.37561562+0.30482317j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.77524063+0.5666332j  0.24403993+0.13554814j]\n",
      "RESULT: False\n",
      "reward till now:  -66.0\n",
      "episode number:  724\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.77524063+0.5666332j  0.24403993+0.13554814j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.72074019+0.49651719j 0.37561562+0.30482317j]\n",
      "RESULT: False\n",
      "reward till now:  -66.0\n",
      "episode number:  724\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.72074019+0.49651719j 0.37561562+0.30482317j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.77524063+0.5666332j  0.24403993+0.13554814j]\n",
      "RESULT: False\n",
      "reward till now:  -66.0\n",
      "episode number:  724\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.77524063+0.5666332j  0.24403993+0.13554814j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.72074019+0.49651719j 0.37561562+0.30482317j]\n",
      "RESULT: False\n",
      "reward till now:  -67.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 725 reward -67.00, Last 30ep Avg. rewards -67.00.\n",
      "episode number:  725\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.59083159+0.14740159j 0.10585362-0.78612074j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.49263083-0.45164265j 0.34293121+0.66009997j]\n",
      "RESULT: False\n",
      "reward till now:  -67.0\n",
      "episode number:  725\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.49263083-0.45164265j 0.34293121+0.66009997j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.34293121+0.66009997j 0.49263083-0.45164265j]\n",
      "RESULT: False\n",
      "reward till now:  -67.0\n",
      "episode number:  725\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.34293121+0.66009997j 0.49263083-0.45164265j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.49263083-0.45164265j 0.34293121+0.66009997j]\n",
      "RESULT: False\n",
      "reward till now:  -67.0\n",
      "episode number:  725\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.49263083-0.45164265j 0.34293121+0.66009997j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.34293121+0.66009997j 0.49263083-0.45164265j]\n",
      "RESULT: False\n",
      "reward till now:  -67.0\n",
      "episode number:  725\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.34293121+0.66009997j 0.49263083-0.45164265j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.49263083-0.45164265j 0.34293121+0.66009997j]\n",
      "RESULT: False\n",
      "reward till now:  -68.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 726 reward -68.00, Last 30ep Avg. rewards -68.00.\n",
      "episode number:  726\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.35576603-0.68086157j -0.02844528-0.63956932j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -68.0\n",
      "episode number:  726\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -68.0\n",
      "episode number:  726\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -67.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 727 reward -67.00, Last 30ep Avg. rewards -67.00.\n",
      "episode number:  727\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.08068037-0.61517678j -0.41032464-0.66834265j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -67.0\n",
      "episode number:  727\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -66.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 728 reward -66.00, Last 30ep Avg. rewards -66.00.\n",
      "episode number:  728\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.22046006-0.31386479j 0.80076049+0.46007489j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.80076049+0.46007489j 0.22046006-0.31386479j]\n",
      "RESULT: False\n",
      "reward till now:  -66.0\n",
      "episode number:  728\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.80076049+0.46007489j 0.22046006-0.31386479j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.72211198+0.10338615j 0.41033436+0.54725799j]\n",
      "RESULT: False\n",
      "reward till now:  -66.0\n",
      "episode number:  728\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.72211198+0.10338615j 0.41033436+0.54725799j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.80076049+0.46007489j 0.22046006-0.31386479j]\n",
      "RESULT: False\n",
      "reward till now:  -66.0\n",
      "episode number:  728\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.80076049+0.46007489j 0.22046006-0.31386479j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.72211198+0.10338615j 0.41033436+0.54725799j]\n",
      "RESULT: False\n",
      "reward till now:  -66.0\n",
      "episode number:  728\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.72211198+0.10338615j 0.41033436+0.54725799j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.80076049+0.46007489j 0.22046006-0.31386479j]\n",
      "RESULT: False\n",
      "reward till now:  -67.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 729 reward -67.00, Last 30ep Avg. rewards -67.00.\n",
      "episode number:  729\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.22019854-0.51998087j 0.28009728+0.77632339j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -67.0\n",
      "episode number:  729\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -67.0\n",
      "episode number:  729\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -66.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 730 reward -66.00, Last 30ep Avg. rewards -66.00.\n",
      "episode number:  730\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.56402918-0.73663676j 0.23723869+0.28801939j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.56658195-0.31722039j 0.23107577-0.72454131j]\n",
      "RESULT: False\n",
      "reward till now:  -66.0\n",
      "episode number:  730\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.56658195-0.31722039j 0.23107577-0.72454131j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.56402918-0.73663676j 0.23723869+0.28801939j]\n",
      "RESULT: False\n",
      "reward till now:  -66.0\n",
      "episode number:  730\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.56402918-0.73663676j 0.23723869+0.28801939j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.56658195-0.31722039j 0.23107577-0.72454131j]\n",
      "RESULT: False\n",
      "reward till now:  -66.0\n",
      "episode number:  730\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.56658195-0.31722039j 0.23107577-0.72454131j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.56402918-0.73663676j 0.23723869+0.28801939j]\n",
      "RESULT: False\n",
      "reward till now:  -66.0\n",
      "episode number:  730\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.56402918-0.73663676j 0.23723869+0.28801939j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.56658195-0.31722039j 0.23107577-0.72454131j]\n",
      "RESULT: False\n",
      "reward till now:  -67.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 731 reward -67.00, Last 30ep Avg. rewards -67.00.\n",
      "episode number:  731\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.70916241-0.29985642j  0.47686105+0.42400277j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -67.0\n",
      "episode number:  731\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -66.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 732 reward -66.00, Last 30ep Avg. rewards -66.00.\n",
      "episode number:  732\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.17268867+0.34636144j  0.29884578+0.87229787j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -66.0\n",
      "episode number:  732\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -66.0\n",
      "episode number:  732\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -65.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 733 reward -65.00, Last 30ep Avg. rewards -65.00.\n",
      "episode number:  733\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.02478388-0.9562313j   0.05266244-0.28676492j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -65.0\n",
      "episode number:  733\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -65.0\n",
      "episode number:  733\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -64.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 734 reward -64.00, Last 30ep Avg. rewards -64.00.\n",
      "episode number:  734\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.19292197-0.91516793j 0.13459657-0.32731106j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -64.0\n",
      "episode number:  734\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -63.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 735 reward -63.00, Last 30ep Avg. rewards -63.00.\n",
      "episode number:  735\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.38481764-0.79017865j  0.40177457+0.25711921j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -63.0\n",
      "episode number:  735\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -63.0\n",
      "episode number:  735\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -62.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 736 reward -62.00, Last 30ep Avg. rewards -62.00.\n",
      "episode number:  736\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.97915172-0.18946553j  0.04319072-0.05915477j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -62.0\n",
      "episode number:  736\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -61.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 737 reward -61.00, Last 30ep Avg. rewards -61.00.\n",
      "episode number:  737\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.54540369-0.74344107j -0.17112411-0.34719841j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -61.0\n",
      "episode number:  737\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -60.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 738 reward -60.00, Last 30ep Avg. rewards -60.00.\n",
      "episode number:  738\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.52600663+0.37917864j -0.0948665 +0.7553416j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -60.0\n",
      "episode number:  738\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -60.0\n",
      "episode number:  738\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -59.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 739 reward -59.00, Last 30ep Avg. rewards -59.00.\n",
      "episode number:  739\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.098729  +0.59089301j  0.64290607+0.47725237j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -59.0\n",
      "episode number:  739\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -59.0\n",
      "episode number:  739\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -58.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 740 reward -58.00, Last 30ep Avg. rewards -58.00.\n",
      "episode number:  740\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.75787905+0.60183812j -0.25175804-0.00530164j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.3578816 +0.42181499j 0.71392123+0.42931264j]\n",
      "RESULT: False\n",
      "reward till now:  -58.0\n",
      "episode number:  740\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.3578816 +0.42181499j 0.71392123+0.42931264j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -58.0\n",
      "episode number:  740\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -57.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 741 reward -57.00, Last 30ep Avg. rewards -57.00.\n",
      "episode number:  741\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.20628852-0.27460218j -0.72400975+0.59820446j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -57.0\n",
      "episode number:  741\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -57.0\n",
      "episode number:  741\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -56.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 742 reward -56.00, Last 30ep Avg. rewards -56.00.\n",
      "episode number:  742\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.03897875+0.44966832j -0.68978876+0.56610116j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -56.0\n",
      "episode number:  742\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -56.0\n",
      "episode number:  742\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -55.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 743 reward -55.00, Last 30ep Avg. rewards -55.00.\n",
      "episode number:  743\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.83763486+0.47189718j 0.26739095-0.06467592j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.78137124+0.28794891j 0.40322333+0.37941448j]\n",
      "RESULT: False\n",
      "reward till now:  -55.0\n",
      "episode number:  743\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.78137124+0.28794891j 0.40322333+0.37941448j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.83763486+0.47189718j 0.26739095-0.06467592j]\n",
      "RESULT: False\n",
      "reward till now:  -55.0\n",
      "episode number:  743\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.83763486+0.47189718j 0.26739095-0.06467592j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.78137124+0.28794891j 0.40322333+0.37941448j]\n",
      "RESULT: False\n",
      "reward till now:  -55.0\n",
      "episode number:  743\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.78137124+0.28794891j 0.40322333+0.37941448j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.83763486+0.47189718j 0.26739095-0.06467592j]\n",
      "RESULT: False\n",
      "reward till now:  -55.0\n",
      "episode number:  743\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.83763486+0.47189718j 0.26739095-0.06467592j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.78137124+0.28794891j 0.40322333+0.37941448j]\n",
      "RESULT: False\n",
      "reward till now:  -56.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 744 reward -56.00, Last 30ep Avg. rewards -56.00.\n",
      "episode number:  744\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.50922314-0.83986514j -0.18413029-0.03760806j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -56.0\n",
      "episode number:  744\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -55.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 745 reward -55.00, Last 30ep Avg. rewards -55.00.\n",
      "episode number:  745\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.67238343+0.5608876j  0.21759697+0.43122753j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.62931118+0.70153133j 0.32158259+0.09168352j]\n",
      "RESULT: False\n",
      "reward till now:  -55.0\n",
      "episode number:  745\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.62931118+0.70153133j 0.32158259+0.09168352j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.67238343+0.5608876j  0.21759697+0.43122753j]\n",
      "RESULT: False\n",
      "reward till now:  -55.0\n",
      "episode number:  745\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.67238343+0.5608876j  0.21759697+0.43122753j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.62931118+0.70153133j 0.32158259+0.09168352j]\n",
      "RESULT: False\n",
      "reward till now:  -55.0\n",
      "episode number:  745\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.62931118+0.70153133j 0.32158259+0.09168352j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.67238343+0.5608876j  0.21759697+0.43122753j]\n",
      "RESULT: False\n",
      "reward till now:  -55.0\n",
      "episode number:  745\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.67238343+0.5608876j  0.21759697+0.43122753j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.62931118+0.70153133j 0.32158259+0.09168352j]\n",
      "RESULT: False\n",
      "reward till now:  -56.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 746 reward -56.00, Last 30ep Avg. rewards -56.00.\n",
      "episode number:  746\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.74471349-0.46395208j 0.43952282+0.1922758j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.83738152-0.19210414j 0.21580239-0.46402318j]\n",
      "RESULT: False\n",
      "reward till now:  -56.0\n",
      "episode number:  746\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.83738152-0.19210414j 0.21580239-0.46402318j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.74471349-0.46395208j 0.43952282+0.1922758j ]\n",
      "RESULT: False\n",
      "reward till now:  -56.0\n",
      "episode number:  746\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.74471349-0.46395208j 0.43952282+0.1922758j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.83738152-0.19210414j 0.21580239-0.46402318j]\n",
      "RESULT: False\n",
      "reward till now:  -56.0\n",
      "episode number:  746\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.83738152-0.19210414j 0.21580239-0.46402318j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.74471349-0.46395208j 0.43952282+0.1922758j ]\n",
      "RESULT: False\n",
      "reward till now:  -56.0\n",
      "episode number:  746\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.74471349-0.46395208j 0.43952282+0.1922758j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.83738152-0.19210414j 0.21580239-0.46402318j]\n",
      "RESULT: False\n",
      "reward till now:  -57.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 747 reward -57.00, Last 30ep Avg. rewards -57.00.\n",
      "episode number:  747\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.09815985-0.10432302j -0.27611752-0.95038964j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -57.0\n",
      "episode number:  747\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -57.0\n",
      "episode number:  747\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -57.0\n",
      "episode number:  747\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -56.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 748 reward -56.00, Last 30ep Avg. rewards -56.00.\n",
      "episode number:  748\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.33032699+0.87149007j -0.33384186-0.14120464j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -56.0\n",
      "episode number:  748\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -55.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 749 reward -55.00, Last 30ep Avg. rewards -55.00.\n",
      "episode number:  749\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.65981033-0.72744656j 0.11736684-0.14729853j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.54954724-0.61853818j 0.38356547-0.41022661j]\n",
      "RESULT: False\n",
      "reward till now:  -55.0\n",
      "episode number:  749\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.54954724-0.61853818j 0.38356547-0.41022661j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -55.0\n",
      "episode number:  749\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -55.0\n",
      "episode number:  749\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -54.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 750 reward -54.00, Last 30ep Avg. rewards -54.00.\n",
      "episode number:  750\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.13439792+0.583984j   0.41837305-0.68254222j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -54.0\n",
      "episode number:  750\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -53.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]]\n",
      "Episode 751 reward -53.00, Last 30ep Avg. rewards -53.00.\n",
      "episode number:  751\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.01530346-0.06559446j 0.26988926+0.96053264j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -53.0\n",
      "episode number:  751\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -53.0\n",
      "episode number:  751\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -53.0\n",
      "episode number:  751\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -53.0\n",
      "episode number:  751\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -54.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 752 reward -54.00, Last 30ep Avg. rewards -54.00.\n",
      "episode number:  752\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.44739668+0.33413859j -0.15221005-0.81548741j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -54.0\n",
      "episode number:  752\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -54.0\n",
      "episode number:  752\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -54.0\n",
      "episode number:  752\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -54.0\n",
      "episode number:  752\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -55.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 753 reward -55.00, Last 30ep Avg. rewards -55.00.\n",
      "episode number:  753\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.559712  -0.39582143j  0.61612251+0.38786716j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -55.0\n",
      "episode number:  753\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -55.0\n",
      "episode number:  753\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -55.0\n",
      "episode number:  753\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -55.0\n",
      "episode number:  753\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -56.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 754 reward -56.00, Last 30ep Avg. rewards -56.00.\n",
      "episode number:  754\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.38035119+0.31358821j -0.85276758-0.17257711j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -56.0\n",
      "episode number:  754\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -56.0\n",
      "episode number:  754\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -56.0\n",
      "episode number:  754\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -56.0\n",
      "episode number:  754\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -57.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 755 reward -57.00, Last 30ep Avg. rewards -57.00.\n",
      "episode number:  755\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.27092814-0.33222097j -0.87774102-0.21400439j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -57.0\n",
      "episode number:  755\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -57.0\n",
      "episode number:  755\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -57.0\n",
      "episode number:  755\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -57.0\n",
      "episode number:  755\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -58.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 756 reward -58.00, Last 30ep Avg. rewards -58.00.\n",
      "episode number:  756\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.81925899-0.28968324j -0.47032197-0.15393367j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -58.0\n",
      "episode number:  756\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -57.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 757 reward -57.00, Last 30ep Avg. rewards -57.00.\n",
      "episode number:  757\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.5330258 +0.79056946j -0.13939351-0.2673067j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -57.0\n",
      "episode number:  757\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -56.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 758 reward -56.00, Last 30ep Avg. rewards -56.00.\n",
      "episode number:  758\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.51286029-0.24664385j -0.81815756-0.08221522j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -56.0\n",
      "episode number:  758\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -55.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 759 reward -55.00, Last 30ep Avg. rewards -55.00.\n",
      "episode number:  759\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.24178865+0.44775088j -0.19943228-0.83742711j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -55.0\n",
      "episode number:  759\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -55.0\n",
      "episode number:  759\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -55.0\n",
      "episode number:  759\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -55.0\n",
      "episode number:  759\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -56.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 760 reward -56.00, Last 30ep Avg. rewards -56.00.\n",
      "episode number:  760\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.90050505+0.41075424j  0.02168122+0.14107279j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -56.0\n",
      "episode number:  760\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -55.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]]\n",
      "Episode 761 reward -55.00, Last 30ep Avg. rewards -55.00.\n",
      "episode number:  761\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.22135483-0.83339205j  0.308392  +0.40168907j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -55.0\n",
      "episode number:  761\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -54.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]\n",
      " [0.95]\n",
      " [1.  ]]\n",
      "Episode 762 reward -54.00, Last 30ep Avg. rewards -54.00.\n",
      "episode number:  762\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.01377001+0.40886185j  0.16987168-0.89654112j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -54.0\n",
      "episode number:  762\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -53.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]\n",
      " [0.95]\n",
      " [1.  ]\n",
      " [0.95]\n",
      " [1.  ]]\n",
      "Episode 763 reward -53.00, Last 30ep Avg. rewards -53.00.\n",
      "episode number:  763\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.72316296-0.67489689j 0.14112193-0.0404243j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.6111417 -0.50580846j 0.41156516-0.44863987j]\n",
      "RESULT: False\n",
      "reward till now:  -53.0\n",
      "episode number:  763\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.6111417 -0.50580846j 0.41156516-0.44863987j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -53.0\n",
      "episode number:  763\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -52.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 764 reward -52.00, Last 30ep Avg. rewards -52.00.\n",
      "episode number:  764\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.56022708-0.53633886j 0.04540491-0.62962262j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.42824648-0.82445927j 0.36403425+0.06596158j]\n",
      "RESULT: False\n",
      "reward till now:  -52.0\n",
      "episode number:  764\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.42824648-0.82445927j 0.36403425+0.06596158j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -52.0\n",
      "episode number:  764\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -51.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 765 reward -51.00, Last 30ep Avg. rewards -51.00.\n",
      "episode number:  765\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.10586801+0.7834536j   0.42623096-0.43968124j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -51.0\n",
      "episode number:  765\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -50.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 766 reward -50.00, Last 30ep Avg. rewards -50.00.\n",
      "episode number:  766\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.28717157+0.10079496j -0.82068525-0.48357893j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -50.0\n",
      "episode number:  766\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -50.0\n",
      "episode number:  766\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -50.0\n",
      "episode number:  766\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -50.0\n",
      "episode number:  766\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -51.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 767 reward -51.00, Last 30ep Avg. rewards -51.00.\n",
      "episode number:  767\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.36590974+0.51290963j -0.24054196+0.73835854j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -51.0\n",
      "episode number:  767\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -50.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 768 reward -50.00, Last 30ep Avg. rewards -50.00.\n",
      "episode number:  768\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.86731646-0.33798139j -0.25113745+0.26544437j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -50.0\n",
      "episode number:  768\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -49.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 769 reward -49.00, Last 30ep Avg. rewards -49.00.\n",
      "episode number:  769\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.18575611-0.24153397j  0.56184183+0.76908372j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -49.0\n",
      "episode number:  769\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -49.0\n",
      "episode number:  769\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -49.0\n",
      "episode number:  769\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -49.0\n",
      "episode number:  769\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -50.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 770 reward -50.00, Last 30ep Avg. rewards -50.00.\n",
      "episode number:  770\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.54073959+0.13359936j -0.78588711+0.26857653j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -50.0\n",
      "episode number:  770\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -50.0\n",
      "episode number:  770\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -49.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 771 reward -49.00, Last 30ep Avg. rewards -49.00.\n",
      "episode number:  771\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.74084072+0.49391156j 0.00248884+0.45519249j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.52561337+0.67111791j 0.52209362+0.02737851j]\n",
      "RESULT: False\n",
      "reward till now:  -49.0\n",
      "episode number:  771\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.52561337+0.67111791j 0.52209362+0.02737851j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -49.0\n",
      "episode number:  771\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -48.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 772 reward -48.00, Last 30ep Avg. rewards -48.00.\n",
      "episode number:  772\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.26975224-0.26008627j -0.72994215+0.5716409j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -48.0\n",
      "episode number:  772\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -48.0\n",
      "episode number:  772\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -47.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 773 reward -47.00, Last 30ep Avg. rewards -47.00.\n",
      "episode number:  773\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.26409352+0.54512377j  0.4100974 -0.68184662j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -47.0\n",
      "episode number:  773\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -47.0\n",
      "episode number:  773\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -46.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 774 reward -46.00, Last 30ep Avg. rewards -46.00.\n",
      "episode number:  774\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.32238409-0.22532966j  0.16856375-0.90381486j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -46.0\n",
      "episode number:  774\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -45.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 775 reward -45.00, Last 30ep Avg. rewards -45.00.\n",
      "episode number:  775\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.5379365 -0.82278233j 0.17905238-0.03992247j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.17905238-0.03992247j 0.5379365 -0.82278233j]\n",
      "RESULT: False\n",
      "reward till now:  -45.0\n",
      "episode number:  775\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.17905238-0.03992247j 0.5379365 -0.82278233j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -45.0\n",
      "episode number:  775\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -45.0\n",
      "episode number:  775\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -44.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 776 reward -44.00, Last 30ep Avg. rewards -44.00.\n",
      "episode number:  776\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-1.14205492e-04+0.72286254j  6.14904571e-01+0.31521755j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -44.0\n",
      "episode number:  776\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -44.0\n",
      "episode number:  776\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -43.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 777 reward -43.00, Last 30ep Avg. rewards -43.00.\n",
      "episode number:  777\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.70022745-0.64031896j  0.31214392+0.04732142j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -43.0\n",
      "episode number:  777\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -42.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 778 reward -42.00, Last 30ep Avg. rewards -42.00.\n",
      "episode number:  778\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.10134937+0.678431j   -0.72657752-0.03930377j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -42.0\n",
      "episode number:  778\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -41.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 779 reward -41.00, Last 30ep Avg. rewards -41.00.\n",
      "episode number:  779\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.09028882-0.3831131j  0.04130294-0.9183498j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -41.0\n",
      "episode number:  779\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -41.0\n",
      "episode number:  779\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -40.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 780 reward -40.00, Last 30ep Avg. rewards -40.00.\n",
      "episode number:  780\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.24803348+0.13762692j -0.95292255-0.10713002j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -40.0\n",
      "episode number:  780\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -40.0\n",
      "episode number:  780\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -40.0\n",
      "episode number:  780\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -39.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 781 reward -39.00, Last 30ep Avg. rewards -39.00.\n",
      "episode number:  781\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.39672469-0.51595636j -0.73520323+0.18940634j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -39.0\n",
      "episode number:  781\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -39.0\n",
      "episode number:  781\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -39.0\n",
      "episode number:  781\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -39.0\n",
      "episode number:  781\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -40.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 782 reward -40.00, Last 30ep Avg. rewards -40.00.\n",
      "episode number:  782\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.12248374+0.84093389j  0.01362445+0.52691774j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -40.0\n",
      "episode number:  782\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -39.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 783 reward -39.00, Last 30ep Avg. rewards -39.00.\n",
      "episode number:  783\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.61909055-0.63492984j  0.13085758+0.44324631j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -39.0\n",
      "episode number:  783\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -38.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 784 reward -38.00, Last 30ep Avg. rewards -38.00.\n",
      "episode number:  784\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.21382107+0.85814102j -0.03971878-0.46507737j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -38.0\n",
      "episode number:  784\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -37.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 785 reward -37.00, Last 30ep Avg. rewards -37.00.\n",
      "episode number:  785\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.66001243+0.08094694j -0.70568288-0.244628j  ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -37.0\n",
      "episode number:  785\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -36.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 786 reward -36.00, Last 30ep Avg. rewards -36.00.\n",
      "episode number:  786\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.43503647+0.53710291j -0.71698442+0.0905377j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -36.0\n",
      "episode number:  786\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -36.0\n",
      "episode number:  786\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -35.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 787 reward -35.00, Last 30ep Avg. rewards -35.00.\n",
      "episode number:  787\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.14432625-0.58741063j -0.175724  -0.77668511j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -35.0\n",
      "episode number:  787\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -34.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 788 reward -34.00, Last 30ep Avg. rewards -34.00.\n",
      "episode number:  788\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.72249098-0.12593847j -0.5244511 +0.43254749j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -34.0\n",
      "episode number:  788\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -33.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 789 reward -33.00, Last 30ep Avg. rewards -33.00.\n",
      "episode number:  789\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.55684861-0.52007054j 0.57738842-0.29337498j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULT: False\n",
      "reward till now:  -33.0\n",
      "episode number:  789\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -32.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 790 reward -32.00, Last 30ep Avg. rewards -32.00.\n",
      "episode number:  790\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.06619323+0.32178114j -0.46026789-0.82475986j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -32.0\n",
      "episode number:  790\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -31.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]]\n",
      "Episode 791 reward -31.00, Last 30ep Avg. rewards -31.00.\n",
      "episode number:  791\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.1657676 -0.22487188j -0.03144772+0.9596691j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -31.0\n",
      "episode number:  791\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -31.0\n",
      "episode number:  791\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -31.0\n",
      "episode number:  791\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -31.0\n",
      "episode number:  791\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -32.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 792 reward -32.00, Last 30ep Avg. rewards -32.00.\n",
      "episode number:  792\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.13839565+0.08431347j  0.18902872+0.96850711j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -32.0\n",
      "episode number:  792\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -32.0\n",
      "episode number:  792\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -32.0\n",
      "episode number:  792\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -31.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 793 reward -31.00, Last 30ep Avg. rewards -31.00.\n",
      "episode number:  793\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.07335349-0.59654345j 0.53865377-0.59042975j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -31.0\n",
      "episode number:  793\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -31.0\n",
      "episode number:  793\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -31.0\n",
      "episode number:  793\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -31.0\n",
      "episode number:  793\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -32.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 794 reward -32.00, Last 30ep Avg. rewards -32.00.\n",
      "episode number:  794\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.17454837+0.05148269j  0.97785021+0.10339902j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -32.0\n",
      "episode number:  794\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -31.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 795 reward -31.00, Last 30ep Avg. rewards -31.00.\n",
      "episode number:  795\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.8864473+0.1106318j  0.4437309-0.07123678j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.94057802+0.02785649j 0.31304777+0.12860051j]\n",
      "RESULT: False\n",
      "reward till now:  -31.0\n",
      "episode number:  795\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.94057802+0.02785649j 0.31304777+0.12860051j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.8864473+0.1106318j  0.4437309-0.07123678j]\n",
      "RESULT: False\n",
      "reward till now:  -31.0\n",
      "episode number:  795\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.8864473+0.1106318j  0.4437309-0.07123678j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.94057802+0.02785649j 0.31304777+0.12860051j]\n",
      "RESULT: False\n",
      "reward till now:  -31.0\n",
      "episode number:  795\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.94057802+0.02785649j 0.31304777+0.12860051j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.8864473+0.1106318j  0.4437309-0.07123678j]\n",
      "RESULT: False\n",
      "reward till now:  -31.0\n",
      "episode number:  795\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.8864473+0.1106318j  0.4437309-0.07123678j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.94057802+0.02785649j 0.31304777+0.12860051j]\n",
      "RESULT: False\n",
      "reward till now:  -32.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 796 reward -32.00, Last 30ep Avg. rewards -32.00.\n",
      "episode number:  796\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.30407284+0.87010524j -0.13783607+0.3625711j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -32.0\n",
      "episode number:  796\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -31.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 797 reward -31.00, Last 30ep Avg. rewards -31.00.\n",
      "episode number:  797\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.73155581+0.03681703j 0.60175209-0.31837874j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.94279106-0.19909419j 0.09178509+0.25116134j]\n",
      "RESULT: False\n",
      "reward till now:  -31.0\n",
      "episode number:  797\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.94279106-0.19909419j 0.09178509+0.25116134j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.73155581+0.03681703j 0.60175209-0.31837874j]\n",
      "RESULT: False\n",
      "reward till now:  -31.0\n",
      "episode number:  797\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.73155581+0.03681703j 0.60175209-0.31837874j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.94279106-0.19909419j 0.09178509+0.25116134j]\n",
      "RESULT: False\n",
      "reward till now:  -31.0\n",
      "episode number:  797\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.94279106-0.19909419j 0.09178509+0.25116134j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.73155581+0.03681703j 0.60175209-0.31837874j]\n",
      "RESULT: False\n",
      "reward till now:  -31.0\n",
      "episode number:  797\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.73155581+0.03681703j 0.60175209-0.31837874j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.94279106-0.19909419j 0.09178509+0.25116134j]\n",
      "RESULT: False\n",
      "reward till now:  -32.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 798 reward -32.00, Last 30ep Avg. rewards -32.00.\n",
      "episode number:  798\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.34287872-0.5972158j  -0.55999689+0.46062018j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -32.0\n",
      "episode number:  798\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -31.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 799 reward -31.00, Last 30ep Avg. rewards -31.00.\n",
      "episode number:  799\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.17871542-0.67964354j -0.70201807-0.11539539j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -31.0\n",
      "episode number:  799\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -30.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 800 reward -30.00, Last 30ep Avg. rewards -30.00.\n",
      "episode number:  800\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.56696763-0.1110255j   0.72329779-0.37823451j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -30.0\n",
      "episode number:  800\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -30.0\n",
      "episode number:  800\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -30.0\n",
      "episode number:  800\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -29.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 801 reward -29.00, Last 30ep Avg. rewards -29.00.\n",
      "episode number:  801\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.13727414-0.47041386j 0.55749599-0.670123j  ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -29.0\n",
      "episode number:  801\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -29.0\n",
      "episode number:  801\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -29.0\n",
      "episode number:  801\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -29.0\n",
      "episode number:  801\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -30.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 802 reward -30.00, Last 30ep Avg. rewards -30.00.\n",
      "episode number:  802\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.72720631+0.2382192j  -0.58200339+0.27512661j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -30.0\n",
      "episode number:  802\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -29.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 803 reward -29.00, Last 30ep Avg. rewards -29.00.\n",
      "episode number:  803\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.44814415-0.8329933j  -0.02131625+0.32378171j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -29.0\n",
      "episode number:  803\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -28.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 804 reward -28.00, Last 30ep Avg. rewards -28.00.\n",
      "episode number:  804\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.34232211-0.65168108j 0.60095867-0.31140974j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -28.0\n",
      "episode number:  804\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -27.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 805 reward -27.00, Last 30ep Avg. rewards -27.00.\n",
      "episode number:  805\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.35400577-0.015746j   -0.27099753+0.89498174j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -27.0\n",
      "episode number:  805\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -27.0\n",
      "episode number:  805\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -27.0\n",
      "episode number:  805\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -27.0\n",
      "episode number:  805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -28.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 806 reward -28.00, Last 30ep Avg. rewards -28.00.\n",
      "episode number:  806\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.03252547+0.40423093j -0.68633881+0.60372054j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -28.0\n",
      "episode number:  806\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -28.0\n",
      "episode number:  806\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -27.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 807 reward -27.00, Last 30ep Avg. rewards -27.00.\n",
      "episode number:  807\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.4632149 -0.4324406j  -0.68500936-0.35942908j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -27.0\n",
      "episode number:  807\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -27.0\n",
      "episode number:  807\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -27.0\n",
      "episode number:  807\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -26.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 808 reward -26.00, Last 30ep Avg. rewards -26.00.\n",
      "episode number:  808\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.89602675-0.44074916j  0.05270447+0.00992362j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -26.0\n",
      "episode number:  808\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -25.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 809 reward -25.00, Last 30ep Avg. rewards -25.00.\n",
      "episode number:  809\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.37023826+0.24999057j  0.23909809+0.86212554j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -25.0\n",
      "episode number:  809\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -25.0\n",
      "episode number:  809\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -25.0\n",
      "episode number:  809\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -25.0\n",
      "episode number:  809\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -26.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 810 reward -26.00, Last 30ep Avg. rewards -26.00.\n",
      "episode number:  810\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.85977361-0.06713571j  0.39992785+0.31038661j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -26.0\n",
      "episode number:  810\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -25.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]]\n",
      "Episode 811 reward -25.00, Last 30ep Avg. rewards -25.00.\n",
      "episode number:  811\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.19904582+0.2706192j  -0.90511172+0.26061232j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -25.0\n",
      "episode number:  811\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -25.0\n",
      "episode number:  811\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -25.0\n",
      "episode number:  811\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -25.0\n",
      "episode number:  811\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -26.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 812 reward -26.00, Last 30ep Avg. rewards -26.00.\n",
      "episode number:  812\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.37657096-0.27051297j -0.87290812-0.15181718j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -26.0\n",
      "episode number:  812\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -26.0\n",
      "episode number:  812\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -26.0\n",
      "episode number:  812\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -25.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 813 reward -25.00, Last 30ep Avg. rewards -25.00.\n",
      "episode number:  813\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.24288904-0.78187547j -0.10705821-0.56410478j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -25.0\n",
      "episode number:  813\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -25.0\n",
      "episode number:  813\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -25.0\n",
      "episode number:  813\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -25.0\n",
      "episode number:  813\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -26.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 814 reward -26.00, Last 30ep Avg. rewards -26.00.\n",
      "episode number:  814\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.43922183-0.23276422j 0.06235957-0.86545727j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.35467161-0.77655986j 0.26648186+0.44738155j]\n",
      "RESULT: False\n",
      "reward till now:  -26.0\n",
      "episode number:  814\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.35467161-0.77655986j 0.26648186+0.44738155j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -26.0\n",
      "episode number:  814\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -25.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 815 reward -25.00, Last 30ep Avg. rewards -25.00.\n",
      "episode number:  815\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.85991349+0.2171164j  -0.4178062 -0.19709703j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -25.0\n",
      "episode number:  815\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -25.0\n",
      "episode number:  815\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -25.0\n",
      "episode number:  815\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -24.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 816 reward -24.00, Last 30ep Avg. rewards -24.00.\n",
      "episode number:  816\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.93384374-0.30622603j -0.10622573+0.15125339j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -24.0\n",
      "episode number:  816\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -23.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 817 reward -23.00, Last 30ep Avg. rewards -23.00.\n",
      "episode number:  817\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.44162158+0.87043291j  0.03439734+0.21478768j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -23.0\n",
      "episode number:  817\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -22.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 818 reward -22.00, Last 30ep Avg. rewards -22.00.\n",
      "episode number:  818\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.08117146-0.54621758j  0.82051954+0.14766598j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -22.0\n",
      "episode number:  818\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -22.0\n",
      "episode number:  818\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -22.0\n",
      "episode number:  818\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  nothing(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -22.0\n",
      "episode number:  818\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -23.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 819 reward -23.00, Last 30ep Avg. rewards -23.00.\n",
      "episode number:  819\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.90439873+0.2170832j  -0.31938764+0.18146448j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.41366531+0.28181577j 0.86534764+0.02518623j]\n",
      "RESULT: False\n",
      "reward till now:  -23.0\n",
      "episode number:  819\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.41366531+0.28181577j 0.86534764+0.02518623j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.90439873+0.2170832j  -0.31938764+0.18146448j]\n",
      "RESULT: False\n",
      "reward till now:  -23.0\n",
      "episode number:  819\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.90439873+0.2170832j  -0.31938764+0.18146448j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.41366531+0.28181577j 0.86534764+0.02518623j]\n",
      "RESULT: False\n",
      "reward till now:  -23.0\n",
      "episode number:  819\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.41366531+0.28181577j 0.86534764+0.02518623j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.90439873+0.2170832j  -0.31938764+0.18146448j]\n",
      "RESULT: False\n",
      "reward till now:  -23.0\n",
      "episode number:  819\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.90439873+0.2170832j  -0.31938764+0.18146448j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.41366531+0.28181577j 0.86534764+0.02518623j]\n",
      "RESULT: False\n",
      "reward till now:  -24.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 820 reward -24.00, Last 30ep Avg. rewards -24.00.\n",
      "episode number:  820\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.36640166+0.59605205j -0.36354006+0.61506943j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -24.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode number:  820\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -24.0\n",
      "episode number:  820\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -24.0\n",
      "episode number:  820\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -24.0\n",
      "episode number:  820\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -25.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 821 reward -25.00, Last 30ep Avg. rewards -25.00.\n",
      "episode number:  821\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.94036583-0.02216652j -0.09105405+0.32700138j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.60055412+0.2155508j  0.72932399-0.24689899j]\n",
      "RESULT: False\n",
      "reward till now:  -25.0\n",
      "episode number:  821\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.60055412+0.2155508j  0.72932399-0.24689899j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.94036583-0.02216652j -0.09105405+0.32700138j]\n",
      "RESULT: False\n",
      "reward till now:  -25.0\n",
      "episode number:  821\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.94036583-0.02216652j -0.09105405+0.32700138j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.60055412+0.2155508j  0.72932399-0.24689899j]\n",
      "RESULT: False\n",
      "reward till now:  -25.0\n",
      "episode number:  821\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.60055412+0.2155508j  0.72932399-0.24689899j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.94036583-0.02216652j -0.09105405+0.32700138j]\n",
      "RESULT: False\n",
      "reward till now:  -25.0\n",
      "episode number:  821\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.94036583-0.02216652j -0.09105405+0.32700138j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.60055412+0.2155508j  0.72932399-0.24689899j]\n",
      "RESULT: False\n",
      "reward till now:  -26.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 822 reward -26.00, Last 30ep Avg. rewards -26.00.\n",
      "episode number:  822\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.49042839-0.17305364j -0.78062837-0.34662917j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -26.0\n",
      "episode number:  822\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -26.0\n",
      "episode number:  822\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -26.0\n",
      "episode number:  822\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -26.0\n",
      "episode number:  822\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -27.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 823 reward -27.00, Last 30ep Avg. rewards -27.00.\n",
      "episode number:  823\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.56767716+0.23198552j 0.42381216+0.66656478j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70108882+0.63537101j 0.10172792-0.30729394j]\n",
      "RESULT: False\n",
      "reward till now:  -27.0\n",
      "episode number:  823\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.70108882+0.63537101j 0.10172792-0.30729394j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.56767716+0.23198552j 0.42381216+0.66656478j]\n",
      "RESULT: False\n",
      "reward till now:  -27.0\n",
      "episode number:  823\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.56767716+0.23198552j 0.42381216+0.66656478j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70108882+0.63537101j 0.10172792-0.30729394j]\n",
      "RESULT: False\n",
      "reward till now:  -27.0\n",
      "episode number:  823\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.70108882+0.63537101j 0.10172792-0.30729394j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.56767716+0.23198552j 0.42381216+0.66656478j]\n",
      "RESULT: False\n",
      "reward till now:  -27.0\n",
      "episode number:  823\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.56767716+0.23198552j 0.42381216+0.66656478j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70108882+0.63537101j 0.10172792-0.30729394j]\n",
      "RESULT: False\n",
      "reward till now:  -28.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 824 reward -28.00, Last 30ep Avg. rewards -28.00.\n",
      "episode number:  824\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.78127501-0.49074864j -0.02401927+0.3849652j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.53546067-0.07480019j 0.56942904-0.6192232j ]\n",
      "RESULT: False\n",
      "reward till now:  -28.0\n",
      "episode number:  824\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.53546067-0.07480019j 0.56942904-0.6192232j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -28.0\n",
      "episode number:  824\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -27.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 825 reward -27.00, Last 30ep Avg. rewards -27.00.\n",
      "episode number:  825\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.22877878-0.01391201j -0.88074545+0.41443236j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -27.0\n",
      "episode number:  825\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -27.0\n",
      "episode number:  825\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -27.0\n",
      "episode number:  825\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -27.0\n",
      "episode number:  825\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -28.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 826 reward -28.00, Last 30ep Avg. rewards -28.00.\n",
      "episode number:  826\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.78923076-0.39545111j -0.41380485+0.22248319j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.2654662 -0.12230679j 0.85067464-0.43694554j]\n",
      "RESULT: False\n",
      "reward till now:  -28.0\n",
      "episode number:  826\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.2654662 -0.12230679j 0.85067464-0.43694554j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.78923076-0.39545111j -0.41380485+0.22248319j]\n",
      "RESULT: False\n",
      "reward till now:  -28.0\n",
      "episode number:  826\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.78923076-0.39545111j -0.41380485+0.22248319j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.2654662 -0.12230679j 0.85067464-0.43694554j]\n",
      "RESULT: False\n",
      "reward till now:  -28.0\n",
      "episode number:  826\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.2654662 -0.12230679j 0.85067464-0.43694554j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.78923076-0.39545111j -0.41380485+0.22248319j]\n",
      "RESULT: False\n",
      "reward till now:  -28.0\n",
      "episode number:  826\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.78923076-0.39545111j -0.41380485+0.22248319j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.2654662 -0.12230679j 0.85067464-0.43694554j]\n",
      "RESULT: False\n",
      "reward till now:  -29.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 827 reward -29.00, Last 30ep Avg. rewards -29.00.\n",
      "episode number:  827\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.42824857-0.3681824j   0.61492567+0.55037379j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -29.0\n",
      "episode number:  827\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -29.0\n",
      "episode number:  827\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -29.0\n",
      "episode number:  827\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -28.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 828 reward -28.00, Last 30ep Avg. rewards -28.00.\n",
      "episode number:  828\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.09498292-0.96024489j -0.2124627 -0.15416744j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -28.0\n",
      "episode number:  828\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -27.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 829 reward -27.00, Last 30ep Avg. rewards -27.00.\n",
      "episode number:  829\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.3075304 +0.93897791j  0.15305858-0.01784944j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -27.0\n",
      "episode number:  829\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -26.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 830 reward -26.00, Last 30ep Avg. rewards -26.00.\n",
      "episode number:  830\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.62402544+0.4349077j  -0.37813338+0.52769564j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.17387195+0.68066335j 0.70863329-0.06561098j]\n",
      "RESULT: False\n",
      "reward till now:  -26.0\n",
      "episode number:  830\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.17387195+0.68066335j 0.70863329-0.06561098j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -26.0\n",
      "episode number:  830\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -25.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 831 reward -25.00, Last 30ep Avg. rewards -25.00.\n",
      "episode number:  831\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.22034119+0.68833338j 0.13321912-0.67815896j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -25.0\n",
      "episode number:  831\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -25.0\n",
      "episode number:  831\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -25.0\n",
      "episode number:  831\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -24.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 832 reward -24.00, Last 30ep Avg. rewards -24.00.\n",
      "episode number:  832\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.29464092-0.77252516j -0.26408298+0.4966405j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -24.0\n",
      "episode number:  832\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -23.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 833 reward -23.00, Last 30ep Avg. rewards -23.00.\n",
      "episode number:  833\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.58634513+0.5830845j  -0.37400825+0.41991628j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.15014485+0.70922865j 0.67907239+0.11537736j]\n",
      "RESULT: False\n",
      "reward till now:  -23.0\n",
      "episode number:  833\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.15014485+0.70922865j 0.67907239+0.11537736j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -23.0\n",
      "episode number:  833\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -22.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n",
      "Episode 834 reward -22.00, Last 30ep Avg. rewards -22.00.\n",
      "episode number:  834\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.77023588-0.6344823j   0.03570902+0.05379375j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -22.0\n",
      "episode number:  834\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -21.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.857375]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.9025  ]\n",
      " [0.95    ]\n",
      " [1.      ]\n",
      " [0.95    ]\n",
      " [1.      ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 835 reward -21.00, Last 30ep Avg. rewards -21.00.\n",
      "episode number:  835\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.74317922+0.23000568j -0.43861993-0.44988285j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.21535594-0.15547664j 0.8356582 +0.48075379j]\n",
      "RESULT: False\n",
      "reward till now:  -21.0\n",
      "episode number:  835\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.21535594-0.15547664j 0.8356582 +0.48075379j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.74317922+0.23000568j -0.43861993-0.44988285j]\n",
      "RESULT: False\n",
      "reward till now:  -21.0\n",
      "episode number:  835\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.74317922+0.23000568j -0.43861993-0.44988285j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.21535594-0.15547664j 0.8356582 +0.48075379j]\n",
      "RESULT: False\n",
      "reward till now:  -21.0\n",
      "episode number:  835\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.21535594-0.15547664j 0.8356582 +0.48075379j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.74317922+0.23000568j -0.43861993-0.44988285j]\n",
      "RESULT: False\n",
      "reward till now:  -21.0\n",
      "episode number:  835\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.74317922+0.23000568j -0.43861993-0.44988285j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.21535594-0.15547664j 0.8356582 +0.48075379j]\n",
      "RESULT: False\n",
      "reward till now:  -22.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 836 reward -22.00, Last 30ep Avg. rewards -22.00.\n",
      "episode number:  836\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.7702923 +0.00404785j -0.58520119+0.25332382j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -22.0\n",
      "episode number:  836\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -21.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 837 reward -21.00, Last 30ep Avg. rewards -21.00.\n",
      "episode number:  837\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.42209937-0.31940404j  0.33483714+0.7795494j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -21.0\n",
      "episode number:  837\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -21.0\n",
      "episode number:  837\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -21.0\n",
      "episode number:  837\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -21.0\n",
      "episode number:  837\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -22.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 838 reward -22.00, Last 30ep Avg. rewards -22.00.\n",
      "episode number:  838\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.47224104-0.39014642j 0.65388392-0.44408332j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -22.0\n",
      "episode number:  838\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -22.0\n",
      "episode number:  838\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -22.0\n",
      "episode number:  838\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -21.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 839 reward -21.00, Last 30ep Avg. rewards -21.00.\n",
      "episode number:  839\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.78215783-0.57146358j -0.24355234+0.04838146j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.3808516 -0.36987491j 0.72528662-0.43829663j]\n",
      "RESULT: False\n",
      "reward till now:  -21.0\n",
      "episode number:  839\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.3808516 -0.36987491j 0.72528662-0.43829663j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -21.0\n",
      "episode number:  839\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -21.0\n",
      "episode number:  839\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -21.0\n",
      "episode number:  839\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -20.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 840 reward -20.00, Last 30ep Avg. rewards -20.00.\n",
      "episode number:  840\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.39640395+0.37179019j -0.39054945+0.74303909j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -20.0\n",
      "episode number:  840\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -20.0\n",
      "episode number:  840\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -20.0\n",
      "episode number:  840\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -20.0\n",
      "episode number:  840\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -21.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 841 reward -21.00, Last 30ep Avg. rewards -21.00.\n",
      "episode number:  841\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.64667646-0.60537211j 0.34775621+0.30724548j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70317008-0.21080737j 0.21136853-0.64531809j]\n",
      "RESULT: False\n",
      "reward till now:  -21.0\n",
      "episode number:  841\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.70317008-0.21080737j 0.21136853-0.64531809j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.64667646-0.60537211j 0.34775621+0.30724548j]\n",
      "RESULT: False\n",
      "reward till now:  -21.0\n",
      "episode number:  841\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.64667646-0.60537211j 0.34775621+0.30724548j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70317008-0.21080737j 0.21136853-0.64531809j]\n",
      "RESULT: False\n",
      "reward till now:  -21.0\n",
      "episode number:  841\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.70317008-0.21080737j 0.21136853-0.64531809j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.64667646-0.60537211j 0.34775621+0.30724548j]\n",
      "RESULT: False\n",
      "reward till now:  -21.0\n",
      "episode number:  841\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.64667646-0.60537211j 0.34775621+0.30724548j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70317008-0.21080737j 0.21136853-0.64531809j]\n",
      "RESULT: False\n",
      "reward till now:  -22.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 842 reward -22.00, Last 30ep Avg. rewards -22.00.\n",
      "episode number:  842\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.39164505-0.77976088j -0.33481009-0.35565338j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -22.0\n",
      "episode number:  842\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -22.0\n",
      "episode number:  842\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -22.0\n",
      "episode number:  842\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -22.0\n",
      "episode number:  842\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -23.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 843 reward -23.00, Last 30ep Avg. rewards -23.00.\n",
      "episode number:  843\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.46801882-0.25714146j -0.15766587+0.83064922j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.21945267+0.40553123j 0.44242588-0.76918417j]\n",
      "RESULT: False\n",
      "reward till now:  -23.0\n",
      "episode number:  843\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.21945267+0.40553123j 0.44242588-0.76918417j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -23.0\n",
      "episode number:  843\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -22.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 844 reward -22.00, Last 30ep Avg. rewards -22.00.\n",
      "episode number:  844\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.17653857-0.45717537j -0.58359738-0.64748662j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -22.0\n",
      "episode number:  844\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -22.0\n",
      "episode number:  844\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -22.0\n",
      "episode number:  844\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -22.0\n",
      "episode number:  844\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -23.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 845 reward -23.00, Last 30ep Avg. rewards -23.00.\n",
      "episode number:  845\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.26083286-0.68209527j  0.5991035 -0.32830968j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -23.0\n",
      "episode number:  845\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -22.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 846 reward -22.00, Last 30ep Avg. rewards -22.00.\n",
      "episode number:  846\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.36414578+0.28028983j -0.31450628+0.83061499j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -22.0\n",
      "episode number:  846\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -22.0\n",
      "episode number:  846\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -22.0\n",
      "episode number:  846\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -22.0\n",
      "episode number:  846\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -23.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 847 reward -23.00, Last 30ep Avg. rewards -23.00.\n",
      "episode number:  847\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.68662022+0.41900277j -0.35718565+0.47477127j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -23.0\n",
      "episode number:  847\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -23.0\n",
      "episode number:  847\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -23.0\n",
      "episode number:  847\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -23.0\n",
      "episode number:  847\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -24.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 848 reward -24.00, Last 30ep Avg. rewards -24.00.\n",
      "episode number:  848\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.2665311 -0.13338012j  0.34068497-0.89168642j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -24.0\n",
      "episode number:  848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -23.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 849 reward -23.00, Last 30ep Avg. rewards -23.00.\n",
      "episode number:  849\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.50604093-0.40672783j -0.17271115-0.74071986j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.23569975-0.81136804j 0.4799502 +0.23616803j]\n",
      "RESULT: False\n",
      "reward till now:  -23.0\n",
      "episode number:  849\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.23569975-0.81136804j 0.4799502 +0.23616803j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -23.0\n",
      "episode number:  849\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -23.0\n",
      "episode number:  849\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -23.0\n",
      "episode number:  849\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -24.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 850 reward -24.00, Last 30ep Avg. rewards -24.00.\n",
      "episode number:  850\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.26956629-0.86500691j -0.18280324-0.38168054j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -24.0\n",
      "episode number:  850\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -23.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]]\n",
      "Episode 851 reward -23.00, Last 30ep Avg. rewards -23.00.\n",
      "episode number:  851\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.6268326 -0.38512638j 0.60241374-0.30960659j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.86920843-0.49125039j 0.01726674-0.05340056j]\n",
      "RESULT: False\n",
      "reward till now:  -23.0\n",
      "episode number:  851\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.86920843-0.49125039j 0.01726674-0.05340056j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.6268326 -0.38512638j 0.60241374-0.30960659j]\n",
      "RESULT: False\n",
      "reward till now:  -23.0\n",
      "episode number:  851\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.6268326 -0.38512638j 0.60241374-0.30960659j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.86920843-0.49125039j 0.01726674-0.05340056j]\n",
      "RESULT: False\n",
      "reward till now:  -23.0\n",
      "episode number:  851\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.86920843-0.49125039j 0.01726674-0.05340056j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.6268326 -0.38512638j 0.60241374-0.30960659j]\n",
      "RESULT: False\n",
      "reward till now:  -23.0\n",
      "episode number:  851\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.6268326 -0.38512638j 0.60241374-0.30960659j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.86920843-0.49125039j 0.01726674-0.05340056j]\n",
      "RESULT: False\n",
      "reward till now:  -24.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 852 reward -24.00, Last 30ep Avg. rewards -24.00.\n",
      "episode number:  852\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.70577739+0.14855678j 0.61116517-0.3260158j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.93121901-0.12548248j 0.06690094+0.33557349j]\n",
      "RESULT: False\n",
      "reward till now:  -24.0\n",
      "episode number:  852\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.93121901-0.12548248j 0.06690094+0.33557349j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70577739+0.14855678j 0.61116517-0.3260158j ]\n",
      "RESULT: False\n",
      "reward till now:  -24.0\n",
      "episode number:  852\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.70577739+0.14855678j 0.61116517-0.3260158j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.93121901-0.12548248j 0.06690094+0.33557349j]\n",
      "RESULT: False\n",
      "reward till now:  -24.0\n",
      "episode number:  852\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.93121901-0.12548248j 0.06690094+0.33557349j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70577739+0.14855678j 0.61116517-0.3260158j ]\n",
      "RESULT: False\n",
      "reward till now:  -24.0\n",
      "episode number:  852\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.70577739+0.14855678j 0.61116517-0.3260158j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.93121901-0.12548248j 0.06690094+0.33557349j]\n",
      "RESULT: False\n",
      "reward till now:  -25.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 853 reward -25.00, Last 30ep Avg. rewards -25.00.\n",
      "episode number:  853\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.08264255+0.33774599j -0.78030618-0.51982701j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -25.0\n",
      "episode number:  853\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -25.0\n",
      "episode number:  853\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -25.0\n",
      "episode number:  853\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -25.0\n",
      "episode number:  853\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -26.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 854 reward -26.00, Last 30ep Avg. rewards -26.00.\n",
      "episode number:  854\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.59512895+0.38979769j 0.41890032-0.56427104j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.71702698-0.12337129j 0.12461246+0.67462847j]\n",
      "RESULT: False\n",
      "reward till now:  -26.0\n",
      "episode number:  854\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.71702698-0.12337129j 0.12461246+0.67462847j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.59512895+0.38979769j 0.41890032-0.56427104j]\n",
      "RESULT: False\n",
      "reward till now:  -26.0\n",
      "episode number:  854\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.59512895+0.38979769j 0.41890032-0.56427104j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.71702698-0.12337129j 0.12461246+0.67462847j]\n",
      "RESULT: False\n",
      "reward till now:  -26.0\n",
      "episode number:  854\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.71702698-0.12337129j 0.12461246+0.67462847j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.59512895+0.38979769j 0.41890032-0.56427104j]\n",
      "RESULT: False\n",
      "reward till now:  -26.0\n",
      "episode number:  854\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.59512895+0.38979769j 0.41890032-0.56427104j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.71702698-0.12337129j 0.12461246+0.67462847j]\n",
      "RESULT: False\n",
      "reward till now:  -27.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 855 reward -27.00, Last 30ep Avg. rewards -27.00.\n",
      "episode number:  855\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.13394893+0.60595008j -0.32125969-0.71531419j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -27.0\n",
      "episode number:  855\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -26.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 856 reward -26.00, Last 30ep Avg. rewards -26.00.\n",
      "episode number:  856\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.34435485+0.17601552j -0.05215149-0.9207163j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.20661898-0.52658297j 0.28037232+0.7755065j ]\n",
      "RESULT: False\n",
      "reward till now:  -26.0\n",
      "episode number:  856\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.20661898-0.52658297j 0.28037232+0.7755065j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -26.0\n",
      "episode number:  856\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -26.0\n",
      "episode number:  856\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -26.0\n",
      "episode number:  856\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -27.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 857 reward -27.00, Last 30ep Avg. rewards -27.00.\n",
      "episode number:  857\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.4711544 -0.49359199j -0.58605976+0.43693757j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -27.0\n",
      "episode number:  857\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -27.0\n",
      "episode number:  857\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -27.0\n",
      "episode number:  857\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -27.0\n",
      "episode number:  857\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -28.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 858 reward -28.00, Last 30ep Avg. rewards -28.00.\n",
      "episode number:  858\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.63584566+0.13302748j 0.44657631-0.61528334j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.76538791-0.34100639j 0.13383364+0.52913566j]\n",
      "RESULT: False\n",
      "reward till now:  -28.0\n",
      "episode number:  858\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.76538791-0.34100639j 0.13383364+0.52913566j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.63584566+0.13302748j 0.44657631-0.61528334j]\n",
      "RESULT: False\n",
      "reward till now:  -28.0\n",
      "episode number:  858\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.63584566+0.13302748j 0.44657631-0.61528334j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.76538791-0.34100639j 0.13383364+0.52913566j]\n",
      "RESULT: False\n",
      "reward till now:  -28.0\n",
      "episode number:  858\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.76538791-0.34100639j 0.13383364+0.52913566j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.63584566+0.13302748j 0.44657631-0.61528334j]\n",
      "RESULT: False\n",
      "reward till now:  -28.0\n",
      "episode number:  858\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.63584566+0.13302748j 0.44657631-0.61528334j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.76538791-0.34100639j 0.13383364+0.52913566j]\n",
      "RESULT: False\n",
      "reward till now:  -29.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 859 reward -29.00, Last 30ep Avg. rewards -29.00.\n",
      "episode number:  859\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.15068374+0.15884951j  0.17958064-0.95906832j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -29.0\n",
      "episode number:  859\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -29.0\n",
      "episode number:  859\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -29.0\n",
      "episode number:  859\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -29.0\n",
      "episode number:  859\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -30.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 860 reward -30.00, Last 30ep Avg. rewards -30.00.\n",
      "episode number:  860\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.34432429+0.7869782j  0.04981508-0.50953366j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.27869862+0.19618292j 0.20824946+0.91677233j]\n",
      "RESULT: False\n",
      "reward till now:  -30.0\n",
      "episode number:  860\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.27869862+0.19618292j 0.20824946+0.91677233j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -30.0\n",
      "episode number:  860\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -30.0\n",
      "episode number:  860\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -30.0\n",
      "episode number:  860\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -31.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 861 reward -31.00, Last 30ep Avg. rewards -31.00.\n",
      "episode number:  861\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.14515014+0.43070046j 0.87778708+0.15138823j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -31.0\n",
      "episode number:  861\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -31.0\n",
      "episode number:  861\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -31.0\n",
      "episode number:  861\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -31.0\n",
      "episode number:  861\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -32.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 862 reward -32.00, Last 30ep Avg. rewards -32.00.\n",
      "episode number:  862\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.67948141+0.04015018j 0.37047605+0.63201303j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.74243204+0.47529116j 0.21849979-0.41851023j]\n",
      "RESULT: False\n",
      "reward till now:  -32.0\n",
      "episode number:  862\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.74243204+0.47529116j 0.21849979-0.41851023j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.67948141+0.04015018j 0.37047605+0.63201303j]\n",
      "RESULT: False\n",
      "reward till now:  -32.0\n",
      "episode number:  862\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.67948141+0.04015018j 0.37047605+0.63201303j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.74243204+0.47529116j 0.21849979-0.41851023j]\n",
      "RESULT: False\n",
      "reward till now:  -32.0\n",
      "episode number:  862\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.74243204+0.47529116j 0.21849979-0.41851023j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.67948141+0.04015018j 0.37047605+0.63201303j]\n",
      "RESULT: False\n",
      "reward till now:  -32.0\n",
      "episode number:  862\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.67948141+0.04015018j 0.37047605+0.63201303j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.74243204+0.47529116j 0.21849979-0.41851023j]\n",
      "RESULT: False\n",
      "reward till now:  -33.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 863 reward -33.00, Last 30ep Avg. rewards -33.00.\n",
      "episode number:  863\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.99762717-0.00255101j -0.05343089-0.04334342j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.66764759-0.03245227j 0.74321029+0.02884459j]\n",
      "RESULT: False\n",
      "reward till now:  -33.0\n",
      "episode number:  863\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.66764759-0.03245227j 0.74321029+0.02884459j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.99762717-0.00255101j -0.05343089-0.04334342j]\n",
      "RESULT: False\n",
      "reward till now:  -33.0\n",
      "episode number:  863\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.99762717-0.00255101j -0.05343089-0.04334342j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.66764759-0.03245227j 0.74321029+0.02884459j]\n",
      "RESULT: False\n",
      "reward till now:  -33.0\n",
      "episode number:  863\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.66764759-0.03245227j 0.74321029+0.02884459j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.99762717-0.00255101j -0.05343089-0.04334342j]\n",
      "RESULT: False\n",
      "reward till now:  -33.0\n",
      "episode number:  863\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.99762717-0.00255101j -0.05343089-0.04334342j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.66764759-0.03245227j 0.74321029+0.02884459j]\n",
      "RESULT: False\n",
      "reward till now:  -34.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 864 reward -34.00, Last 30ep Avg. rewards -34.00.\n",
      "episode number:  864\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.19179816-0.17577321j 0.12439109-0.95751977j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -34.0\n",
      "episode number:  864\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -34.0\n",
      "episode number:  864\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -34.0\n",
      "episode number:  864\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -34.0\n",
      "episode number:  864\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -35.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 865 reward -35.00, Last 30ep Avg. rewards -35.00.\n",
      "episode number:  865\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.6205523 -0.52564527j -0.20117565-0.54602221j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -35.0\n",
      "episode number:  865\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -34.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 866 reward -34.00, Last 30ep Avg. rewards -34.00.\n",
      "episode number:  866\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.10651419-0.16754482j  0.92560845-0.32223045j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -34.0\n",
      "episode number:  866\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -34.0\n",
      "episode number:  866\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -34.0\n",
      "episode number:  866\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -34.0\n",
      "episode number:  866\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -35.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 867 reward -35.00, Last 30ep Avg. rewards -35.00.\n",
      "episode number:  867\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.13175334-0.89738295j -0.31380082+0.28084506j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -35.0\n",
      "episode number:  867\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -34.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 868 reward -34.00, Last 30ep Avg. rewards -34.00.\n",
      "episode number:  868\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.0243585 -0.22436294j  0.97221101+0.06223892j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.67023294-0.11463899j -0.70468106-0.20265812j]\n",
      "RESULT: False\n",
      "reward till now:  -34.0\n",
      "episode number:  868\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[ 0.67023294-0.11463899j -0.70468106-0.20265812j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -34.0\n",
      "episode number:  868\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -33.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 869 reward -33.00, Last 30ep Avg. rewards -33.00.\n",
      "episode number:  869\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.71792497-0.37440811j -0.21973265+0.54416897j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.35227517+0.12003906j 0.66302406-0.64953208j]\n",
      "RESULT: False\n",
      "reward till now:  -33.0\n",
      "episode number:  869\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.35227517+0.12003906j 0.66302406-0.64953208j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -33.0\n",
      "episode number:  869\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -33.0\n",
      "episode number:  869\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[ 0.70710678+0.j -0.70710678+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -33.0\n",
      "episode number:  869\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [ 0.70710678+0.j -0.70710678+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -34.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 870 reward -34.00, Last 30ep Avg. rewards -34.00.\n",
      "episode number:  870\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.17302446-0.06165997j -0.20118939+0.96217639j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -34.0\n",
      "episode number:  870\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -34.0\n",
      "episode number:  870\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -34.0\n",
      "episode number:  870\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -34.0\n",
      "episode number:  870\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -35.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 871 reward -35.00, Last 30ep Avg. rewards -35.00.\n",
      "episode number:  871\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.50404618+0.53280683j -0.56482223-0.37819331j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -35.0\n",
      "episode number:  871\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -35.0\n",
      "episode number:  871\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -35.0\n",
      "episode number:  871\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -35.0\n",
      "episode number:  871\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -36.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 872 reward -36.00, Last 30ep Avg. rewards -36.00.\n",
      "episode number:  872\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.38191976-0.62624759j  0.43755875+0.52008998j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -36.0\n",
      "episode number:  872\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -36.0\n",
      "episode number:  872\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -36.0\n",
      "episode number:  872\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -36.0\n",
      "episode number:  872\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -37.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 873 reward -37.00, Last 30ep Avg. rewards -37.00.\n",
      "episode number:  873\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.11710296+0.22560655j  0.38549861-0.88700586j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -37.0\n",
      "episode number:  873\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -37.0\n",
      "episode number:  873\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -37.0\n",
      "episode number:  873\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -37.0\n",
      "episode number:  873\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -38.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 874 reward -38.00, Last 30ep Avg. rewards -38.00.\n",
      "episode number:  874\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.44596789-0.75929633j -0.47207898-0.04151111j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -38.0\n",
      "episode number:  874\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -37.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 875 reward -37.00, Last 30ep Avg. rewards -37.00.\n",
      "episode number:  875\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.10080749+0.95849202j -0.13885211-0.22770814j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -37.0\n",
      "episode number:  875\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -36.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 876 reward -36.00, Last 30ep Avg. rewards -36.00.\n",
      "episode number:  876\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.59363713-0.59955983j -0.21269407-0.49283284j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.26936742-0.77243826j 0.57016225-0.07546738j]\n",
      "RESULT: False\n",
      "reward till now:  -36.0\n",
      "episode number:  876\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.26936742-0.77243826j 0.57016225-0.07546738j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -36.0\n",
      "episode number:  876\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -36.0\n",
      "episode number:  876\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -36.0\n",
      "episode number:  876\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -37.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 877 reward -37.00, Last 30ep Avg. rewards -37.00.\n",
      "episode number:  877\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.18655541+0.79026405j 0.36966673-0.45169272j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -37.0\n",
      "episode number:  877\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -36.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 878 reward -36.00, Last 30ep Avg. rewards -36.00.\n",
      "episode number:  878\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.47970803-0.12069114j  0.86270097+0.10517073j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -36.0\n",
      "episode number:  878\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -36.0\n",
      "episode number:  878\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -36.0\n",
      "episode number:  878\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -36.0\n",
      "episode number:  878\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -37.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 879 reward -37.00, Last 30ep Avg. rewards -37.00.\n",
      "episode number:  879\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.65362636-0.17668241j 0.38375805-0.6279217j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.73354156-0.56894102j 0.19082571+0.31907436j]\n",
      "RESULT: False\n",
      "reward till now:  -37.0\n",
      "episode number:  879\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.73354156-0.56894102j 0.19082571+0.31907436j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.65362636-0.17668241j 0.38375805-0.6279217j ]\n",
      "RESULT: False\n",
      "reward till now:  -37.0\n",
      "episode number:  879\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.65362636-0.17668241j 0.38375805-0.6279217j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.73354156-0.56894102j 0.19082571+0.31907436j]\n",
      "RESULT: False\n",
      "reward till now:  -37.0\n",
      "episode number:  879\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.73354156-0.56894102j 0.19082571+0.31907436j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.65362636-0.17668241j 0.38375805-0.6279217j ]\n",
      "RESULT: False\n",
      "reward till now:  -37.0\n",
      "episode number:  879\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.65362636-0.17668241j 0.38375805-0.6279217j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.73354156-0.56894102j 0.19082571+0.31907436j]\n",
      "RESULT: False\n",
      "reward till now:  -38.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 880 reward -38.00, Last 30ep Avg. rewards -38.00.\n",
      "episode number:  880\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.27398843-0.37339782j  0.12998136+0.87670363j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -38.0\n",
      "episode number:  880\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -38.0\n",
      "episode number:  880\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -38.0\n",
      "episode number:  880\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -38.0\n",
      "episode number:  880\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -39.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 881 reward -39.00, Last 30ep Avg. rewards -39.00.\n",
      "episode number:  881\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.67939043+0.45317308j -0.49005627+0.30480758j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.13387947+0.53597327j 0.82692369+0.10491025j]\n",
      "RESULT: False\n",
      "reward till now:  -39.0\n",
      "episode number:  881\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.13387947+0.53597327j 0.82692369+0.10491025j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -39.0\n",
      "episode number:  881\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -39.0\n",
      "episode number:  881\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -39.0\n",
      "episode number:  881\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -40.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 882 reward -40.00, Last 30ep Avg. rewards -40.00.\n",
      "episode number:  882\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.55151076+0.19145947j  0.64916192+0.48761456j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -40.0\n",
      "episode number:  882\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -40.0\n",
      "episode number:  882\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -40.0\n",
      "episode number:  882\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -40.0\n",
      "episode number:  882\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -41.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 883 reward -41.00, Last 30ep Avg. rewards -41.00.\n",
      "episode number:  883\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.34630906+0.61222397j -0.61801979+0.3511458j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -41.0\n",
      "episode number:  883\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -41.0\n",
      "episode number:  883\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -41.0\n",
      "episode number:  883\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -41.0\n",
      "episode number:  883\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -42.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 884 reward -42.00, Last 30ep Avg. rewards -42.00.\n",
      "episode number:  884\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.45005572+0.84073131j -0.24198591+0.17906294j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -42.0\n",
      "episode number:  884\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -41.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 885 reward -41.00, Last 30ep Avg. rewards -41.00.\n",
      "episode number:  885\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.70883708-0.47503732j 0.49148215-0.17416898j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.84875387-0.45905818j 0.15369315-0.21274604j]\n",
      "RESULT: False\n",
      "reward till now:  -41.0\n",
      "episode number:  885\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.84875387-0.45905818j 0.15369315-0.21274604j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70883708-0.47503732j 0.49148215-0.17416898j]\n",
      "RESULT: False\n",
      "reward till now:  -41.0\n",
      "episode number:  885\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.70883708-0.47503732j 0.49148215-0.17416898j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.84875387-0.45905818j 0.15369315-0.21274604j]\n",
      "RESULT: False\n",
      "reward till now:  -41.0\n",
      "episode number:  885\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.84875387-0.45905818j 0.15369315-0.21274604j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70883708-0.47503732j 0.49148215-0.17416898j]\n",
      "RESULT: False\n",
      "reward till now:  -41.0\n",
      "episode number:  885\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.70883708-0.47503732j 0.49148215-0.17416898j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.84875387-0.45905818j 0.15369315-0.21274604j]\n",
      "RESULT: False\n",
      "reward till now:  -42.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 886 reward -42.00, Last 30ep Avg. rewards -42.00.\n",
      "episode number:  886\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.09841521+0.37921631j -0.30753635-0.86713945j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -42.0\n",
      "episode number:  886\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -41.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 887 reward -41.00, Last 30ep Avg. rewards -41.00.\n",
      "episode number:  887\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.66605775-0.52624159j  0.52576172-0.05487701j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -41.0\n",
      "episode number:  887\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -40.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 888 reward -40.00, Last 30ep Avg. rewards -40.00.\n",
      "episode number:  888\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.76637578+0.50621469j  0.19247595-0.34549655j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -40.0\n",
      "episode number:  888\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -39.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 889 reward -39.00, Last 30ep Avg. rewards -39.00.\n",
      "episode number:  889\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.66462721+0.17915661j 0.00775992+0.72533673j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.4754495 +0.63957337j 0.46447531-0.38620766j]\n",
      "RESULT: False\n",
      "reward till now:  -39.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode number:  889\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.4754495 +0.63957337j 0.46447531-0.38620766j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -39.0\n",
      "episode number:  889\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -38.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 890 reward -38.00, Last 30ep Avg. rewards -38.00.\n",
      "episode number:  890\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.45381439+0.69014146j -0.53672947-0.17227517j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -38.0\n",
      "episode number:  890\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -37.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]]\n",
      "Episode 891 reward -37.00, Last 30ep Avg. rewards -37.00.\n",
      "episode number:  891\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.52237719-0.27245522j 0.68273323+0.43216381j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -37.0\n",
      "episode number:  891\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -37.0\n",
      "episode number:  891\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -37.0\n",
      "episode number:  891\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -37.0\n",
      "episode number:  891\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -38.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 892 reward -38.00, Last 30ep Avg. rewards -38.00.\n",
      "episode number:  892\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.06890005+0.46182951j  0.79214489+0.39303022j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -38.0\n",
      "episode number:  892\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -37.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 893 reward -37.00, Last 30ep Avg. rewards -37.00.\n",
      "episode number:  893\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.75711368+0.11729672j 0.29947044-0.56862801j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.74711779-0.31913942j 0.32360264+0.48502203j]\n",
      "RESULT: False\n",
      "reward till now:  -37.0\n",
      "episode number:  893\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.74711779-0.31913942j 0.32360264+0.48502203j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.75711368+0.11729672j 0.29947044-0.56862801j]\n",
      "RESULT: False\n",
      "reward till now:  -37.0\n",
      "episode number:  893\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.75711368+0.11729672j 0.29947044-0.56862801j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.74711779-0.31913942j 0.32360264+0.48502203j]\n",
      "RESULT: False\n",
      "reward till now:  -37.0\n",
      "episode number:  893\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.74711779-0.31913942j 0.32360264+0.48502203j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.75711368+0.11729672j 0.29947044-0.56862801j]\n",
      "RESULT: False\n",
      "reward till now:  -37.0\n",
      "episode number:  893\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.75711368+0.11729672j 0.29947044-0.56862801j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.74711779-0.31913942j 0.32360264+0.48502203j]\n",
      "RESULT: False\n",
      "reward till now:  -38.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 894 reward -38.00, Last 30ep Avg. rewards -38.00.\n",
      "episode number:  894\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.25838332-0.90692187j  0.27515598-0.18713623j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -38.0\n",
      "episode number:  894\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -37.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 895 reward -37.00, Last 30ep Avg. rewards -37.00.\n",
      "episode number:  895\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.33930792+0.0543939j   0.43322652+0.83320239j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -37.0\n",
      "episode number:  895\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -37.0\n",
      "episode number:  895\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -37.0\n",
      "episode number:  895\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -37.0\n",
      "episode number:  895\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -38.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 896 reward -38.00, Last 30ep Avg. rewards -38.00.\n",
      "episode number:  896\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.46445005-0.49185922j 0.08073273+0.73201291j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.38550244+0.1698143j  0.27132912-0.86540828j]\n",
      "RESULT: False\n",
      "reward till now:  -38.0\n",
      "episode number:  896\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.38550244+0.1698143j  0.27132912-0.86540828j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -38.0\n",
      "episode number:  896\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -38.0\n",
      "episode number:  896\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -38.0\n",
      "episode number:  896\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -39.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 897 reward -39.00, Last 30ep Avg. rewards -39.00.\n",
      "episode number:  897\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.35456104-0.74047085j 0.57044066+0.02422491j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -39.0\n",
      "episode number:  897\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -38.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 898 reward -38.00, Last 30ep Avg. rewards -38.00.\n",
      "episode number:  898\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.38155588-0.39031322j  0.22790047+0.80630768j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -38.0\n",
      "episode number:  898\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -37.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 899 reward -37.00, Last 30ep Avg. rewards -37.00.\n",
      "episode number:  899\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.57001144-0.68773312j 0.42249631+0.15364563j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -37.0\n",
      "episode number:  899\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -36.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 900 reward -36.00, Last 30ep Avg. rewards -36.00.\n",
      "episode number:  900\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.54659798-0.75468593j 0.21773229-0.29029717j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.54046312-0.73891464j 0.23254316-0.32837244j]\n",
      "RESULT: False\n",
      "reward till now:  -36.0\n",
      "episode number:  900\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.54046312-0.73891464j 0.23254316-0.32837244j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.54659798-0.75468593j 0.21773229-0.29029717j]\n",
      "RESULT: False\n",
      "reward till now:  -36.0\n",
      "episode number:  900\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.54659798-0.75468593j 0.21773229-0.29029717j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.54046312-0.73891464j 0.23254316-0.32837244j]\n",
      "RESULT: False\n",
      "reward till now:  -36.0\n",
      "episode number:  900\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.54046312-0.73891464j 0.23254316-0.32837244j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.54659798-0.75468593j 0.21773229-0.29029717j]\n",
      "RESULT: False\n",
      "reward till now:  -36.0\n",
      "episode number:  900\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.54659798-0.75468593j 0.21773229-0.29029717j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.54046312-0.73891464j 0.23254316-0.32837244j]\n",
      "RESULT: False\n",
      "reward till now:  -37.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 901 reward -37.00, Last 30ep Avg. rewards -37.00.\n",
      "episode number:  901\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.11880453+0.17841914j 0.62681655-0.74910153j]]]\n",
      "Action:  hadamard_Y(new_state)\n",
      "NEW STATE:  [-0.44568728-0.31706485j -0.56938762+0.61370226j]\n",
      "RESULT: False\n",
      "reward till now:  -37.0\n",
      "episode number:  901\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.44568728-0.31706485j -0.56938762+0.61370226j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -37.0\n",
      "episode number:  901\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -36.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 902 reward -36.00, Last 30ep Avg. rewards -36.00.\n",
      "episode number:  902\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.91158091+0.13165948j 0.15824172+0.35587298j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.75647884+0.34473751j 0.53269125-0.15854289j]\n",
      "RESULT: False\n",
      "reward till now:  -36.0\n",
      "episode number:  902\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.75647884+0.34473751j 0.53269125-0.15854289j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.91158091+0.13165948j 0.15824172+0.35587298j]\n",
      "RESULT: False\n",
      "reward till now:  -36.0\n",
      "episode number:  902\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.91158091+0.13165948j 0.15824172+0.35587298j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.75647884+0.34473751j 0.53269125-0.15854289j]\n",
      "RESULT: False\n",
      "reward till now:  -36.0\n",
      "episode number:  902\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.75647884+0.34473751j 0.53269125-0.15854289j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.91158091+0.13165948j 0.15824172+0.35587298j]\n",
      "RESULT: False\n",
      "reward till now:  -36.0\n",
      "episode number:  902\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.91158091+0.13165948j 0.15824172+0.35587298j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.75647884+0.34473751j 0.53269125-0.15854289j]\n",
      "RESULT: False\n",
      "reward till now:  -37.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 903 reward -37.00, Last 30ep Avg. rewards -37.00.\n",
      "episode number:  903\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.21759866+0.18920631j -0.8741275 +0.39083617j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -37.0\n",
      "episode number:  903\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -37.0\n",
      "episode number:  903\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -36.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 904 reward -36.00, Last 30ep Avg. rewards -36.00.\n",
      "episode number:  904\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.29467468-0.03109642j  0.69891777-0.65093302j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -36.0\n",
      "episode number:  904\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -36.0\n",
      "episode number:  904\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -35.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 905 reward -35.00, Last 30ep Avg. rewards -35.00.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode number:  905\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.2985227 -0.81088832j -0.32406924-0.38512784j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -35.0\n",
      "episode number:  905\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -34.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 906 reward -34.00, Last 30ep Avg. rewards -34.00.\n",
      "episode number:  906\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.44432219+0.04029962j -0.53261659-0.71921714j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -34.0\n",
      "episode number:  906\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -33.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 907 reward -33.00, Last 30ep Avg. rewards -33.00.\n",
      "episode number:  907\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.15894237+0.9728366j  -0.13630305+0.09873068j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -33.0\n",
      "episode number:  907\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -32.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 908 reward -32.00, Last 30ep Avg. rewards -32.00.\n",
      "episode number:  908\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.08333299+0.2249453j   0.15151141+0.95890537j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -32.0\n",
      "episode number:  908\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -32.0\n",
      "episode number:  908\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -31.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 909 reward -31.00, Last 30ep Avg. rewards -31.00.\n",
      "episode number:  909\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.47893404-0.74849913j  0.06663466+0.45379626j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -31.0\n",
      "episode number:  909\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -30.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 910 reward -30.00, Last 30ep Avg. rewards -30.00.\n",
      "episode number:  910\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.91884874-0.17391365j -0.17664782+0.30702863j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.5248153 +0.09412651j 0.77463305-0.34007755j]\n",
      "RESULT: False\n",
      "reward till now:  -30.0\n",
      "episode number:  910\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.5248153 +0.09412651j 0.77463305-0.34007755j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.77463305-0.34007755j 0.5248153 +0.09412651j]\n",
      "RESULT: False\n",
      "reward till now:  -30.0\n",
      "episode number:  910\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.77463305-0.34007755j 0.5248153 +0.09412651j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.91884874-0.17391365j 0.17664782-0.30702863j]\n",
      "RESULT: False\n",
      "reward till now:  -30.0\n",
      "episode number:  910\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.91884874-0.17391365j 0.17664782-0.30702863j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.77463305-0.34007755j 0.5248153 +0.09412651j]\n",
      "RESULT: False\n",
      "reward till now:  -30.0\n",
      "episode number:  910\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.77463305-0.34007755j 0.5248153 +0.09412651j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.91884874-0.17391365j 0.17664782-0.30702863j]\n",
      "RESULT: False\n",
      "reward till now:  -31.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 911 reward -31.00, Last 30ep Avg. rewards -31.00.\n",
      "episode number:  911\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.62562465+0.6767895j   0.25831985-0.28951793j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -31.0\n",
      "episode number:  911\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -30.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 912 reward -30.00, Last 30ep Avg. rewards -30.00.\n",
      "episode number:  912\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.81350577+0.04086844j -0.39762009+0.42241732j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -30.0\n",
      "episode number:  912\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -29.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 913 reward -29.00, Last 30ep Avg. rewards -29.00.\n",
      "episode number:  913\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.01719435-0.84132628j -0.52886697-0.11033666j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -29.0\n",
      "episode number:  913\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -28.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 914 reward -28.00, Last 30ep Avg. rewards -28.00.\n",
      "episode number:  914\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.2066642 +0.07654275j -0.78249177-0.58235534j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -28.0\n",
      "episode number:  914\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -28.0\n",
      "episode number:  914\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -27.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 915 reward -27.00, Last 30ep Avg. rewards -27.00.\n",
      "episode number:  915\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.44017597-0.55033107j -0.55612718-0.44057166j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -27.0\n",
      "episode number:  915\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -27.0\n",
      "episode number:  915\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -26.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 916 reward -26.00, Last 30ep Avg. rewards -26.00.\n",
      "episode number:  916\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.88671986-0.04733806j -0.25281408+0.38415105j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.44823907+0.23816275j 0.80577218-0.30510888j]\n",
      "RESULT: False\n",
      "reward till now:  -26.0\n",
      "episode number:  916\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.44823907+0.23816275j 0.80577218-0.30510888j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.80577218-0.30510888j 0.44823907+0.23816275j]\n",
      "RESULT: False\n",
      "reward till now:  -26.0\n",
      "episode number:  916\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.80577218-0.30510888j 0.44823907+0.23816275j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.88671986-0.04733806j 0.25281408-0.38415105j]\n",
      "RESULT: False\n",
      "reward till now:  -26.0\n",
      "episode number:  916\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.88671986-0.04733806j 0.25281408-0.38415105j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.80577218-0.30510888j 0.44823907+0.23816275j]\n",
      "RESULT: False\n",
      "reward till now:  -26.0\n",
      "episode number:  916\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.80577218-0.30510888j 0.44823907+0.23816275j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.88671986-0.04733806j 0.25281408-0.38415105j]\n",
      "RESULT: False\n",
      "reward till now:  -27.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 917 reward -27.00, Last 30ep Avg. rewards -27.00.\n",
      "episode number:  917\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.22456412+0.37338636j -0.40075007+0.80594849j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -27.0\n",
      "episode number:  917\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -27.0\n",
      "episode number:  917\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -26.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 918 reward -26.00, Last 30ep Avg. rewards -26.00.\n",
      "episode number:  918\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.64054992+0.12746203j -0.73369621+0.18745425j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -26.0\n",
      "episode number:  918\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -26.0\n",
      "episode number:  918\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -25.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 919 reward -25.00, Last 30ep Avg. rewards -25.00.\n",
      "episode number:  919\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.4500451 +0.48705517j 0.23112765+0.71191058j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.23112765+0.71191058j 0.4500451 +0.48705517j]\n",
      "RESULT: False\n",
      "reward till now:  -25.0\n",
      "episode number:  919\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.23112765+0.71191058j 0.4500451 +0.48705517j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -25.0\n",
      "episode number:  919\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -24.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 920 reward -24.00, Last 30ep Avg. rewards -24.00.\n",
      "episode number:  920\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.11151259+0.77696402j -0.12957339+0.60588992j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -24.0\n",
      "episode number:  920\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -24.0\n",
      "episode number:  920\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -23.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 921 reward -23.00, Last 30ep Avg. rewards -23.00.\n",
      "episode number:  921\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.7161661 +0.59843613j -0.35913208+0.00211394j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -23.0\n",
      "episode number:  921\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -22.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 922 reward -22.00, Last 30ep Avg. rewards -22.00.\n",
      "episode number:  922\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.33825445-0.31922032j  0.87018136-0.16268594j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -22.0\n",
      "episode number:  922\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -22.0\n",
      "episode number:  922\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -21.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 923 reward -21.00, Last 30ep Avg. rewards -21.00.\n",
      "episode number:  923\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.94710492+0.18335437j -0.24721115-0.09088507j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -21.0\n",
      "episode number:  923\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -20.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 924 reward -20.00, Last 30ep Avg. rewards -20.00.\n",
      "episode number:  924\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.30080586-0.94229587j -0.13730933-0.05234957j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -20.0\n",
      "episode number:  924\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -19.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 925 reward -19.00, Last 30ep Avg. rewards -19.00.\n",
      "episode number:  925\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.32006779-0.15532182j  0.38469928+0.8517266j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -19.0\n",
      "episode number:  925\n",
      "time step:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -19.0\n",
      "episode number:  925\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -18.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 926 reward -18.00, Last 30ep Avg. rewards -18.00.\n",
      "episode number:  926\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.83347711+0.11873578j 0.04022443-0.5381447j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.61780028-0.29656689j 0.56091435+0.46448464j]\n",
      "RESULT: False\n",
      "reward till now:  -18.0\n",
      "episode number:  926\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.61780028-0.29656689j 0.56091435+0.46448464j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -18.0\n",
      "episode number:  926\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -17.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 927 reward -17.00, Last 30ep Avg. rewards -17.00.\n",
      "episode number:  927\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.06813012+0.84005463j -0.48749979+0.22805802j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -17.0\n",
      "episode number:  927\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -16.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 928 reward -16.00, Last 30ep Avg. rewards -16.00.\n",
      "episode number:  928\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.07032816+0.28739223j -0.95477831+0.02929237j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -16.0\n",
      "episode number:  928\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -16.0\n",
      "episode number:  928\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -15.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 929 reward -15.00, Last 30ep Avg. rewards -15.00.\n",
      "episode number:  929\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.30216332+0.59366853j  0.70243344+0.25068359j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -15.0\n",
      "episode number:  929\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -14.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 930 reward -14.00, Last 30ep Avg. rewards -14.00.\n",
      "episode number:  930\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.51252076+0.22276727j -0.5476585 +0.62270971j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -14.0\n",
      "episode number:  930\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -13.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]]\n",
      "Episode 931 reward -13.00, Last 30ep Avg. rewards -13.00.\n",
      "episode number:  931\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.59019798-0.20089498j 0.3488535 +0.6997205j ]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.3488535 +0.6997205j  0.59019798-0.20089498j]\n",
      "RESULT: False\n",
      "reward till now:  -13.0\n",
      "episode number:  931\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.3488535 +0.6997205j  0.59019798-0.20089498j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -13.0\n",
      "episode number:  931\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -12.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 932 reward -12.00, Last 30ep Avg. rewards -12.00.\n",
      "episode number:  932\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.29682739+0.53541665j -0.76808058-0.18781569j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -12.0\n",
      "episode number:  932\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -12.0\n",
      "episode number:  932\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -11.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 933 reward -11.00, Last 30ep Avg. rewards -11.00.\n",
      "episode number:  933\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.30850399-0.74151285j 0.58948665-0.08654171j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -11.0\n",
      "episode number:  933\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -11.0\n",
      "episode number:  933\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -10.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 934 reward -10.00, Last 30ep Avg. rewards -10.00.\n",
      "episode number:  934\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.14894068-0.02178668j -0.98569363+0.07582931j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -10.0\n",
      "episode number:  934\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -10.0\n",
      "episode number:  934\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -9.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 935 reward -9.00, Last 30ep Avg. rewards -9.00.\n",
      "episode number:  935\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.50021963+0.67315514j 0.52212415+0.15501243j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -9.0\n",
      "episode number:  935\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -8.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 936 reward -8.00, Last 30ep Avg. rewards -8.00.\n",
      "episode number:  936\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.25131571-0.84177793j -0.46534176+0.1082006j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -8.0\n",
      "episode number:  936\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -7.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 937 reward -7.00, Last 30ep Avg. rewards -7.00.\n",
      "episode number:  937\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.51577852-0.1990823j  -0.22485483-0.80235844j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [-0.22485483-0.80235844j  0.51577852-0.1990823j ]\n",
      "RESULT: False\n",
      "reward till now:  -7.0\n",
      "episode number:  937\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[-0.22485483-0.80235844j  0.51577852-0.1990823j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -7.0\n",
      "episode number:  937\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -6.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 938 reward -6.00, Last 30ep Avg. rewards -6.00.\n",
      "episode number:  938\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.16194381-0.15841286j -0.56897413-0.79053653j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -6.0\n",
      "episode number:  938\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -6.0\n",
      "episode number:  938\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -5.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 939 reward -5.00, Last 30ep Avg. rewards -5.00.\n",
      "episode number:  939\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.57902455-0.21860107j -0.69438097-0.36712289j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -5.0\n",
      "episode number:  939\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -4.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 940 reward -4.00, Last 30ep Avg. rewards -4.00.\n",
      "episode number:  940\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.14640591+0.48140156j -0.83093632+0.23740825j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -4.0\n",
      "episode number:  940\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -4.0\n",
      "episode number:  940\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -3.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 941 reward -3.00, Last 30ep Avg. rewards -3.00.\n",
      "episode number:  941\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.87501986-0.19701594j  0.28125561+0.34120411j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -3.0\n",
      "episode number:  941\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -3.0\n",
      "episode number:  941\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -2.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 942 reward -2.00, Last 30ep Avg. rewards -2.00.\n",
      "episode number:  942\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.01047209-0.20752735j -0.95248833-0.22268522j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -2.0\n",
      "episode number:  942\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -2.0\n",
      "episode number:  942\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  -1.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 943 reward -1.00, Last 30ep Avg. rewards -1.00.\n",
      "episode number:  943\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.22267283-0.72390342j 0.65109941-0.04949963j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  -1.0\n",
      "episode number:  943\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  0.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 944 reward 0.00, Last 30ep Avg. rewards 0.00.\n",
      "episode number:  944\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.18007496+0.97194633j -0.0783072 +0.12946551j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  0.0\n",
      "episode number:  944\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  1.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 945 reward 1.00, Last 30ep Avg. rewards 1.00.\n",
      "episode number:  945\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.09813987-0.74013305j 0.0134753 +0.66512409j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  1.0\n",
      "episode number:  945\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  2.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 946 reward 2.00, Last 30ep Avg. rewards 2.00.\n",
      "episode number:  946\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.79442681+0.27390883j -0.38485288-0.3817699j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.28961251-0.07626929j 0.83387666+0.46363488j]\n",
      "RESULT: False\n",
      "reward till now:  2.0\n",
      "episode number:  946\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.28961251-0.07626929j 0.83387666+0.46363488j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.83387666+0.46363488j 0.28961251-0.07626929j]\n",
      "RESULT: False\n",
      "reward till now:  2.0\n",
      "episode number:  946\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.83387666+0.46363488j 0.28961251-0.07626929j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.79442681+0.27390883j 0.38485288+0.3817699j ]\n",
      "RESULT: False\n",
      "reward till now:  2.0\n",
      "episode number:  946\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.79442681+0.27390883j 0.38485288+0.3817699j ]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.83387666+0.46363488j 0.28961251-0.07626929j]\n",
      "RESULT: False\n",
      "reward till now:  2.0\n",
      "episode number:  946\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.83387666+0.46363488j 0.28961251-0.07626929j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.79442681+0.27390883j 0.38485288+0.3817699j ]\n",
      "RESULT: False\n",
      "reward till now:  1.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 947 reward 1.00, Last 30ep Avg. rewards 1.00.\n",
      "episode number:  947\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.70508226+0.15032313j  0.31381964-0.61788283j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  1.0\n",
      "episode number:  947\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  1.0\n",
      "episode number:  947\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  2.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 948 reward 2.00, Last 30ep Avg. rewards 2.00.\n",
      "episode number:  948\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.42630533-0.72935991j 0.35081115-0.40401661j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  2.0\n",
      "episode number:  948\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  3.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 949 reward 3.00, Last 30ep Avg. rewards 3.00.\n",
      "episode number:  949\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.12825171-0.30702665j -0.71048674-0.62007639j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  3.0\n",
      "episode number:  949\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  3.0\n",
      "episode number:  949\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  4.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 950 reward 4.00, Last 30ep Avg. rewards 4.00.\n",
      "episode number:  950\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.48137392+0.68454172j  0.33012386-0.43669213j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  4.0\n",
      "episode number:  950\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  5.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.95]\n",
      " [1.  ]]\n",
      "Episode 951 reward 5.00, Last 30ep Avg. rewards 5.00.\n",
      "episode number:  951\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.3413347 -0.27290694j -0.31703834-0.84172389j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  5.0\n",
      "episode number:  951\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  5.0\n",
      "episode number:  951\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  5.0\n",
      "episode number:  951\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  5.0\n",
      "episode number:  951\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  4.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 952 reward 4.00, Last 30ep Avg. rewards 4.00.\n",
      "episode number:  952\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.43206765-0.83009237j -0.33253596-0.1169788j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  4.0\n",
      "episode number:  952\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  4.0\n",
      "episode number:  952\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  4.0\n",
      "episode number:  952\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  4.0\n",
      "episode number:  952\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  3.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 953 reward 3.00, Last 30ep Avg. rewards 3.00.\n",
      "episode number:  953\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.74450811+0.06223375j -0.59099294+0.30423999j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  3.0\n",
      "episode number:  953\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  4.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 954 reward 4.00, Last 30ep Avg. rewards 4.00.\n",
      "episode number:  954\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.72589998+0.17642942j -0.07759237+0.66024337j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.45842271+0.591617j   0.56815488-0.34210812j]\n",
      "RESULT: False\n",
      "reward till now:  4.0\n",
      "episode number:  954\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.45842271+0.591617j   0.56815488-0.34210812j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  4.0\n",
      "episode number:  954\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  4.0\n",
      "episode number:  954\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  4.0\n",
      "episode number:  954\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  3.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 955 reward 3.00, Last 30ep Avg. rewards 3.00.\n",
      "episode number:  955\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.8917049 -0.08154924j -0.23298634-0.37938037j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  3.0\n",
      "episode number:  955\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  4.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 956 reward 4.00, Last 30ep Avg. rewards 4.00.\n",
      "episode number:  956\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.76823813+0.51955697j -0.37160257+0.04221682j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.28046369+0.39723406j 0.80598909+0.33753046j]\n",
      "RESULT: False\n",
      "reward till now:  4.0\n",
      "episode number:  956\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.28046369+0.39723406j 0.80598909+0.33753046j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.80598909+0.33753046j 0.28046369+0.39723406j]\n",
      "RESULT: False\n",
      "reward till now:  4.0\n",
      "episode number:  956\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.80598909+0.33753046j 0.28046369+0.39723406j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.76823813+0.51955697j 0.37160257-0.04221682j]\n",
      "RESULT: False\n",
      "reward till now:  4.0\n",
      "episode number:  956\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.76823813+0.51955697j 0.37160257-0.04221682j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.80598909+0.33753046j 0.28046369+0.39723406j]\n",
      "RESULT: False\n",
      "reward till now:  4.0\n",
      "episode number:  956\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.80598909+0.33753046j 0.28046369+0.39723406j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.76823813+0.51955697j 0.37160257-0.04221682j]\n",
      "RESULT: False\n",
      "reward till now:  3.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 957 reward 3.00, Last 30ep Avg. rewards 3.00.\n",
      "episode number:  957\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.19614849+0.7541075j  0.56597107+0.26930354j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  3.0\n",
      "episode number:  957\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  4.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 958 reward 4.00, Last 30ep Avg. rewards 4.00.\n",
      "episode number:  958\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.21220812+0.93399683j -0.28697447+0.01622615j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  4.0\n",
      "episode number:  958\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  5.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 959 reward 5.00, Last 30ep Avg. rewards 5.00.\n",
      "episode number:  959\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.31365794-0.41866316j -0.52762359+0.66929306j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  5.0\n",
      "episode number:  959\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  5.0\n",
      "episode number:  959\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  5.0\n",
      "episode number:  959\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  5.0\n",
      "episode number:  959\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  4.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 960 reward 4.00, Last 30ep Avg. rewards 4.00.\n",
      "episode number:  960\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.05596793-0.69593889j -0.35392225-0.62231478j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  4.0\n",
      "episode number:  960\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  4.0\n",
      "episode number:  960\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  5.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 961 reward 5.00, Last 30ep Avg. rewards 5.00.\n",
      "episode number:  961\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.01720148+0.45489168j -0.60635964-0.65200126j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  5.0\n",
      "episode number:  961\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  5.0\n",
      "episode number:  961\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  6.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 962 reward 6.00, Last 30ep Avg. rewards 6.00.\n",
      "episode number:  962\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.58652301-0.26149231j 0.0979005 -0.76028154j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.4839605 -0.72250322j 0.34550829+0.35269725j]\n",
      "RESULT: False\n",
      "reward till now:  6.0\n",
      "episode number:  962\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.4839605 -0.72250322j 0.34550829+0.35269725j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.34550829+0.35269725j 0.4839605 -0.72250322j]\n",
      "RESULT: False\n",
      "reward till now:  6.0\n",
      "episode number:  962\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.34550829+0.35269725j 0.4839605 -0.72250322j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  6.0\n",
      "episode number:  962\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  6.0\n",
      "episode number:  962\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  7.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025    ]\n",
      " [0.95      ]\n",
      " [1.        ]\n",
      " [0.9025    ]\n",
      " [0.95      ]\n",
      " [1.        ]\n",
      " [0.81450625]\n",
      " [0.857375  ]\n",
      " [0.9025    ]\n",
      " [0.95      ]\n",
      " [1.        ]]\n",
      "Episode 963 reward 7.00, Last 30ep Avg. rewards 7.00.\n",
      "episode number:  963\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.11139005-0.67607914j  0.7076596 +0.17241561j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  7.0\n",
      "episode number:  963\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  8.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025    ]\n",
      " [0.95      ]\n",
      " [1.        ]\n",
      " [0.9025    ]\n",
      " [0.95      ]\n",
      " [1.        ]\n",
      " [0.81450625]\n",
      " [0.857375  ]\n",
      " [0.9025    ]\n",
      " [0.95      ]\n",
      " [1.        ]\n",
      " [0.95      ]\n",
      " [1.        ]]\n",
      "Episode 964 reward 8.00, Last 30ep Avg. rewards 8.00.\n",
      "episode number:  964\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.97461612+0.02408419j -0.0740478 +0.20990542j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  8.0\n",
      "episode number:  964\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULT: True\n",
      "reward till now:  9.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025    ]\n",
      " [0.95      ]\n",
      " [1.        ]\n",
      " [0.9025    ]\n",
      " [0.95      ]\n",
      " [1.        ]\n",
      " [0.81450625]\n",
      " [0.857375  ]\n",
      " [0.9025    ]\n",
      " [0.95      ]\n",
      " [1.        ]\n",
      " [0.95      ]\n",
      " [1.        ]\n",
      " [0.95      ]\n",
      " [1.        ]]\n",
      "Episode 965 reward 9.00, Last 30ep Avg. rewards 9.00.\n",
      "episode number:  965\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.26097047+0.64057279j  0.70444491+0.15911719j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  9.0\n",
      "episode number:  965\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  9.0\n",
      "episode number:  965\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  10.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025    ]\n",
      " [0.95      ]\n",
      " [1.        ]\n",
      " [0.9025    ]\n",
      " [0.95      ]\n",
      " [1.        ]\n",
      " [0.81450625]\n",
      " [0.857375  ]\n",
      " [0.9025    ]\n",
      " [0.95      ]\n",
      " [1.        ]\n",
      " [0.95      ]\n",
      " [1.        ]\n",
      " [0.95      ]\n",
      " [1.        ]\n",
      " [0.9025    ]\n",
      " [0.95      ]\n",
      " [1.        ]]\n",
      "Episode 966 reward 10.00, Last 30ep Avg. rewards 10.00.\n",
      "episode number:  966\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.23449577-0.02801295j -0.83545159-0.49623347j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  10.0\n",
      "episode number:  966\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  10.0\n",
      "episode number:  966\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  11.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025    ]\n",
      " [0.95      ]\n",
      " [1.        ]\n",
      " [0.9025    ]\n",
      " [0.95      ]\n",
      " [1.        ]\n",
      " [0.81450625]\n",
      " [0.857375  ]\n",
      " [0.9025    ]\n",
      " [0.95      ]\n",
      " [1.        ]\n",
      " [0.95      ]\n",
      " [1.        ]\n",
      " [0.95      ]\n",
      " [1.        ]\n",
      " [0.9025    ]\n",
      " [0.95      ]\n",
      " [1.        ]\n",
      " [0.9025    ]\n",
      " [0.95      ]\n",
      " [1.        ]]\n",
      "Episode 967 reward 11.00, Last 30ep Avg. rewards 11.00.\n",
      "episode number:  967\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.71852164-0.55956648j  0.25867689+0.32202217j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  11.0\n",
      "episode number:  967\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  12.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025    ]\n",
      " [0.95      ]\n",
      " [1.        ]\n",
      " [0.9025    ]\n",
      " [0.95      ]\n",
      " [1.        ]\n",
      " [0.81450625]\n",
      " [0.857375  ]\n",
      " [0.9025    ]\n",
      " [0.95      ]\n",
      " [1.        ]\n",
      " [0.95      ]\n",
      " [1.        ]\n",
      " [0.95      ]\n",
      " [1.        ]\n",
      " [0.9025    ]\n",
      " [0.95      ]\n",
      " [1.        ]\n",
      " [0.9025    ]\n",
      " [0.95      ]\n",
      " [1.        ]\n",
      " [0.95      ]\n",
      " [1.        ]]\n",
      "Episode 968 reward 12.00, Last 30ep Avg. rewards 12.00.\n",
      "episode number:  968\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.7718523 +0.2288294j  0.58590094+0.09274282j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.96007652+0.2273859j  0.13148747+0.09622774j]\n",
      "RESULT: False\n",
      "reward till now:  12.0\n",
      "episode number:  968\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.96007652+0.2273859j  0.13148747+0.09622774j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.7718523 +0.2288294j  0.58590094+0.09274282j]\n",
      "RESULT: False\n",
      "reward till now:  12.0\n",
      "episode number:  968\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.7718523 +0.2288294j  0.58590094+0.09274282j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.96007652+0.2273859j  0.13148747+0.09622774j]\n",
      "RESULT: False\n",
      "reward till now:  12.0\n",
      "episode number:  968\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.96007652+0.2273859j  0.13148747+0.09622774j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.7718523 +0.2288294j  0.58590094+0.09274282j]\n",
      "RESULT: False\n",
      "reward till now:  12.0\n",
      "episode number:  968\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.7718523 +0.2288294j  0.58590094+0.09274282j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.96007652+0.2273859j  0.13148747+0.09622774j]\n",
      "RESULT: False\n",
      "reward till now:  11.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 969 reward 11.00, Last 30ep Avg. rewards 11.00.\n",
      "episode number:  969\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.23597493+0.61307633j  0.07574554-0.75014389j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  11.0\n",
      "episode number:  969\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  11.0\n",
      "episode number:  969\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  12.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.81450625]\n",
      " [ 0.857375  ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 970 reward 12.00, Last 30ep Avg. rewards 12.00.\n",
      "episode number:  970\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.36016458-0.64126889j  0.0143885 +0.67738369j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  12.0\n",
      "episode number:  970\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  12.0\n",
      "episode number:  970\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  13.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 971 reward 13.00, Last 30ep Avg. rewards 13.00.\n",
      "episode number:  971\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.85924916+0.06190641j  0.30408138+0.40668537j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  13.0\n",
      "episode number:  971\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  14.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 972 reward 14.00, Last 30ep Avg. rewards 14.00.\n",
      "episode number:  972\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.16738097+0.08123297j 0.92535165-0.33031672j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.92535165-0.33031672j 0.16738097+0.08123297j]\n",
      "RESULT: False\n",
      "reward till now:  14.0\n",
      "episode number:  972\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.92535165-0.33031672j 0.16738097+0.08123297j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.77267865-0.17612881j 0.53596621-0.29100958j]\n",
      "RESULT: False\n",
      "reward till now:  14.0\n",
      "episode number:  972\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.77267865-0.17612881j 0.53596621-0.29100958j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.92535165-0.33031672j 0.16738097+0.08123297j]\n",
      "RESULT: False\n",
      "reward till now:  14.0\n",
      "episode number:  972\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.92535165-0.33031672j 0.16738097+0.08123297j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.77267865-0.17612881j 0.53596621-0.29100958j]\n",
      "RESULT: False\n",
      "reward till now:  14.0\n",
      "episode number:  972\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.77267865-0.17612881j 0.53596621-0.29100958j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.92535165-0.33031672j 0.16738097+0.08123297j]\n",
      "RESULT: False\n",
      "reward till now:  13.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 973 reward 13.00, Last 30ep Avg. rewards 13.00.\n",
      "episode number:  973\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.06398821-0.98459048j -0.08838511+0.1366571j ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  13.0\n",
      "episode number:  973\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  14.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 974 reward 14.00, Last 30ep Avg. rewards 14.00.\n",
      "episode number:  974\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.76478842-0.10586947j -0.16231625-0.61444589j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.42601216-0.50933988j 0.655562  +0.35961784j]\n",
      "RESULT: False\n",
      "reward till now:  14.0\n",
      "episode number:  974\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.42601216-0.50933988j 0.655562  +0.35961784j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.655562  +0.35961784j 0.42601216-0.50933988j]\n",
      "RESULT: False\n",
      "reward till now:  14.0\n",
      "episode number:  974\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.655562  +0.35961784j 0.42601216-0.50933988j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.76478842-0.10586947j 0.16231625+0.61444589j]\n",
      "RESULT: False\n",
      "reward till now:  14.0\n",
      "episode number:  974\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.76478842-0.10586947j 0.16231625+0.61444589j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.655562  +0.35961784j 0.42601216-0.50933988j]\n",
      "RESULT: False\n",
      "reward till now:  14.0\n",
      "episode number:  974\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.655562  +0.35961784j 0.42601216-0.50933988j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.76478842-0.10586947j 0.16231625+0.61444589j]\n",
      "RESULT: False\n",
      "reward till now:  13.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 975 reward 13.00, Last 30ep Avg. rewards 13.00.\n",
      "episode number:  975\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.64210006-0.29230549j -0.30921219-0.63769337j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  13.0\n",
      "episode number:  975\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  13.0\n",
      "episode number:  975\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  14.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 976 reward 14.00, Last 30ep Avg. rewards 14.00.\n",
      "episode number:  976\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.14141978+0.83224762j -0.46478513-0.26709384j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  14.0\n",
      "episode number:  976\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  15.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 977 reward 15.00, Last 30ep Avg. rewards 15.00.\n",
      "episode number:  977\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.22245698+0.14336507j -0.41441539+0.87075785j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  15.0\n",
      "episode number:  977\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  15.0\n",
      "episode number:  977\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  16.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 978 reward 16.00, Last 30ep Avg. rewards 16.00.\n",
      "episode number:  978\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.78528288+0.55639909j  0.26550209+0.05709205j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  16.0\n",
      "episode number:  978\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  17.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 979 reward 17.00, Last 30ep Avg. rewards 17.00.\n",
      "episode number:  979\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.21442074+0.34679655j  0.90449388-0.12508683j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  17.0\n",
      "episode number:  979\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  17.0\n",
      "episode number:  979\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  18.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 980 reward 18.00, Last 30ep Avg. rewards 18.00.\n",
      "episode number:  980\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.42598534+0.66022078j 0.57204419+0.23539426j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.57204419+0.23539426j 0.42598534+0.66022078j]\n",
      "RESULT: False\n",
      "reward till now:  18.0\n",
      "episode number:  980\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.57204419+0.23539426j 0.42598534+0.66022078j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.42598534+0.66022078j 0.57204419+0.23539426j]\n",
      "RESULT: False\n",
      "reward till now:  18.0\n",
      "episode number:  980\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.42598534+0.66022078j 0.57204419+0.23539426j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.57204419+0.23539426j 0.42598534+0.66022078j]\n",
      "RESULT: False\n",
      "reward till now:  18.0\n",
      "episode number:  980\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.57204419+0.23539426j 0.42598534+0.66022078j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.42598534+0.66022078j 0.57204419+0.23539426j]\n",
      "RESULT: False\n",
      "reward till now:  18.0\n",
      "episode number:  980\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.42598534+0.66022078j 0.57204419+0.23539426j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.57204419+0.23539426j 0.42598534+0.66022078j]\n",
      "RESULT: False\n",
      "reward till now:  17.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 981 reward 17.00, Last 30ep Avg. rewards 17.00.\n",
      "episode number:  981\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.53809488+0.27366354j -0.1339043 -0.78589554j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  17.0\n",
      "episode number:  981\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  17.0\n",
      "episode number:  981\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  17.0\n",
      "episode number:  981\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  17.0\n",
      "episode number:  981\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  16.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 982 reward 16.00, Last 30ep Avg. rewards 16.00.\n",
      "episode number:  982\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.18749248+0.93866795j -0.13179902+0.25763942j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULT: False\n",
      "reward till now:  16.0\n",
      "episode number:  982\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  17.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 983 reward 17.00, Last 30ep Avg. rewards 17.00.\n",
      "episode number:  983\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.7630303 -0.11820508j -0.37539162+0.51273136j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  17.0\n",
      "episode number:  983\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  18.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 984 reward 18.00, Last 30ep Avg. rewards 18.00.\n",
      "episode number:  984\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.21311986+0.17068315j -0.36567279+0.88979245j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  18.0\n",
      "episode number:  984\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  18.0\n",
      "episode number:  984\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  18.0\n",
      "episode number:  984\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  18.0\n",
      "episode number:  984\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  17.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 985 reward 17.00, Last 30ep Avg. rewards 17.00.\n",
      "episode number:  985\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.16558854+0.32508119j 0.25046224-0.896756j  ]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  17.0\n",
      "episode number:  985\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  17.0\n",
      "episode number:  985\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  17.0\n",
      "episode number:  985\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  17.0\n",
      "episode number:  985\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  16.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 986 reward 16.00, Last 30ep Avg. rewards 16.00.\n",
      "episode number:  986\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.32011292+0.26341311j  0.04476806+0.90892083j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  16.0\n",
      "episode number:  986\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  16.0\n",
      "episode number:  986\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  16.0\n",
      "episode number:  986\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  16.0\n",
      "episode number:  986\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  15.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 987 reward 15.00, Last 30ep Avg. rewards 15.00.\n",
      "episode number:  987\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.58642778-0.11574185j 0.79854509-0.0709367j ]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.79854509-0.0709367j  0.58642778-0.11574185j]\n",
      "RESULT: False\n",
      "reward till now:  15.0\n",
      "episode number:  987\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.79854509-0.0709367j  0.58642778-0.11574185j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.97932371-0.13200167j 0.14998959+0.03168203j]\n",
      "RESULT: False\n",
      "reward till now:  15.0\n",
      "episode number:  987\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.97932371-0.13200167j 0.14998959+0.03168203j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.79854509-0.0709367j  0.58642778-0.11574185j]\n",
      "RESULT: False\n",
      "reward till now:  15.0\n",
      "episode number:  987\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.79854509-0.0709367j  0.58642778-0.11574185j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.97932371-0.13200167j 0.14998959+0.03168203j]\n",
      "RESULT: False\n",
      "reward till now:  15.0\n",
      "episode number:  987\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.97932371-0.13200167j 0.14998959+0.03168203j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.79854509-0.0709367j  0.58642778-0.11574185j]\n",
      "RESULT: False\n",
      "reward till now:  14.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 988 reward 14.00, Last 30ep Avg. rewards 14.00.\n",
      "episode number:  988\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.99367519+0.08701129j -0.0261516 -0.06599056j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  14.0\n",
      "episode number:  988\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  15.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 989 reward 15.00, Last 30ep Avg. rewards 15.00.\n",
      "episode number:  989\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.18918086-0.32476804j  0.92603521-0.03457043j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  15.0\n",
      "episode number:  989\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  15.0\n",
      "episode number:  989\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  15.0\n",
      "episode number:  989\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  15.0\n",
      "episode number:  989\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  14.0\n",
      "in discounted reward\n",
      "disc_rw:  [[-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 990 reward 14.00, Last 30ep Avg. rewards 14.00.\n",
      "episode number:  990\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.04586328+0.53617143j -0.51462467+0.66751645j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  14.0\n",
      "episode number:  990\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  14.0\n",
      "episode number:  990\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  15.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 991 reward 15.00, Last 30ep Avg. rewards 15.00.\n",
      "episode number:  991\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.22552512+0.93785223j 0.16011285+0.20960795j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  15.0\n",
      "episode number:  991\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  16.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 992 reward 16.00, Last 30ep Avg. rewards 16.00.\n",
      "episode number:  992\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.72141239-0.31761174j -0.32851392-0.52035138j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  16.0\n",
      "episode number:  992\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  16.0\n",
      "episode number:  992\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  17.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 993 reward 17.00, Last 30ep Avg. rewards 17.00.\n",
      "episode number:  993\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.97685642+0.04191737j  0.1728276 +0.11884905j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  17.0\n",
      "episode number:  993\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  18.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 994 reward 18.00, Last 30ep Avg. rewards 18.00.\n",
      "episode number:  994\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.33962456-0.28813259j  0.26201683-0.85614365j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  18.0\n",
      "episode number:  994\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  18.0\n",
      "episode number:  994\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  19.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 995 reward 19.00, Last 30ep Avg. rewards 19.00.\n",
      "episode number:  995\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.57133172+0.06796227j -0.65295231-0.49255911j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  19.0\n",
      "episode number:  995\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  19.0\n",
      "episode number:  995\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  20.0\n",
      "in discounted reward\n",
      "disc_rw:  [[0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]\n",
      " [0.9025]\n",
      " [0.95  ]\n",
      " [1.    ]]\n",
      "Episode 996 reward 20.00, Last 30ep Avg. rewards 20.00.\n",
      "episode number:  996\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[0.31322737+0.29550412j 0.67939713-0.5941258j ]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [0.67939713-0.5941258j  0.31322737+0.29550412j]\n",
      "RESULT: False\n",
      "reward till now:  20.0\n",
      "episode number:  996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.67939713-0.5941258j  0.31322737+0.29550412j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70189152-0.21115742j 0.25892112-0.62906335j]\n",
      "RESULT: False\n",
      "reward till now:  20.0\n",
      "episode number:  996\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[0.70189152-0.21115742j 0.25892112-0.62906335j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.67939713-0.5941258j  0.31322737+0.29550412j]\n",
      "RESULT: False\n",
      "reward till now:  20.0\n",
      "episode number:  996\n",
      "time step:  3\n",
      "ORIGINAL STATE:  [[[0.67939713-0.5941258j  0.31322737+0.29550412j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70189152-0.21115742j 0.25892112-0.62906335j]\n",
      "RESULT: False\n",
      "reward till now:  20.0\n",
      "episode number:  996\n",
      "time step:  4\n",
      "ORIGINAL STATE:  [[[0.70189152-0.21115742j 0.25892112-0.62906335j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.67939713-0.5941258j  0.31322737+0.29550412j]\n",
      "RESULT: False\n",
      "reward till now:  19.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]]\n",
      "Episode 997 reward 19.00, Last 30ep Avg. rewards 19.00.\n",
      "episode number:  997\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.46403478-0.01492462j  0.45809404+0.75802297j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  19.0\n",
      "episode number:  997\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  19.0\n",
      "episode number:  997\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  20.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 998 reward 20.00, Last 30ep Avg. rewards 20.00.\n",
      "episode number:  998\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[-0.44764659+0.1536854j   0.69930752+0.53568865j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  20.0\n",
      "episode number:  998\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  20.0\n",
      "episode number:  998\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  21.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 999 reward 21.00, Last 30ep Avg. rewards 21.00.\n",
      "episode number:  999\n",
      "time step:  0\n",
      "ORIGINAL STATE:  [[[ 0.49367025+0.31698095j -0.79761054+0.14010778j]]]\n",
      "Action:  measurement(new_state[0],new_state[1])\n",
      "NEW STATE:  [0.+0.j 1.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  21.0\n",
      "episode number:  999\n",
      "time step:  1\n",
      "ORIGINAL STATE:  [[[0.+0.j 1.+0.j]]]\n",
      "Action:  bit_flip_X(new_state)\n",
      "NEW STATE:  [1.+0.j 0.+0.j]\n",
      "RESULT: False\n",
      "reward till now:  21.0\n",
      "episode number:  999\n",
      "time step:  2\n",
      "ORIGINAL STATE:  [[[1.+0.j 0.+0.j]]]\n",
      "Action:  hadamard_X(new_state)\n",
      "NEW STATE:  [0.70710678+0.j 0.70710678+0.j]\n",
      "RESULT: True\n",
      "reward till now:  22.0\n",
      "in discounted reward\n",
      "disc_rw:  [[ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [-0.81450625]\n",
      " [-0.857375  ]\n",
      " [-0.9025    ]\n",
      " [-0.95      ]\n",
      " [-1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]\n",
      " [ 0.9025    ]\n",
      " [ 0.95      ]\n",
      " [ 1.        ]]\n",
      "Episode 1000 reward 22.00, Last 30ep Avg. rewards 22.00.\n"
     ]
    }
   ],
   "source": [
    "env = Environment()\n",
    "state = env.reset()\n",
    "\n",
    "agent = Agent(state[0].shape, state)\n",
    "final_reward = agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(final_reward)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.model.save(\"model_July_7.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING\n",
    "\n",
    "Two models have been created, model_June_23 and model_July_7. The former has some issues but the latter seems to be working. One issue with the latter one is that it prefers Hadamard X over Hadamard Y. So need work on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = load_model(\"model_July_7.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hadamard_Y\n",
      "1\n",
      "hadamard_Y\n",
      "2\n",
      "hadamard_Y\n",
      "3\n",
      "hadamard_Y\n",
      "4\n",
      "hadamard_Y\n",
      "5\n",
      "hadamard_Y\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: The `random_state` function is deprecated as of 0.13.0, and will be removed no earlier than 3 months after that release date. You should use the `random_statevector` function instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "env = Environment()\n",
    "new_state = env.reset()\n",
    "final_state = np.array([(1/math.sqrt(2))+0j,(1/math.sqrt(2))+0j])\n",
    "count=0\n",
    "while not np.allclose(new_state,final_state):\n",
    "    if (count<=5):\n",
    "        options = model_test.predict(new_state)\n",
    "        options = np.squeeze(options)\n",
    "        action = np.where(options == np.amax(options))[0][0]\n",
    "        print(command[action].split('(')[0])\n",
    "        new_state = np.squeeze(new_state)\n",
    "        new_state = eval(command[action])\n",
    "        new_state = np.reshape(new_state, (1,1,2))\n",
    "        count+=1\n",
    "        print(count)\n",
    "    \n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
