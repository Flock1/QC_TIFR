{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating agent and its properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-9-e2ebd02c1738>, line 61)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-e2ebd02c1738>\"\u001b[0;36m, line \u001b[0;32m61\u001b[0m\n\u001b[0;31m    def calc_reward(states):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "states, prob_actions, dlogps, drs, prot_reward, prot_rt2 = [],[],[],[],[],[]\n",
    "prev_action, tr_y = [],[]\n",
    "avg_reward = []\n",
    "reward_sum = 0\n",
    "time_step = 0\n",
    "P = 0.1                      # Punishment factor \n",
    "state = 0                    #CHANGE\n",
    "# observation = starting state?\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, state_size, is_eval=False, model_name=\"\"):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = 3 # measurement, CNOT, bit-flip\n",
    "        self.memory = deque(maxlen=1000)\n",
    "        self.inventory = []\n",
    "        self.model_name = model_name\n",
    "        self.is_eval = is_eval\n",
    "        self.done = false\n",
    "\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "\n",
    "#         self.model = load_model(\"models/\" + model_name) if is_eval else self._model()\n",
    "\n",
    "    def _model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=4, input_dim=self.state_size, activation=\"relu\"))\n",
    "        model.add(Dense(units=32, activation=\"relu\"))\n",
    "        model.add(Dense(units=8, activation=\"relu\"))\n",
    "        model.add(Dense(self.action_size, activation=\"softmax\"))\n",
    "        model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.003))\n",
    "        return model\n",
    "    \n",
    "    def act(self, state):\n",
    "#         if not self.is_eval and random.random() <= self.epsilon:\n",
    "#             return random.randrange(self.action_size)\n",
    "        \n",
    "        options = self.model.predict(state)\n",
    "        return np.argmax(options[0]), options\n",
    "    \n",
    "#     def policy_gradient(act_prob, rewards):\n",
    "    '''Here, get the log probabilites and return and multiply it.'''\n",
    "#         discounted_rewards = []\n",
    "        \n",
    "#         for i in range(len(rewards)):\n",
    "#             Gt = 0\n",
    "#             pw = 0\n",
    "            \n",
    "#             for r in rewards[t:]:\n",
    "#                 Gt = Gt + gamma**pw * r\n",
    "#                 pw = pw + 1\n",
    "            \n",
    "#             discounted_rewards.append(Gt)\n",
    "\n",
    "\n",
    "'''This function will help in calculating full reward for all the states till \"done\" state'''\n",
    "    def calc_reward(states):\n",
    "        \n",
    "        next_state = 0\n",
    "        for i in range(len(states)):\n",
    "            prot_reward, prot_rt2 = protect_reward(next_state, states[i], P, dt, Ttriv)\n",
    "            next_state = states[i]\n",
    "            return \n",
    "        \n",
    "            \n",
    "    def train(self):\n",
    "\n",
    "'''Add recoverable QI after we get the state (measurment). This RQI will help us get the reward. This reward is \n",
    "   then used to calculate discounted reward and then the return. We'll use this for training''' \n",
    "\n",
    "\n",
    "        if time_step==0:\n",
    "            action, aprob = self.act(state)\n",
    "            \n",
    "            '''Use these actions to get the new state through the simulator'''\n",
    "            x = state\n",
    "            \n",
    "            '''Use this state to calculate the Recoverable QI'''\n",
    "            rq_train = Rq(state)\n",
    "            \n",
    "            '''Store these RQI in a list for future'''\n",
    "            states.append(rq_train)\n",
    "            \n",
    "            '''Use this RQI to calculate reward'''\n",
    "            \n",
    "            \n",
    "            \n",
    "            prob_actions.append(aprob)\n",
    "        \n",
    "        elif time_step<200:\n",
    "            \n",
    "        elif time_step == 200:\n",
    "            done = True\n",
    "            \n",
    "        '''If reached the done state, use the list to calculate the new states and the reward'''\n",
    "        if done:\n",
    "            \n",
    "            '''Get the information, get the reward needed and the return and start the training all over again'''\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protection reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def protect_reward(next_Rqavg, Rqavd, P, dt, Ttriv):\n",
    "\n",
    "    if(next_Rqavg > 0):\n",
    "        eq = (next_Rqavg-Rq)/(2*dt/Ttriv)\n",
    "        reward = 1 + eq\n",
    "        rt2 = 0\n",
    "        return reward, rt2\n",
    "\n",
    "    elif(Rqavd !=0 and next_Rqavg==0):\n",
    "        reward = -P\n",
    "        rt2 = -P\n",
    "        return reward, rt2\n",
    "\n",
    "    elif(Rqavd==0):\n",
    "        reward = 0\n",
    "        rt2 = 0\n",
    "        return reward, rt2\n",
    "\n",
    "def return_reward(gamma, current_time, final_time, rt2, reward):\n",
    "    \n",
    "    target = []\n",
    "    for i in range(0,final_time-current_time):\n",
    "        target.append(np.sum((gamma**i)*reward[i]))\n",
    "\n",
    "    return (((1-gamma)*np.sum(target)) + rt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recoverable QI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rq(msmt):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) How to compute RQI? It's challenging\n",
    "\n",
    "2) Difference between input state and density matrix and how to compute the output state\n",
    "\n",
    "3) How long does it have to run for? I'm taking 200 time steps and what should I take Rq(t+1) in reward?\n",
    "\n",
    "4) How is the Rqavg calculated? \n",
    "\n",
    "5) What are the four inputs?\n",
    "\n",
    "6) How the input is normalized?\n",
    "\n",
    "7) CNOT between which qbits? How to incoporate in output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
